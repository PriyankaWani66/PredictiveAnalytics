{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a2a66cc-c247-40e6-a279-ce62bf864882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91cc9571",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from  torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c5657-3438-4c70-b639-048e3de7fac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e2e7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "diagnosis_df = pd.read_csv('./ed_data/diagnosis.csv')\n",
    "edstays_df = pd.read_csv('./ed_data/edstays.csv')\n",
    "medrecon_df = pd.read_csv('./ed_data/medrecon.csv')\n",
    "pyxis_df = pd.read_csv('./ed_data/pyxis.csv')\n",
    "triage_df = pd.read_csv('./ed_data/triage.csv')\n",
    "vitalsign_df = pd.read_csv('./ed_data/vitalsign.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6efed4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>seq_num</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>icd_version</th>\n",
       "      <th>icd_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>1</td>\n",
       "      <td>4589</td>\n",
       "      <td>9</td>\n",
       "      <td>HYPOTENSION NOS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2</td>\n",
       "      <td>07070</td>\n",
       "      <td>9</td>\n",
       "      <td>UNSPECIFIED VIRAL HEPATITIS C WITHOUT HEPATIC ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>3</td>\n",
       "      <td>V08</td>\n",
       "      <td>9</td>\n",
       "      <td>ASYMPTOMATIC HIV INFECTION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>33258284</td>\n",
       "      <td>1</td>\n",
       "      <td>5728</td>\n",
       "      <td>9</td>\n",
       "      <td>OTH SEQUELA, CHR LIV DIS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>33258284</td>\n",
       "      <td>2</td>\n",
       "      <td>78959</td>\n",
       "      <td>9</td>\n",
       "      <td>OTHER ASCITES</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   stay_id  seq_num icd_code  icd_version  \\\n",
       "0    10000032  32952584        1     4589            9   \n",
       "1    10000032  32952584        2    07070            9   \n",
       "2    10000032  32952584        3      V08            9   \n",
       "3    10000032  33258284        1     5728            9   \n",
       "4    10000032  33258284        2    78959            9   \n",
       "\n",
       "                                           icd_title  \n",
       "0                                    HYPOTENSION NOS  \n",
       "1  UNSPECIFIED VIRAL HEPATITIS C WITHOUT HEPATIC ...  \n",
       "2                         ASYMPTOMATIC HIV INFECTION  \n",
       "3                           OTH SEQUELA, CHR LIV DIS  \n",
       "4                                      OTHER ASCITES  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diagnosis_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee460a1-8019-4638-9eaa-d8d516e204b4",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd6b37c9-cd5e-4a2b-8c4f-8766bed7540b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to object dtype\n",
    "def convert_dtype(df, columns, dtype):\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            try:\n",
    "                df[col] = df[col].astype(dtype)\n",
    "            except ValueError as e:\n",
    "                print(f\"Error converting column {col}, {e}\")\n",
    "        else:\n",
    "            print(f\"Column {col} does not exist in {df}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce1a503d-fbc0-40b0-ac4d-21c148a67c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 899050 entries, 0 to 899049\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   subject_id   899050 non-null  int64 \n",
      " 1   stay_id      899050 non-null  int64 \n",
      " 2   seq_num      899050 non-null  int64 \n",
      " 3   icd_code     899050 non-null  object\n",
      " 4   icd_version  899050 non-null  int64 \n",
      " 5   icd_title    899050 non-null  object\n",
      "dtypes: int64(4), object(2)\n",
      "memory usage: 41.2+ MB\n"
     ]
    }
   ],
   "source": [
    "diagnosis_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9712423-1273-458e-8a7c-042895330ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"subject_id\", \"stay_id\", \"icd_version\"]\n",
    "diagnosis_df = convert_dtype(diagnosis_df, columns, object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5bbade5-bea8-4b4b-94a2-f1aa2b3f617a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 899050 entries, 0 to 899049\n",
      "Data columns (total 6 columns):\n",
      " #   Column       Non-Null Count   Dtype \n",
      "---  ------       --------------   ----- \n",
      " 0   subject_id   899050 non-null  object\n",
      " 1   stay_id      899050 non-null  object\n",
      " 2   seq_num      899050 non-null  int64 \n",
      " 3   icd_code     899050 non-null  object\n",
      " 4   icd_version  899050 non-null  object\n",
      " 5   icd_title    899050 non-null  object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 41.2+ MB\n"
     ]
    }
   ],
   "source": [
    "diagnosis_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aac86fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>gender</th>\n",
       "      <th>race</th>\n",
       "      <th>arrival_transport</th>\n",
       "      <th>disposition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>33258284</td>\n",
       "      <td>2180-05-06 19:17:00</td>\n",
       "      <td>2180-05-06 23:30:00</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>AMBULANCE</td>\n",
       "      <td>ADMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>22841357.0</td>\n",
       "      <td>38112554</td>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>AMBULANCE</td>\n",
       "      <td>ADMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>25742920.0</td>\n",
       "      <td>35968195</td>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>AMBULANCE</td>\n",
       "      <td>ADMITTED</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034.0</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 16:24:00</td>\n",
       "      <td>2180-07-23 05:54:00</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>AMBULANCE</td>\n",
       "      <td>HOME</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034.0</td>\n",
       "      <td>39399961</td>\n",
       "      <td>2180-07-23 05:54:00</td>\n",
       "      <td>2180-07-23 14:00:00</td>\n",
       "      <td>F</td>\n",
       "      <td>WHITE</td>\n",
       "      <td>AMBULANCE</td>\n",
       "      <td>ADMITTED</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id     hadm_id   stay_id               intime              outtime  \\\n",
       "0    10000032  22595853.0  33258284  2180-05-06 19:17:00  2180-05-06 23:30:00   \n",
       "1    10000032  22841357.0  38112554  2180-06-26 15:54:00  2180-06-26 21:31:00   \n",
       "2    10000032  25742920.0  35968195  2180-08-05 20:58:00  2180-08-06 01:44:00   \n",
       "3    10000032  29079034.0  32952584  2180-07-22 16:24:00  2180-07-23 05:54:00   \n",
       "4    10000032  29079034.0  39399961  2180-07-23 05:54:00  2180-07-23 14:00:00   \n",
       "\n",
       "  gender   race arrival_transport disposition  \n",
       "0      F  WHITE         AMBULANCE    ADMITTED  \n",
       "1      F  WHITE         AMBULANCE    ADMITTED  \n",
       "2      F  WHITE         AMBULANCE    ADMITTED  \n",
       "3      F  WHITE         AMBULANCE        HOME  \n",
       "4      F  WHITE         AMBULANCE    ADMITTED  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edstays_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29074d74-53e1-4eb2-a384-358f8f133154",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 425087 entries, 0 to 425086\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count   Dtype  \n",
      "---  ------             --------------   -----  \n",
      " 0   subject_id         425087 non-null  int64  \n",
      " 1   hadm_id            203016 non-null  float64\n",
      " 2   stay_id            425087 non-null  int64  \n",
      " 3   intime             425087 non-null  object \n",
      " 4   outtime            425087 non-null  object \n",
      " 5   gender             425087 non-null  object \n",
      " 6   race               425087 non-null  object \n",
      " 7   arrival_transport  425087 non-null  object \n",
      " 8   disposition        425087 non-null  object \n",
      "dtypes: float64(1), int64(2), object(6)\n",
      "memory usage: 29.2+ MB\n"
     ]
    }
   ],
   "source": [
    "edstays_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c70da734-5bf1-4c59-9f91-934e57f12d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"subject_id\", \"hadm_id\", \"stay_id\"]\n",
    "edstays_df = convert_dtype(edstays_df, columns, object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce8165d2-998a-44dc-a076-19c63e46e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "edstays_df[\"intime\"] = pd.to_datetime(edstays_df[\"intime\"])\n",
    "edstays_df[\"outtime\"] = pd.to_datetime(edstays_df[\"outtime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72d3e968-c27f-41ce-9b7c-e105d07e7877",
   "metadata": {},
   "outputs": [],
   "source": [
    "edstays_df = convert_dtype(edstays_df, [\"gender\"], bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "60fa8a63-e5e2-42ec-847f-c4967046d0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 425087 entries, 0 to 425086\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   subject_id         425087 non-null  object        \n",
      " 1   hadm_id            203016 non-null  object        \n",
      " 2   stay_id            425087 non-null  object        \n",
      " 3   intime             425087 non-null  datetime64[ns]\n",
      " 4   outtime            425087 non-null  datetime64[ns]\n",
      " 5   gender             425087 non-null  bool          \n",
      " 6   race               425087 non-null  object        \n",
      " 7   arrival_transport  425087 non-null  object        \n",
      " 8   disposition        425087 non-null  object        \n",
      "dtypes: bool(1), datetime64[ns](2), object(6)\n",
      "memory usage: 26.4+ MB\n"
     ]
    }
   ],
   "source": [
    "edstays_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "39f248c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>name</th>\n",
       "      <th>gsn</th>\n",
       "      <th>ndc</th>\n",
       "      <th>etc_rn</th>\n",
       "      <th>etccode</th>\n",
       "      <th>etcdescription</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 17:26:00</td>\n",
       "      <td>albuterol sulfate</td>\n",
       "      <td>28090</td>\n",
       "      <td>21695042308</td>\n",
       "      <td>1</td>\n",
       "      <td>5970.0</td>\n",
       "      <td>Asthma/COPD Therapy - Beta 2-Adrenergic Agents...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 17:26:00</td>\n",
       "      <td>calcium carbonate</td>\n",
       "      <td>1340</td>\n",
       "      <td>10135021101</td>\n",
       "      <td>1</td>\n",
       "      <td>733.0</td>\n",
       "      <td>Minerals and Electrolytes - Calcium Replacement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 17:26:00</td>\n",
       "      <td>cholecalciferol (vitamin D3)</td>\n",
       "      <td>65241</td>\n",
       "      <td>37205024678</td>\n",
       "      <td>1</td>\n",
       "      <td>670.0</td>\n",
       "      <td>Vitamins - D Derivatives</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 17:26:00</td>\n",
       "      <td>emtricitabine-tenofovir [Truvada]</td>\n",
       "      <td>57883</td>\n",
       "      <td>35356007003</td>\n",
       "      <td>1</td>\n",
       "      <td>5849.0</td>\n",
       "      <td>Antiretroviral - Nucleoside and Nucleotide Ana...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 17:26:00</td>\n",
       "      <td>fluticasone [Flovent HFA]</td>\n",
       "      <td>21251</td>\n",
       "      <td>49999061401</td>\n",
       "      <td>1</td>\n",
       "      <td>371.0</td>\n",
       "      <td>Asthma Therapy - Inhaled Corticosteroids (Gluc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   stay_id            charttime  \\\n",
       "0    10000032  32952584  2180-07-22 17:26:00   \n",
       "1    10000032  32952584  2180-07-22 17:26:00   \n",
       "2    10000032  32952584  2180-07-22 17:26:00   \n",
       "3    10000032  32952584  2180-07-22 17:26:00   \n",
       "4    10000032  32952584  2180-07-22 17:26:00   \n",
       "\n",
       "                                name    gsn          ndc  etc_rn  etccode  \\\n",
       "0                  albuterol sulfate  28090  21695042308       1   5970.0   \n",
       "1                  calcium carbonate   1340  10135021101       1    733.0   \n",
       "2       cholecalciferol (vitamin D3)  65241  37205024678       1    670.0   \n",
       "3  emtricitabine-tenofovir [Truvada]  57883  35356007003       1   5849.0   \n",
       "4          fluticasone [Flovent HFA]  21251  49999061401       1    371.0   \n",
       "\n",
       "                                      etcdescription  \n",
       "0  Asthma/COPD Therapy - Beta 2-Adrenergic Agents...  \n",
       "1    Minerals and Electrolytes - Calcium Replacement  \n",
       "2                           Vitamins - D Derivatives  \n",
       "3  Antiretroviral - Nucleoside and Nucleotide Ana...  \n",
       "4  Asthma Therapy - Inhaled Corticosteroids (Gluc...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medrecon_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f9aebc7d-05c6-42f6-82fb-3a0e8f9fb050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2987342 entries, 0 to 2987341\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   subject_id      int64  \n",
      " 1   stay_id         int64  \n",
      " 2   charttime       object \n",
      " 3   name            object \n",
      " 4   gsn             int64  \n",
      " 5   ndc             int64  \n",
      " 6   etc_rn          int64  \n",
      " 7   etccode         float64\n",
      " 8   etcdescription  object \n",
      "dtypes: float64(1), int64(5), object(3)\n",
      "memory usage: 205.1+ MB\n"
     ]
    }
   ],
   "source": [
    "medrecon_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5f33065-a88c-4476-af7b-b61dd55de8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"subject_id\", \"stay_id\"]\n",
    "medrecon_df = convert_dtype(medrecon_df, columns, object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4209048d-dbee-491f-b882-9ca0ece3f5b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2987342 entries, 0 to 2987341\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Dtype  \n",
      "---  ------          -----  \n",
      " 0   subject_id      object \n",
      " 1   stay_id         object \n",
      " 2   charttime       object \n",
      " 3   name            object \n",
      " 4   gsn             int64  \n",
      " 5   ndc             int64  \n",
      " 6   etc_rn          int64  \n",
      " 7   etccode         float64\n",
      " 8   etcdescription  object \n",
      "dtypes: float64(1), int64(3), object(5)\n",
      "memory usage: 205.1+ MB\n"
     ]
    }
   ],
   "source": [
    "medrecon_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3a785f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>med_rn</th>\n",
       "      <th>name</th>\n",
       "      <th>gsn_rn</th>\n",
       "      <th>gsn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 17:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Albuterol Inhaler</td>\n",
       "      <td>1</td>\n",
       "      <td>5037.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 17:59:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Albuterol Inhaler</td>\n",
       "      <td>2</td>\n",
       "      <td>28090.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>35968195</td>\n",
       "      <td>2180-08-05 22:29:00</td>\n",
       "      <td>1</td>\n",
       "      <td>Morphine</td>\n",
       "      <td>1</td>\n",
       "      <td>4080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>35968195</td>\n",
       "      <td>2180-08-05 22:55:00</td>\n",
       "      <td>2</td>\n",
       "      <td>Donnatol (Elixir)</td>\n",
       "      <td>1</td>\n",
       "      <td>4773.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>35968195</td>\n",
       "      <td>2180-08-05 22:55:00</td>\n",
       "      <td>3</td>\n",
       "      <td>Aluminum-Magnesium Hydrox.-Simet</td>\n",
       "      <td>1</td>\n",
       "      <td>2701.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   stay_id            charttime  med_rn  \\\n",
       "0    10000032  32952584  2180-07-22 17:59:00       1   \n",
       "1    10000032  32952584  2180-07-22 17:59:00       1   \n",
       "2    10000032  35968195  2180-08-05 22:29:00       1   \n",
       "3    10000032  35968195  2180-08-05 22:55:00       2   \n",
       "4    10000032  35968195  2180-08-05 22:55:00       3   \n",
       "\n",
       "                               name  gsn_rn      gsn  \n",
       "0                 Albuterol Inhaler       1   5037.0  \n",
       "1                 Albuterol Inhaler       2  28090.0  \n",
       "2                          Morphine       1   4080.0  \n",
       "3                 Donnatol (Elixir)       1   4773.0  \n",
       "4  Aluminum-Magnesium Hydrox.-Simet       1   2701.0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyxis_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4369e919-034a-4d71-8a92-1c3d7920f439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1586053 entries, 0 to 1586052\n",
      "Data columns (total 7 columns):\n",
      " #   Column      Non-Null Count    Dtype  \n",
      "---  ------      --------------    -----  \n",
      " 0   subject_id  1586053 non-null  int64  \n",
      " 1   stay_id     1586053 non-null  int64  \n",
      " 2   charttime   1586053 non-null  object \n",
      " 3   med_rn      1586053 non-null  int64  \n",
      " 4   name        1586053 non-null  object \n",
      " 5   gsn_rn      1586053 non-null  int64  \n",
      " 6   gsn         1550601 non-null  float64\n",
      "dtypes: float64(1), int64(4), object(2)\n",
      "memory usage: 84.7+ MB\n"
     ]
    }
   ],
   "source": [
    "pyxis_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "478d6c0d-c91d-4186-9855-e7646a7b01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also, might not be relevant for the analysis as we want to use data in the first few hours\n",
    "columns = [\"subject_id\", \"stay_id\"]\n",
    "pyxis_df = convert_dtype(pyxis_df, columns, object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04949f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>pain</th>\n",
       "      <th>acuity</th>\n",
       "      <th>chiefcomplaint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>97.8</td>\n",
       "      <td>87.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Hypotension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>33258284</td>\n",
       "      <td>98.4</td>\n",
       "      <td>70.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Abd pain, Abdominal distention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>35968195</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>n/v/d, Abd pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>38112554</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Abdominal distention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>39399961</td>\n",
       "      <td>98.7</td>\n",
       "      <td>77.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>13</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Abdominal distention, Abd pain, LETHAGIC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   stay_id  temperature  heartrate  resprate  o2sat    sbp   dbp  \\\n",
       "0    10000032  32952584         97.8       87.0      14.0   97.0   71.0  43.0   \n",
       "1    10000032  33258284         98.4       70.0      16.0   97.0  106.0  63.0   \n",
       "2    10000032  35968195         99.4      105.0      18.0   96.0  106.0  57.0   \n",
       "3    10000032  38112554         98.9       88.0      18.0   97.0  116.0  88.0   \n",
       "4    10000032  39399961         98.7       77.0      16.0   98.0   96.0  50.0   \n",
       "\n",
       "  pain  acuity                            chiefcomplaint  \n",
       "0    7     2.0                               Hypotension  \n",
       "1    0     3.0            Abd pain, Abdominal distention  \n",
       "2   10     3.0                           n/v/d, Abd pain  \n",
       "3   10     3.0                      Abdominal distention  \n",
       "4   13     2.0  Abdominal distention, Abd pain, LETHAGIC  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "55b98241-785d-40fb-abff-29ba6f259fb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 425087 entries, 0 to 425086\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   subject_id      425087 non-null  int64  \n",
      " 1   stay_id         425087 non-null  int64  \n",
      " 2   temperature     401672 non-null  float64\n",
      " 3   heartrate       407997 non-null  float64\n",
      " 4   resprate        404734 non-null  float64\n",
      " 5   o2sat           404491 non-null  float64\n",
      " 6   sbp             406796 non-null  float64\n",
      " 7   dbp             405996 non-null  float64\n",
      " 8   pain            412154 non-null  object \n",
      " 9   acuity          418100 non-null  float64\n",
      " 10  chiefcomplaint  425064 non-null  object \n",
      "dtypes: float64(7), int64(2), object(2)\n",
      "memory usage: 35.7+ MB\n"
     ]
    }
   ],
   "source": [
    "triage_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "892254d1-c70d-43dd-815a-d31daf641c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"subject_id\", \"stay_id\"]\n",
    "triage_df = convert_dtype(triage_df, columns, object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49973c62-c1f3-43a8-a668-8333d44ccfd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pain\n",
       "0                 140719\n",
       "8                  41730\n",
       "10                 40914\n",
       "7                  31423\n",
       "5                  29210\n",
       "                   ...  \n",
       "almost nothing         1\n",
       "\"up there\"             1\n",
       "achey                  1\n",
       ". 10                   1\n",
       "2-10                   1\n",
       "Name: count, Length: 847, dtype: int64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this column contains mixed dtypes. Retain only numeric values\n",
    "triage_df['pain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e9e0fbef-460a-4ffe-8f54-062eaf591bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert non-numeric values to NaN\n",
    "triage_df['pain'] = pd.to_numeric(triage_df['pain'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7d254fcc-390c-44bc-8da7-1949071e332f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28114"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triage_df['pain'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f5577504-903c-4fbe-a7b2-312fe939b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nans\n",
    "triage_df.dropna(subset=['pain'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88999e6e-5a32-4e1e-9e30-1aed1770756a",
   "metadata": {},
   "outputs": [],
   "source": [
    "triage_df = convert_dtype(triage_df, [\"pain\"], int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4eb3be7f-a4a4-4daf-91ce-0b3b0d6373fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 396973 entries, 0 to 425085\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count   Dtype  \n",
      "---  ------          --------------   -----  \n",
      " 0   subject_id      396973 non-null  object \n",
      " 1   stay_id         396973 non-null  object \n",
      " 2   temperature     388721 non-null  float64\n",
      " 3   heartrate       393692 non-null  float64\n",
      " 4   resprate        390870 non-null  float64\n",
      " 5   o2sat           390641 non-null  float64\n",
      " 6   sbp             392617 non-null  float64\n",
      " 7   dbp             391936 non-null  float64\n",
      " 8   pain            396973 non-null  int64  \n",
      " 9   acuity          396786 non-null  float64\n",
      " 10  chiefcomplaint  396961 non-null  object \n",
      "dtypes: float64(7), int64(1), object(3)\n",
      "memory usage: 36.3+ MB\n"
     ]
    }
   ],
   "source": [
    "triage_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08be9b0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>charttime</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>rhythm</th>\n",
       "      <th>pain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 16:36:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>83.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 16:43:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 16:45:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 17:56:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>84.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>86.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2180-07-22 18:37:00</td>\n",
       "      <td>98.4</td>\n",
       "      <td>86.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   subject_id   stay_id            charttime  temperature  heartrate  \\\n",
       "0    10000032  32952584  2180-07-22 16:36:00          NaN       83.0   \n",
       "1    10000032  32952584  2180-07-22 16:43:00          NaN       85.0   \n",
       "2    10000032  32952584  2180-07-22 16:45:00          NaN       84.0   \n",
       "3    10000032  32952584  2180-07-22 17:56:00          NaN       84.0   \n",
       "4    10000032  32952584  2180-07-22 18:37:00         98.4       86.0   \n",
       "\n",
       "   resprate  o2sat   sbp   dbp rhythm pain  \n",
       "0      24.0   97.0  90.0  51.0    NaN    0  \n",
       "1      22.0   98.0  76.0  39.0    NaN    0  \n",
       "2      22.0   97.0  75.0  39.0    NaN    0  \n",
       "3      20.0   99.0  86.0  51.0    NaN  NaN  \n",
       "4      20.0   98.0  65.0  37.0    NaN  NaN  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitalsign_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b357ed83-a7e0-4d4a-b37b-6cf6da66c015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1564610 entries, 0 to 1564609\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count    Dtype  \n",
      "---  ------       --------------    -----  \n",
      " 0   subject_id   1564610 non-null  int64  \n",
      " 1   stay_id      1564610 non-null  int64  \n",
      " 2   charttime    1564610 non-null  object \n",
      " 3   temperature  999642 non-null   float64\n",
      " 4   heartrate    1494900 non-null  float64\n",
      " 5   resprate     1475217 non-null  float64\n",
      " 6   o2sat        1428774 non-null  float64\n",
      " 7   sbp          1483354 non-null  float64\n",
      " 8   dbp          1483354 non-null  float64\n",
      " 9   rhythm       59650 non-null    object \n",
      " 10  pain         1121344 non-null  object \n",
      "dtypes: float64(6), int64(2), object(3)\n",
      "memory usage: 131.3+ MB\n"
     ]
    }
   ],
   "source": [
    "vitalsign_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c4abace4-722e-46a7-8a6c-fbf3f4bb6945",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\"subject_id\", \"stay_id\"]\n",
    "vitalsign_df = convert_dtype(vitalsign_df, columns, object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0e789474-57f5-40ea-8874-7d9ade682383",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1564610 entries, 0 to 1564609\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count    Dtype  \n",
      "---  ------       --------------    -----  \n",
      " 0   subject_id   1564610 non-null  object \n",
      " 1   stay_id      1564610 non-null  object \n",
      " 2   charttime    1564610 non-null  object \n",
      " 3   temperature  999642 non-null   float64\n",
      " 4   heartrate    1494900 non-null  float64\n",
      " 5   resprate     1475217 non-null  float64\n",
      " 6   o2sat        1428774 non-null  float64\n",
      " 7   sbp          1483354 non-null  float64\n",
      " 8   dbp          1483354 non-null  float64\n",
      " 9   rhythm       59650 non-null    object \n",
      " 10  pain         1121344 non-null  object \n",
      "dtypes: float64(6), object(5)\n",
      "memory usage: 131.3+ MB\n"
     ]
    }
   ],
   "source": [
    "vitalsign_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "812c97f7-3428-40dd-8412-bc0bde99c6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rhythm\n",
       "Sinus Rhythm              18894\n",
       "Normal Sinus Rhythm       12316\n",
       "Atrial Fibrillation        5238\n",
       "Sinus Tachycardia          5133\n",
       "Sinus Bradycardia          3246\n",
       "                          ...  \n",
       "re                            1\n",
       "NSR/afib with freq pac        1\n",
       "snr with freq pacs            1\n",
       "Afib-  demand pacer           1\n",
       "sinus w 2:1 block             1\n",
       "Name: count, Length: 1169, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vitalsign_df['rhythm'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "465dd200-a1f8-4f2e-8789-e24f29d44d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert non-numeric values to NaN\n",
    "vitalsign_df['pain'] = pd.to_numeric(vitalsign_df['pain'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "def92ea1-a9d1-4cf2-8616-d77c8e42118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all Nan. \n",
    "vitalsign_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "488b6078-89f9-48ba-89bc-df6e2a2a993a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vitalsign_df = convert_dtype(vitalsign_df, [\"pain\"], int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f635a61-4f8f-4dc3-bd75-346c7dfcb96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 17570 entries, 128 to 1564381\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count  Dtype  \n",
      "---  ------       --------------  -----  \n",
      " 0   subject_id   17570 non-null  object \n",
      " 1   stay_id      17570 non-null  object \n",
      " 2   charttime    17570 non-null  object \n",
      " 3   temperature  17570 non-null  float64\n",
      " 4   heartrate    17570 non-null  float64\n",
      " 5   resprate     17570 non-null  float64\n",
      " 6   o2sat        17570 non-null  float64\n",
      " 7   sbp          17570 non-null  float64\n",
      " 8   dbp          17570 non-null  float64\n",
      " 9   rhythm       17570 non-null  object \n",
      " 10  pain         17570 non-null  int64  \n",
      "dtypes: float64(6), int64(1), object(4)\n",
      "memory usage: 1.6+ MB\n"
     ]
    }
   ],
   "source": [
    "# Might have to reconsider removing all Nans or just impute missing values.\n",
    "# Also, might not be relevant for the analysis as we want to use data in the first few hours\n",
    "vitalsign_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "54d42cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject_id</th>\n",
       "      <th>stay_id</th>\n",
       "      <th>seq_num</th>\n",
       "      <th>icd_code</th>\n",
       "      <th>icd_version</th>\n",
       "      <th>icd_title</th>\n",
       "      <th>subject_id_ed</th>\n",
       "      <th>hadm_id</th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>...</th>\n",
       "      <th>subject_id_tri</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>pain</th>\n",
       "      <th>acuity</th>\n",
       "      <th>chiefcomplaint</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>1</td>\n",
       "      <td>4589</td>\n",
       "      <td>9</td>\n",
       "      <td>HYPOTENSION NOS</td>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034.0</td>\n",
       "      <td>2180-07-22 16:24:00</td>\n",
       "      <td>2180-07-23 05:54:00</td>\n",
       "      <td>...</td>\n",
       "      <td>10000032</td>\n",
       "      <td>97.8</td>\n",
       "      <td>87.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Hypotension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>2</td>\n",
       "      <td>07070</td>\n",
       "      <td>9</td>\n",
       "      <td>UNSPECIFIED VIRAL HEPATITIS C WITHOUT HEPATIC ...</td>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034.0</td>\n",
       "      <td>2180-07-22 16:24:00</td>\n",
       "      <td>2180-07-23 05:54:00</td>\n",
       "      <td>...</td>\n",
       "      <td>10000032</td>\n",
       "      <td>97.8</td>\n",
       "      <td>87.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Hypotension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000032</td>\n",
       "      <td>32952584</td>\n",
       "      <td>3</td>\n",
       "      <td>V08</td>\n",
       "      <td>9</td>\n",
       "      <td>ASYMPTOMATIC HIV INFECTION</td>\n",
       "      <td>10000032</td>\n",
       "      <td>29079034.0</td>\n",
       "      <td>2180-07-22 16:24:00</td>\n",
       "      <td>2180-07-23 05:54:00</td>\n",
       "      <td>...</td>\n",
       "      <td>10000032</td>\n",
       "      <td>97.8</td>\n",
       "      <td>87.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>71.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Hypotension</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000032</td>\n",
       "      <td>33258284</td>\n",
       "      <td>1</td>\n",
       "      <td>5728</td>\n",
       "      <td>9</td>\n",
       "      <td>OTH SEQUELA, CHR LIV DIS</td>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>2180-05-06 19:17:00</td>\n",
       "      <td>2180-05-06 23:30:00</td>\n",
       "      <td>...</td>\n",
       "      <td>10000032</td>\n",
       "      <td>98.4</td>\n",
       "      <td>70.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Abd pain, Abdominal distention</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000032</td>\n",
       "      <td>33258284</td>\n",
       "      <td>2</td>\n",
       "      <td>78959</td>\n",
       "      <td>9</td>\n",
       "      <td>OTHER ASCITES</td>\n",
       "      <td>10000032</td>\n",
       "      <td>22595853.0</td>\n",
       "      <td>2180-05-06 19:17:00</td>\n",
       "      <td>2180-05-06 23:30:00</td>\n",
       "      <td>...</td>\n",
       "      <td>10000032</td>\n",
       "      <td>98.4</td>\n",
       "      <td>70.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>63.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Abd pain, Abdominal distention</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  subject_id   stay_id  seq_num icd_code icd_version  \\\n",
       "0   10000032  32952584        1     4589           9   \n",
       "1   10000032  32952584        2    07070           9   \n",
       "2   10000032  32952584        3      V08           9   \n",
       "3   10000032  33258284        1     5728           9   \n",
       "4   10000032  33258284        2    78959           9   \n",
       "\n",
       "                                           icd_title subject_id_ed  \\\n",
       "0                                    HYPOTENSION NOS      10000032   \n",
       "1  UNSPECIFIED VIRAL HEPATITIS C WITHOUT HEPATIC ...      10000032   \n",
       "2                         ASYMPTOMATIC HIV INFECTION      10000032   \n",
       "3                           OTH SEQUELA, CHR LIV DIS      10000032   \n",
       "4                                      OTHER ASCITES      10000032   \n",
       "\n",
       "      hadm_id              intime             outtime  ...  subject_id_tri  \\\n",
       "0  29079034.0 2180-07-22 16:24:00 2180-07-23 05:54:00  ...        10000032   \n",
       "1  29079034.0 2180-07-22 16:24:00 2180-07-23 05:54:00  ...        10000032   \n",
       "2  29079034.0 2180-07-22 16:24:00 2180-07-23 05:54:00  ...        10000032   \n",
       "3  22595853.0 2180-05-06 19:17:00 2180-05-06 23:30:00  ...        10000032   \n",
       "4  22595853.0 2180-05-06 19:17:00 2180-05-06 23:30:00  ...        10000032   \n",
       "\n",
       "  temperature heartrate resprate o2sat    sbp   dbp  pain  acuity  \\\n",
       "0        97.8      87.0     14.0  97.0   71.0  43.0   7.0     2.0   \n",
       "1        97.8      87.0     14.0  97.0   71.0  43.0   7.0     2.0   \n",
       "2        97.8      87.0     14.0  97.0   71.0  43.0   7.0     2.0   \n",
       "3        98.4      70.0     16.0  97.0  106.0  63.0   0.0     3.0   \n",
       "4        98.4      70.0     16.0  97.0  106.0  63.0   0.0     3.0   \n",
       "\n",
       "                   chiefcomplaint  \n",
       "0                     Hypotension  \n",
       "1                     Hypotension  \n",
       "2                     Hypotension  \n",
       "3  Abd pain, Abdominal distention  \n",
       "4  Abd pain, Abdominal distention  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Merge all relevant dfs for analysis\n",
    "df_combined = diagnosis_df.merge(edstays_df, on='stay_id', how='left', suffixes=('', '_ed'))\n",
    "df_combined = df_combined.merge(triage_df, on='stay_id', how='left', suffixes=('', '_tri'))\n",
    "\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c391c5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 899050 entries, 0 to 899049\n",
      "Data columns (total 24 columns):\n",
      " #   Column             Non-Null Count   Dtype         \n",
      "---  ------             --------------   -----         \n",
      " 0   subject_id         899050 non-null  object        \n",
      " 1   stay_id            899050 non-null  object        \n",
      " 2   seq_num            899050 non-null  int64         \n",
      " 3   icd_code           899050 non-null  object        \n",
      " 4   icd_version        899050 non-null  object        \n",
      " 5   icd_title          899050 non-null  object        \n",
      " 6   subject_id_ed      899050 non-null  object        \n",
      " 7   hadm_id            421589 non-null  object        \n",
      " 8   intime             899050 non-null  datetime64[ns]\n",
      " 9   outtime            899050 non-null  datetime64[ns]\n",
      " 10  gender             899050 non-null  bool          \n",
      " 11  race               899050 non-null  object        \n",
      " 12  arrival_transport  899050 non-null  object        \n",
      " 13  disposition        899050 non-null  object        \n",
      " 14  subject_id_tri     836429 non-null  object        \n",
      " 15  temperature        818756 non-null  float64       \n",
      " 16  heartrate          829567 non-null  float64       \n",
      " 17  resprate           823395 non-null  float64       \n",
      " 18  o2sat              823007 non-null  float64       \n",
      " 19  sbp                827492 non-null  float64       \n",
      " 20  dbp                826010 non-null  float64       \n",
      " 21  pain               836429 non-null  float64       \n",
      " 22  acuity             836060 non-null  float64       \n",
      " 23  chiefcomplaint     836412 non-null  object        \n",
      "dtypes: bool(1), datetime64[ns](2), float64(8), int64(1), object(12)\n",
      "memory usage: 158.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "addb11fe-d04b-446a-a62b-bd5f73066970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chiefcomplaint\n",
       "Chest pain                                    20474\n",
       "Abd pain                                      17928\n",
       "s/p Fall                                      14226\n",
       "SI                                            10950\n",
       "Dyspnea                                       10202\n",
       "                                              ...  \n",
       "N/V, Hematemesis, Hematemesis                     1\n",
       "Dyspnea, Left sided chest pain, Hemoptysis        1\n",
       "L Flank pain, gtube eval                          1\n",
       "N/V/D  ABD  CRAMPS                                1\n",
       "PAIN ELBOW ANKLE                                  1\n",
       "Name: count, Length: 57060, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined[\"chiefcomplaint\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3d4e45c9-fa20-43e8-bfc6-ce4e95fc6acc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "icd_title\n",
       "HYPERTENSION NOS                                         26816\n",
       "Essential (primary) hypertension                         21264\n",
       "Chest pain, unspecified                                  13016\n",
       "CHEST PAIN NOS                                           12398\n",
       "DIABETES UNCOMPL ADULT                                   12026\n",
       "                                                         ...  \n",
       "PEMPHIGUS                                                    1\n",
       "Burn of third degree of upper back, initial encounter        1\n",
       "Pemphigus vulgaris                                           1\n",
       "Pnctr w fb of r mid finger w/o damage to nail, init          1\n",
       "Carbuncle of face                                            1\n",
       "Name: count, Length: 13172, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined[\"icd_title\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "500fc436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "columns_to_drop = [\"seq_num\", \"subject_id_ed\", \"hadm_id\", \"subject_id_tri\", \"subject_id\", \"stay_id\", \"icd_code\", \"chiefcomplaint\",\"icd_version\", \"race\", \n",
    "                   \"arrival_transport\", \"disposition\", \"icd_title\"]\n",
    "df_combined.drop(columns_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ad3b4dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 899050 entries, 0 to 899049\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   intime       899050 non-null  datetime64[ns]\n",
      " 1   outtime      899050 non-null  datetime64[ns]\n",
      " 2   gender       899050 non-null  bool          \n",
      " 3   temperature  818756 non-null  float64       \n",
      " 4   heartrate    829567 non-null  float64       \n",
      " 5   resprate     823395 non-null  float64       \n",
      " 6   o2sat        823007 non-null  float64       \n",
      " 7   sbp          827492 non-null  float64       \n",
      " 8   dbp          826010 non-null  float64       \n",
      " 9   pain         836429 non-null  float64       \n",
      " 10  acuity       836060 non-null  float64       \n",
      "dtypes: bool(1), datetime64[ns](2), float64(8)\n",
      "memory usage: 69.4 MB\n"
     ]
    }
   ],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bc5dd68c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "intime             0\n",
       "outtime            0\n",
       "gender             0\n",
       "temperature    80294\n",
       "heartrate      69483\n",
       "resprate       75655\n",
       "o2sat          76043\n",
       "sbp            71558\n",
       "dbp            73040\n",
       "pain           62621\n",
       "acuity         62990\n",
       "dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count total Nan values\n",
    "df_combined.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505634f8-45d2-42d7-ae1f-4ff537fbf823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40c2be84-3dd0-43ba-ab1e-dfd93d7bd79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "baf7d1d9-dbed-4df8-9361-4fe31efa6a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 800537 entries, 0 to 899048\n",
      "Data columns (total 11 columns):\n",
      " #   Column       Non-Null Count   Dtype         \n",
      "---  ------       --------------   -----         \n",
      " 0   intime       800537 non-null  datetime64[ns]\n",
      " 1   outtime      800537 non-null  datetime64[ns]\n",
      " 2   gender       800537 non-null  bool          \n",
      " 3   temperature  800537 non-null  float64       \n",
      " 4   heartrate    800537 non-null  float64       \n",
      " 5   resprate     800537 non-null  float64       \n",
      " 6   o2sat        800537 non-null  float64       \n",
      " 7   sbp          800537 non-null  float64       \n",
      " 8   dbp          800537 non-null  float64       \n",
      " 9   pain         800537 non-null  float64       \n",
      " 10  acuity       800537 non-null  float64       \n",
      "dtypes: bool(1), datetime64[ns](2), float64(8)\n",
      "memory usage: 67.9 MB\n"
     ]
    }
   ],
   "source": [
    "df_combined.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a6ef78f3-71e6-4ed5-a413-f29b3c1b5cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>pain</th>\n",
       "      <th>acuity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>800537</td>\n",
       "      <td>800537</td>\n",
       "      <td>800537.000000</td>\n",
       "      <td>800537.000000</td>\n",
       "      <td>800537.000000</td>\n",
       "      <td>800537.000000</td>\n",
       "      <td>800537.000000</td>\n",
       "      <td>800537.000000</td>\n",
       "      <td>800537.000000</td>\n",
       "      <td>800537.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2156-10-25 02:39:01.582563328</td>\n",
       "      <td>2156-10-25 10:11:56.708382720</td>\n",
       "      <td>97.999221</td>\n",
       "      <td>84.732226</td>\n",
       "      <td>17.531265</td>\n",
       "      <td>98.475814</td>\n",
       "      <td>136.094094</td>\n",
       "      <td>80.768141</td>\n",
       "      <td>4.457219</td>\n",
       "      <td>2.659436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2110-01-11 01:45:00</td>\n",
       "      <td>2110-01-11 07:04:00</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2136-11-09 08:57:00</td>\n",
       "      <td>2136-11-09 15:50:00</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2156-10-09 14:24:00</td>\n",
       "      <td>2156-10-09 19:03:00</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2176-12-14 15:04:00</td>\n",
       "      <td>2176-12-14 17:17:00</td>\n",
       "      <td>98.500000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2212-04-05 23:23:00</td>\n",
       "      <td>2212-04-06 14:20:00</td>\n",
       "      <td>986.000000</td>\n",
       "      <td>1109.000000</td>\n",
       "      <td>1820.000000</td>\n",
       "      <td>9322.000000</td>\n",
       "      <td>19734.000000</td>\n",
       "      <td>661672.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.873607</td>\n",
       "      <td>17.755306</td>\n",
       "      <td>6.748287</td>\n",
       "      <td>22.010889</td>\n",
       "      <td>31.651863</td>\n",
       "      <td>1054.185858</td>\n",
       "      <td>4.086526</td>\n",
       "      <td>0.662736</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              intime                        outtime  \\\n",
       "count                         800537                         800537   \n",
       "mean   2156-10-25 02:39:01.582563328  2156-10-25 10:11:56.708382720   \n",
       "min              2110-01-11 01:45:00            2110-01-11 07:04:00   \n",
       "25%              2136-11-09 08:57:00            2136-11-09 15:50:00   \n",
       "50%              2156-10-09 14:24:00            2156-10-09 19:03:00   \n",
       "75%              2176-12-14 15:04:00            2176-12-14 17:17:00   \n",
       "max              2212-04-05 23:23:00            2212-04-06 14:20:00   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "         temperature      heartrate       resprate          o2sat  \\\n",
       "count  800537.000000  800537.000000  800537.000000  800537.000000   \n",
       "mean       97.999221      84.732226      17.531265      98.475814   \n",
       "min         0.100000       1.000000       0.000000       0.000000   \n",
       "25%        97.500000      72.000000      16.000000      97.000000   \n",
       "50%        98.000000      83.000000      18.000000      99.000000   \n",
       "75%        98.500000      96.000000      18.000000     100.000000   \n",
       "max       986.000000    1109.000000    1820.000000    9322.000000   \n",
       "std         3.873607      17.755306       6.748287      22.010889   \n",
       "\n",
       "                 sbp            dbp           pain         acuity  \n",
       "count  800537.000000  800537.000000  800537.000000  800537.000000  \n",
       "mean      136.094094      80.768141       4.457219       2.659436  \n",
       "min         5.000000       0.000000      -1.000000       1.000000  \n",
       "25%       120.000000      68.000000       0.000000       2.000000  \n",
       "50%       134.000000      77.000000       5.000000       3.000000  \n",
       "75%       150.000000      87.000000       8.000000       3.000000  \n",
       "max     19734.000000  661672.000000     134.000000       5.000000  \n",
       "std        31.651863    1054.185858       4.086526       0.662736  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "db702516-6a96-4f12-9cf9-5dc96472e724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The min and max value of some columns like temperature, heartrate, etc are extreme.\n",
    "# bounding them within the 0.001 to 0.99 quantile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6d721366-dd9f-4f6f-a754-d1aaf4ddf7b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': 37.1,\n",
       " 'heartrate': 40.0,\n",
       " 'resprate': 12.0,\n",
       " 'o2sat': 85.0,\n",
       " 'sbp': 73.0,\n",
       " 'dbp': 31.0,\n",
       " 'pain': 0.0}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns = [\"temperature\", \"heartrate\", \"resprate\", \"o2sat\", \"sbp\", \"dbp\", \"pain\"]\n",
    "min_values = {col: df_combined[col].quantile(0.001) for col in columns}\n",
    "min_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cb35aa48-712f-466e-804c-77cb9c0bef2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'temperature': 101.2,\n",
       " 'heartrate': 130.0,\n",
       " 'resprate': 24.0,\n",
       " 'o2sat': 100.0,\n",
       " 'sbp': 200.0,\n",
       " 'dbp': 116.0,\n",
       " 'pain': 13.0}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_values = {col: df_combined[col].quantile(0.99) for col in columns}\n",
    "max_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "101830c3-d43c-450b-8dd4-e1e4815b7c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter values lower than 0.001 quantile\n",
    "for col, val in min_values.items():\n",
    "    df_combined = df_combined[df_combined[col] > val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3f5ea3d6-bdd3-4c70-988c-ff19e5150ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter values greater than 0.99 quantile\n",
    "for col, val in max_values.items():\n",
    "    df_combined = df_combined[df_combined[col] <= val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6192e43-c24f-4616-b4c7-4f357af8bc70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>pain</th>\n",
       "      <th>acuity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>494530</td>\n",
       "      <td>494530</td>\n",
       "      <td>494530.000000</td>\n",
       "      <td>494530.000000</td>\n",
       "      <td>494530.000000</td>\n",
       "      <td>494530.000000</td>\n",
       "      <td>494530.000000</td>\n",
       "      <td>494530.000000</td>\n",
       "      <td>494530.000000</td>\n",
       "      <td>494530.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2156-10-23 09:58:47.133846528</td>\n",
       "      <td>2156-10-23 17:11:16.428276736</td>\n",
       "      <td>98.035326</td>\n",
       "      <td>84.129580</td>\n",
       "      <td>17.391527</td>\n",
       "      <td>98.542100</td>\n",
       "      <td>135.616602</td>\n",
       "      <td>77.675502</td>\n",
       "      <td>6.811063</td>\n",
       "      <td>2.778088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2110-01-11 01:45:00</td>\n",
       "      <td>2110-01-11 07:04:00</td>\n",
       "      <td>37.200000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>86.000000</td>\n",
       "      <td>74.000000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2136-10-16 08:31:00</td>\n",
       "      <td>2136-10-16 11:54:00</td>\n",
       "      <td>97.500000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>121.000000</td>\n",
       "      <td>68.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2156-10-06 09:37:00</td>\n",
       "      <td>2156-10-06 11:54:27.000000512</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>83.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2177-01-02 18:09:30</td>\n",
       "      <td>2177-01-03 14:50:00</td>\n",
       "      <td>98.500000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>149.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>3.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2212-01-27 20:34:00</td>\n",
       "      <td>2212-01-28 13:17:00</td>\n",
       "      <td>101.200000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>24.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.123250</td>\n",
       "      <td>15.932991</td>\n",
       "      <td>1.723356</td>\n",
       "      <td>1.787543</td>\n",
       "      <td>20.785362</td>\n",
       "      <td>13.748121</td>\n",
       "      <td>2.819430</td>\n",
       "      <td>0.618439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              intime                        outtime  \\\n",
       "count                         494530                         494530   \n",
       "mean   2156-10-23 09:58:47.133846528  2156-10-23 17:11:16.428276736   \n",
       "min              2110-01-11 01:45:00            2110-01-11 07:04:00   \n",
       "25%              2136-10-16 08:31:00            2136-10-16 11:54:00   \n",
       "50%              2156-10-06 09:37:00  2156-10-06 11:54:27.000000512   \n",
       "75%              2177-01-02 18:09:30            2177-01-03 14:50:00   \n",
       "max              2212-01-27 20:34:00            2212-01-28 13:17:00   \n",
       "std                              NaN                            NaN   \n",
       "\n",
       "         temperature      heartrate       resprate          o2sat  \\\n",
       "count  494530.000000  494530.000000  494530.000000  494530.000000   \n",
       "mean       98.035326      84.129580      17.391527      98.542100   \n",
       "min        37.200000      41.000000      13.000000      86.000000   \n",
       "25%        97.500000      72.000000      16.000000      98.000000   \n",
       "50%        98.000000      83.000000      18.000000      99.000000   \n",
       "75%        98.500000      95.000000      18.000000     100.000000   \n",
       "max       101.200000     130.000000      24.000000     100.000000   \n",
       "std         1.123250      15.932991       1.723356       1.787543   \n",
       "\n",
       "                 sbp            dbp           pain         acuity  \n",
       "count  494530.000000  494530.000000  494530.000000  494530.000000  \n",
       "mean      135.616602      77.675502       6.811063       2.778088  \n",
       "min        74.000000      32.000000       1.000000       1.000000  \n",
       "25%       121.000000      68.000000       5.000000       2.000000  \n",
       "50%       134.000000      78.000000       7.000000       3.000000  \n",
       "75%       149.000000      87.000000       9.000000       3.000000  \n",
       "max       200.000000     116.000000      13.000000       5.000000  \n",
       "std        20.785362      13.748121       2.819430       0.618439  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b1c4bb-b4dd-4db9-af98-fa91b863bc0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "29c232de-6295-4091-9a6b-3d4bdee690cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>gender</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>pain</th>\n",
       "      <th>acuity</th>\n",
       "      <th>LOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.616667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                intime             outtime  gender  temperature  heartrate  \\\n",
       "7  2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4      105.0   \n",
       "8  2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4      105.0   \n",
       "9  2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4      105.0   \n",
       "10 2180-06-26 15:54:00 2180-06-26 21:31:00    True         98.9       88.0   \n",
       "11 2180-06-26 15:54:00 2180-06-26 21:31:00    True         98.9       88.0   \n",
       "\n",
       "    resprate  o2sat    sbp   dbp  pain  acuity       LOS  \n",
       "7       18.0   96.0  106.0  57.0  10.0     3.0  4.766667  \n",
       "8       18.0   96.0  106.0  57.0  10.0     3.0  4.766667  \n",
       "9       18.0   96.0  106.0  57.0  10.0     3.0  4.766667  \n",
       "10      18.0   97.0  116.0  88.0  10.0     3.0  5.616667  \n",
       "11      18.0   97.0  116.0  88.0  10.0     3.0  5.616667  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined['LOS'] = (df_combined[\"outtime\"] - df_combined[\"intime\"]).dt.total_seconds() / 3600\n",
    "df_combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b5e0481-9575-49ea-a2b0-61c6819447b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>gender</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>pain</th>\n",
       "      <th>acuity</th>\n",
       "      <th>LOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899037</th>\n",
       "      <td>2144-03-22 14:27:00</td>\n",
       "      <td>2144-03-22 18:47:00</td>\n",
       "      <td>True</td>\n",
       "      <td>97.7</td>\n",
       "      <td>89.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899038</th>\n",
       "      <td>2144-03-22 14:27:00</td>\n",
       "      <td>2144-03-22 18:47:00</td>\n",
       "      <td>True</td>\n",
       "      <td>97.7</td>\n",
       "      <td>89.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899044</th>\n",
       "      <td>2119-07-09 17:38:00</td>\n",
       "      <td>2119-07-10 00:04:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899046</th>\n",
       "      <td>2149-01-08 09:11:00</td>\n",
       "      <td>2149-01-08 18:12:00</td>\n",
       "      <td>True</td>\n",
       "      <td>96.6</td>\n",
       "      <td>112.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899047</th>\n",
       "      <td>2149-01-08 09:11:00</td>\n",
       "      <td>2149-01-08 18:12:00</td>\n",
       "      <td>True</td>\n",
       "      <td>96.6</td>\n",
       "      <td>112.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.016667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479991 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    intime             outtime  gender  temperature  \\\n",
       "7      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "8      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "9      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "10     2180-06-26 15:54:00 2180-06-26 21:31:00    True         98.9   \n",
       "11     2180-06-26 15:54:00 2180-06-26 21:31:00    True         98.9   \n",
       "...                    ...                 ...     ...          ...   \n",
       "899037 2144-03-22 14:27:00 2144-03-22 18:47:00    True         97.7   \n",
       "899038 2144-03-22 14:27:00 2144-03-22 18:47:00    True         97.7   \n",
       "899044 2119-07-09 17:38:00 2119-07-10 00:04:00    True         98.0   \n",
       "899046 2149-01-08 09:11:00 2149-01-08 18:12:00    True         96.6   \n",
       "899047 2149-01-08 09:11:00 2149-01-08 18:12:00    True         96.6   \n",
       "\n",
       "        heartrate  resprate  o2sat    sbp    dbp  pain  acuity       LOS  \n",
       "7           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667  \n",
       "8           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667  \n",
       "9           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667  \n",
       "10           88.0      18.0   97.0  116.0   88.0  10.0     3.0  5.616667  \n",
       "11           88.0      18.0   97.0  116.0   88.0  10.0     3.0  5.616667  \n",
       "...           ...       ...    ...    ...    ...   ...     ...       ...  \n",
       "899037       89.0      22.0  100.0  176.0  109.0  10.0     3.0  4.333333  \n",
       "899038       89.0      22.0  100.0  176.0  109.0  10.0     3.0  4.333333  \n",
       "899044       91.0      16.0   99.0  148.0   90.0   5.0     2.0  6.433333  \n",
       "899046      112.0      18.0  100.0  110.0   82.0   4.0     2.0  9.016667  \n",
       "899047      112.0      18.0  100.0  110.0   82.0   4.0     2.0  9.016667  \n",
       "\n",
       "[479991 rows x 12 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_df = df_combined[(df_combined['LOS'] <= 24.0)]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7047d72f-ea02-4f3e-9f4d-b02486476d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>gender</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>pain</th>\n",
       "      <th>acuity</th>\n",
       "      <th>LOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.616667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899037</th>\n",
       "      <td>2144-03-22 14:27:00</td>\n",
       "      <td>2144-03-22 18:47:00</td>\n",
       "      <td>True</td>\n",
       "      <td>97.7</td>\n",
       "      <td>89.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899038</th>\n",
       "      <td>2144-03-22 14:27:00</td>\n",
       "      <td>2144-03-22 18:47:00</td>\n",
       "      <td>True</td>\n",
       "      <td>97.7</td>\n",
       "      <td>89.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899044</th>\n",
       "      <td>2119-07-09 17:38:00</td>\n",
       "      <td>2119-07-10 00:04:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899046</th>\n",
       "      <td>2149-01-08 09:11:00</td>\n",
       "      <td>2149-01-08 18:12:00</td>\n",
       "      <td>True</td>\n",
       "      <td>96.6</td>\n",
       "      <td>112.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.016667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899047</th>\n",
       "      <td>2149-01-08 09:11:00</td>\n",
       "      <td>2149-01-08 18:12:00</td>\n",
       "      <td>True</td>\n",
       "      <td>96.6</td>\n",
       "      <td>112.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.016667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479973 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    intime             outtime  gender  temperature  \\\n",
       "7      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "8      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "9      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "10     2180-06-26 15:54:00 2180-06-26 21:31:00    True         98.9   \n",
       "11     2180-06-26 15:54:00 2180-06-26 21:31:00    True         98.9   \n",
       "...                    ...                 ...     ...          ...   \n",
       "899037 2144-03-22 14:27:00 2144-03-22 18:47:00    True         97.7   \n",
       "899038 2144-03-22 14:27:00 2144-03-22 18:47:00    True         97.7   \n",
       "899044 2119-07-09 17:38:00 2119-07-10 00:04:00    True         98.0   \n",
       "899046 2149-01-08 09:11:00 2149-01-08 18:12:00    True         96.6   \n",
       "899047 2149-01-08 09:11:00 2149-01-08 18:12:00    True         96.6   \n",
       "\n",
       "        heartrate  resprate  o2sat    sbp    dbp  pain  acuity       LOS  \n",
       "7           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667  \n",
       "8           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667  \n",
       "9           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667  \n",
       "10           88.0      18.0   97.0  116.0   88.0  10.0     3.0  5.616667  \n",
       "11           88.0      18.0   97.0  116.0   88.0  10.0     3.0  5.616667  \n",
       "...           ...       ...    ...    ...    ...   ...     ...       ...  \n",
       "899037       89.0      22.0  100.0  176.0  109.0  10.0     3.0  4.333333  \n",
       "899038       89.0      22.0  100.0  176.0  109.0  10.0     3.0  4.333333  \n",
       "899044       91.0      16.0   99.0  148.0   90.0   5.0     2.0  6.433333  \n",
       "899046      112.0      18.0  100.0  110.0   82.0   4.0     2.0  9.016667  \n",
       "899047      112.0      18.0  100.0  110.0   82.0   4.0     2.0  9.016667  \n",
       "\n",
       "[479973 rows x 12 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dff = filtered_df[filtered_df['LOS'] > 0]\n",
    "\n",
    "filtered_dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fe709206-f5f3-495c-b04b-d4bebf315b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dff['LOS'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "39bc9fa8-6f86-4239-8af9-87aeb97eea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 4.1, 6.9, 24]\n",
    "\n",
    "# Define labels\n",
    "# labels = ['short', 'Mid', 'Long']\n",
    "labels = [1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5e292e22-f6e6-4a0a-9a4b-dca9d2f62d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1548024/3257291150.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_dff['LOS_CAT'] = pd.cut(filtered_dff['LOS'], bins=bins, labels=labels, right=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>gender</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>pain</th>\n",
       "      <th>acuity</th>\n",
       "      <th>LOS</th>\n",
       "      <th>LOS_CAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.616667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.616667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899037</th>\n",
       "      <td>2144-03-22 14:27:00</td>\n",
       "      <td>2144-03-22 18:47:00</td>\n",
       "      <td>True</td>\n",
       "      <td>97.7</td>\n",
       "      <td>89.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899038</th>\n",
       "      <td>2144-03-22 14:27:00</td>\n",
       "      <td>2144-03-22 18:47:00</td>\n",
       "      <td>True</td>\n",
       "      <td>97.7</td>\n",
       "      <td>89.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899044</th>\n",
       "      <td>2119-07-09 17:38:00</td>\n",
       "      <td>2119-07-10 00:04:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.433333</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899046</th>\n",
       "      <td>2149-01-08 09:11:00</td>\n",
       "      <td>2149-01-08 18:12:00</td>\n",
       "      <td>True</td>\n",
       "      <td>96.6</td>\n",
       "      <td>112.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.016667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>899047</th>\n",
       "      <td>2149-01-08 09:11:00</td>\n",
       "      <td>2149-01-08 18:12:00</td>\n",
       "      <td>True</td>\n",
       "      <td>96.6</td>\n",
       "      <td>112.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.016667</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479973 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    intime             outtime  gender  temperature  \\\n",
       "7      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "8      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "9      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "10     2180-06-26 15:54:00 2180-06-26 21:31:00    True         98.9   \n",
       "11     2180-06-26 15:54:00 2180-06-26 21:31:00    True         98.9   \n",
       "...                    ...                 ...     ...          ...   \n",
       "899037 2144-03-22 14:27:00 2144-03-22 18:47:00    True         97.7   \n",
       "899038 2144-03-22 14:27:00 2144-03-22 18:47:00    True         97.7   \n",
       "899044 2119-07-09 17:38:00 2119-07-10 00:04:00    True         98.0   \n",
       "899046 2149-01-08 09:11:00 2149-01-08 18:12:00    True         96.6   \n",
       "899047 2149-01-08 09:11:00 2149-01-08 18:12:00    True         96.6   \n",
       "\n",
       "        heartrate  resprate  o2sat    sbp    dbp  pain  acuity       LOS  \\\n",
       "7           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667   \n",
       "8           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667   \n",
       "9           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667   \n",
       "10           88.0      18.0   97.0  116.0   88.0  10.0     3.0  5.616667   \n",
       "11           88.0      18.0   97.0  116.0   88.0  10.0     3.0  5.616667   \n",
       "...           ...       ...    ...    ...    ...   ...     ...       ...   \n",
       "899037       89.0      22.0  100.0  176.0  109.0  10.0     3.0  4.333333   \n",
       "899038       89.0      22.0  100.0  176.0  109.0  10.0     3.0  4.333333   \n",
       "899044       91.0      16.0   99.0  148.0   90.0   5.0     2.0  6.433333   \n",
       "899046      112.0      18.0  100.0  110.0   82.0   4.0     2.0  9.016667   \n",
       "899047      112.0      18.0  100.0  110.0   82.0   4.0     2.0  9.016667   \n",
       "\n",
       "       LOS_CAT  \n",
       "7            2  \n",
       "8            2  \n",
       "9            2  \n",
       "10           2  \n",
       "11           2  \n",
       "...        ...  \n",
       "899037       2  \n",
       "899038       2  \n",
       "899044       2  \n",
       "899046       3  \n",
       "899047       3  \n",
       "\n",
       "[479973 rows x 13 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dff['LOS_CAT'] = pd.cut(filtered_dff['LOS'], bins=bins, labels=labels, right=True)\n",
    "filtered_dff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b22bc1d2-dfb7-4248-948a-44f01c60eaa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1548024/4127001170.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_dff['LOS_CAT'].replace(class2idx, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "class2idx = {\n",
    "    1:0,\n",
    "    2:1,\n",
    "    3:2\n",
    "}\n",
    "\n",
    "idx2class = {v: k for k, v in class2idx.items()}\n",
    "\n",
    "filtered_dff['LOS_CAT'].replace(class2idx, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "756fbc53-d5b8-4a0d-b327-cb51999a1068",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOS_CAT\n",
       "1    161897\n",
       "2    160921\n",
       "0    157155\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dff['LOS_CAT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "02c2d06e-a01c-4ad8-8806-0d7637d9b945",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>gender</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>pain</th>\n",
       "      <th>acuity</th>\n",
       "      <th>LOS</th>\n",
       "      <th>LOS_CAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.616667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.616667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479968</th>\n",
       "      <td>2144-03-22 14:27:00</td>\n",
       "      <td>2144-03-22 18:47:00</td>\n",
       "      <td>True</td>\n",
       "      <td>97.7</td>\n",
       "      <td>89.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479969</th>\n",
       "      <td>2144-03-22 14:27:00</td>\n",
       "      <td>2144-03-22 18:47:00</td>\n",
       "      <td>True</td>\n",
       "      <td>97.7</td>\n",
       "      <td>89.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479970</th>\n",
       "      <td>2119-07-09 17:38:00</td>\n",
       "      <td>2119-07-10 00:04:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.433333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479971</th>\n",
       "      <td>2149-01-08 09:11:00</td>\n",
       "      <td>2149-01-08 18:12:00</td>\n",
       "      <td>True</td>\n",
       "      <td>96.6</td>\n",
       "      <td>112.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.016667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479972</th>\n",
       "      <td>2149-01-08 09:11:00</td>\n",
       "      <td>2149-01-08 18:12:00</td>\n",
       "      <td>True</td>\n",
       "      <td>96.6</td>\n",
       "      <td>112.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.016667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479973 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    intime             outtime  gender  temperature  \\\n",
       "0      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "1      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "2      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "3      2180-06-26 15:54:00 2180-06-26 21:31:00    True         98.9   \n",
       "4      2180-06-26 15:54:00 2180-06-26 21:31:00    True         98.9   \n",
       "...                    ...                 ...     ...          ...   \n",
       "479968 2144-03-22 14:27:00 2144-03-22 18:47:00    True         97.7   \n",
       "479969 2144-03-22 14:27:00 2144-03-22 18:47:00    True         97.7   \n",
       "479970 2119-07-09 17:38:00 2119-07-10 00:04:00    True         98.0   \n",
       "479971 2149-01-08 09:11:00 2149-01-08 18:12:00    True         96.6   \n",
       "479972 2149-01-08 09:11:00 2149-01-08 18:12:00    True         96.6   \n",
       "\n",
       "        heartrate  resprate  o2sat    sbp    dbp  pain  acuity       LOS  \\\n",
       "0           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667   \n",
       "1           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667   \n",
       "2           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667   \n",
       "3            88.0      18.0   97.0  116.0   88.0  10.0     3.0  5.616667   \n",
       "4            88.0      18.0   97.0  116.0   88.0  10.0     3.0  5.616667   \n",
       "...           ...       ...    ...    ...    ...   ...     ...       ...   \n",
       "479968       89.0      22.0  100.0  176.0  109.0  10.0     3.0  4.333333   \n",
       "479969       89.0      22.0  100.0  176.0  109.0  10.0     3.0  4.333333   \n",
       "479970       91.0      16.0   99.0  148.0   90.0   5.0     2.0  6.433333   \n",
       "479971      112.0      18.0  100.0  110.0   82.0   4.0     2.0  9.016667   \n",
       "479972      112.0      18.0  100.0  110.0   82.0   4.0     2.0  9.016667   \n",
       "\n",
       "       LOS_CAT  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "...        ...  \n",
       "479968       1  \n",
       "479969       1  \n",
       "479970       1  \n",
       "479971       2  \n",
       "479972       2  \n",
       "\n",
       "[479973 rows x 13 columns]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_df =  filtered_dff.reset_index(drop=True)\n",
    "mimic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d1fbf1c7-65d0-4500-84c7-8289fdb39ef0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016666666666666666"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_df['LOS'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "349cd9e1-081a-4682-89e2-dbe602ed25d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>intime</th>\n",
       "      <th>outtime</th>\n",
       "      <th>gender</th>\n",
       "      <th>temperature</th>\n",
       "      <th>heartrate</th>\n",
       "      <th>resprate</th>\n",
       "      <th>o2sat</th>\n",
       "      <th>sbp</th>\n",
       "      <th>dbp</th>\n",
       "      <th>pain</th>\n",
       "      <th>acuity</th>\n",
       "      <th>LOS</th>\n",
       "      <th>LOS_CAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2180-08-05 20:58:00</td>\n",
       "      <td>2180-08-06 01:44:00</td>\n",
       "      <td>True</td>\n",
       "      <td>99.4</td>\n",
       "      <td>105.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>106.0</td>\n",
       "      <td>57.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.766667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.616667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2180-06-26 15:54:00</td>\n",
       "      <td>2180-06-26 21:31:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.9</td>\n",
       "      <td>88.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>116.0</td>\n",
       "      <td>88.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.616667</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479968</th>\n",
       "      <td>2144-03-22 14:27:00</td>\n",
       "      <td>2144-03-22 18:47:00</td>\n",
       "      <td>True</td>\n",
       "      <td>97.7</td>\n",
       "      <td>89.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479969</th>\n",
       "      <td>2144-03-22 14:27:00</td>\n",
       "      <td>2144-03-22 18:47:00</td>\n",
       "      <td>True</td>\n",
       "      <td>97.7</td>\n",
       "      <td>89.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>109.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479970</th>\n",
       "      <td>2119-07-09 17:38:00</td>\n",
       "      <td>2119-07-10 00:04:00</td>\n",
       "      <td>True</td>\n",
       "      <td>98.0</td>\n",
       "      <td>91.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.433333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479971</th>\n",
       "      <td>2149-01-08 09:11:00</td>\n",
       "      <td>2149-01-08 18:12:00</td>\n",
       "      <td>True</td>\n",
       "      <td>96.6</td>\n",
       "      <td>112.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.016667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>479972</th>\n",
       "      <td>2149-01-08 09:11:00</td>\n",
       "      <td>2149-01-08 18:12:00</td>\n",
       "      <td>True</td>\n",
       "      <td>96.6</td>\n",
       "      <td>112.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9.016667</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>479973 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    intime             outtime  gender  temperature  \\\n",
       "0      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "1      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "2      2180-08-05 20:58:00 2180-08-06 01:44:00    True         99.4   \n",
       "3      2180-06-26 15:54:00 2180-06-26 21:31:00    True         98.9   \n",
       "4      2180-06-26 15:54:00 2180-06-26 21:31:00    True         98.9   \n",
       "...                    ...                 ...     ...          ...   \n",
       "479968 2144-03-22 14:27:00 2144-03-22 18:47:00    True         97.7   \n",
       "479969 2144-03-22 14:27:00 2144-03-22 18:47:00    True         97.7   \n",
       "479970 2119-07-09 17:38:00 2119-07-10 00:04:00    True         98.0   \n",
       "479971 2149-01-08 09:11:00 2149-01-08 18:12:00    True         96.6   \n",
       "479972 2149-01-08 09:11:00 2149-01-08 18:12:00    True         96.6   \n",
       "\n",
       "        heartrate  resprate  o2sat    sbp    dbp  pain  acuity       LOS  \\\n",
       "0           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667   \n",
       "1           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667   \n",
       "2           105.0      18.0   96.0  106.0   57.0  10.0     3.0  4.766667   \n",
       "3            88.0      18.0   97.0  116.0   88.0  10.0     3.0  5.616667   \n",
       "4            88.0      18.0   97.0  116.0   88.0  10.0     3.0  5.616667   \n",
       "...           ...       ...    ...    ...    ...   ...     ...       ...   \n",
       "479968       89.0      22.0  100.0  176.0  109.0  10.0     3.0  4.333333   \n",
       "479969       89.0      22.0  100.0  176.0  109.0  10.0     3.0  4.333333   \n",
       "479970       91.0      16.0   99.0  148.0   90.0   5.0     2.0  6.433333   \n",
       "479971      112.0      18.0  100.0  110.0   82.0   4.0     2.0  9.016667   \n",
       "479972      112.0      18.0  100.0  110.0   82.0   4.0     2.0  9.016667   \n",
       "\n",
       "       LOS_CAT  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "...        ...  \n",
       "479968       1  \n",
       "479969       1  \n",
       "479970       1  \n",
       "479971       2  \n",
       "479972       2  \n",
       "\n",
       "[479973 rows x 13 columns]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "435d0895-9750-4ca6-86a6-e530d5403210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.016666666666666666"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_df['LOS'].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "706d7338-0e29-4d1b-bbcf-631893563105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.0"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_df['LOS'].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "00d9a961-8ddf-4c13-87f1-e1c7e12f74ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LOS_CAT\n",
       "1    161897\n",
       "2    160921\n",
       "0    157155\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_df['LOS_CAT'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "906f9c40-8fd9-470a-9790-9c3b09364cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# might need to hot-encode the categorical features and remove icds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "87933369-f006-4f54-b6e2-f7389c73b51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479973, 13)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mimic_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe4b686-ade7-4678-ba03-1254a3a1d0ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "868961ce-0fb1-4394-b908-95722bb2c8af",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "97460a2e-26f3-47f7-bcaf-b366414166bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_data(col):\n",
    "    mimic_df[col].plot.bar()\n",
    "    plt.xlabel(col)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.title(f\"Distribution of {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "4cc81e0d-66fe-4d55-8c0f-a4c84c318af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Distribution of LOS')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check the data distribution of LOS\n",
    "mimic_df[\"LOS_CAT\"].value_counts(normalize=True).plot.bar()\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"Distribution of LOS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d1ca8-9987-4c29-8bc5-1210f3bb0f9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d0de1-cfee-437d-9f80-7475dcebba15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6bb23bf0-b66c-4c64-a64c-3bdd704eb71d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "005cb044-effd-4696-b341-4878ef6f5117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df into X and Y\n",
    "Y = mimic_df[\"LOS_CAT\"]\n",
    "X = mimic_df.drop([\"LOS\", \"LOS_CAT\", \"outtime\", \"intime\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "43aabbae-7f43-4b0a-bb9d-0c6df60a4b89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479973, 9)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dd59596e-eb6d-4f93-acea-cdd4dd8af3bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(479973,)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c739ca8f-9647-4212-8551-e5e5ad0de911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         1\n",
       "2         1\n",
       "3         1\n",
       "4         1\n",
       "         ..\n",
       "479968    1\n",
       "479969    1\n",
       "479970    1\n",
       "479971    2\n",
       "479972    2\n",
       "Name: LOS_CAT, Length: 479973, dtype: category\n",
       "Categories (3, int64): [0 < 1 < 2]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bde8d5-d558-43e9-a0f4-2fd47b6fd30c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32a12ec-5c8a-4cf9-870b-7ca5d4ecc485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "de9f5c1c-0773-45c8-a2cc-241fe3db0fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into trainval and tes\n",
    "X_trainval, X_test, Y_trainval, Y_test = train_test_split(X, Y, test_size=0.2, shuffle=True, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d644c5af-63bb-453e-afa5-32e0d091c931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train into train-val\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_trainval, Y_trainval, test_size=0.1, shuffle=True, random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "2e5be072-5a4e-4f73-b0cd-01f61fcd79ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a9242abc-3296-493d-a970-f391d285254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "351fa347-1d9b-4c11-86f6-07d848a72cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Documentation says its good to scale both X and Y especially for NNs\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Y_train_scaled = scaler.fit_transform(Y_train.values.reshape(-1,1))\n",
    "# Y_val_scaled = scaler.fit_transform(Y_val.values.reshape(-1,1))\n",
    "# Y_test_scaled = scaler.transform(Y_test.values.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b8e2db5d-327e-4099-aca3-196a5375f30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(345580, 9)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7051027d-5c60-45d5-8939-6f999598ce2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38398, 9)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1e7b791a-a0d7-4724-97a0-28ffd22d5be1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95995, 9)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "26346cdf-31b6-4dd8-9e49-4f571a13b139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95995,)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "cc49f0b9-e30c-4ea1-a266-c43e99143deb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.set_default_device(device)\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "e6e67ee2-3bf2-47df-8ed5-71524aa86d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Convert train data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(np.array(X_train_scaled), dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(np.array(Y_train), dtype=torch.long).to(device)\n",
    "\n",
    "# Convert val data to PyTorch tensors\n",
    "X_val_tensor = torch.tensor(np.array(X_val_scaled), dtype=torch.float32).to(device)\n",
    "y_val_tensor = torch.tensor(np.array(Y_val), dtype=torch.long).to(device)\n",
    "\n",
    "# Convert test data to PyTorch tensors\n",
    "X_test_tensor = torch.tensor(np.array(X_test_scaled), dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(np.array(Y_test), dtype=torch.long).to(device)\n",
    "\n",
    "# Create a DataLoader for the training data\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the validate data\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Create a DataLoader for the test data\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "eee791fb-f7f5-4e55-93a7-1dca536794a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7f746c095bd0>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7ba0fca6-4b6b-4fe5-bc56-de7d36dd19d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 1,  ..., 2, 1, 2])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "58b6d7c8-bb44-4f13-a483-d552d679bc5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=50, output_size=3):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 20)\n",
    "        self.fc3 = nn.Linear(20, output_size)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(50)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(20)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.batchnorm1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.batchnorm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3350dc0e-0a32-4051-aee8-07330e327f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (fc1): Linear(in_features=9, out_features=50, bias=True)\n",
       "  (fc2): Linear(in_features=50, out_features=20, bias=True)\n",
       "  (fc3): Linear(in_features=20, out_features=3, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       "  (batchnorm1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (batchnorm2): BatchNorm1d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       ")"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(X_train_scaled.shape[1]).to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "a5fe9f3c-dc9f-4931-bd4a-8542b2d54f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "crossentropy_loss = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "836c15ed-8915-4a25-8fec-3e35ab2c0297",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, model, loss_fn, optimizer):\n",
    "    size = len(data.dataset)\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for batch, (X, y) in enumerate(data):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    \n",
    "        # Bachpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "       \n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"Train loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
    "    return (total_loss / len(data)), (correct/ size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "a2ffbd65-0aec-4ac9-ada2-fcb0dcdabe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test(data, model, loss_fn):\n",
    "    size = len(data.dataset)\n",
    "    num_batches = len(data)\n",
    "   \n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():      \n",
    "        for X, y in data:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "        \n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            \n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error:\\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "   \n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bbcfab-ddbe-46b9-a770-dfe647e6c68a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec59262-ff49-440b-80ea-ac95225aecae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      " -------------------\n",
      "Train loss: 1.052632 [    0/345580]\n",
      "Train loss: 1.092445 [ 3200/345580]\n",
      "Train loss: 1.030116 [ 6400/345580]\n",
      "Train loss: 1.063921 [ 9600/345580]\n",
      "Train loss: 1.020415 [12800/345580]\n",
      "Train loss: 1.064226 [16000/345580]\n",
      "Train loss: 1.107343 [19200/345580]\n",
      "Train loss: 1.069378 [22400/345580]\n",
      "Train loss: 1.056816 [25600/345580]\n",
      "Train loss: 0.994606 [28800/345580]\n",
      "Train loss: 1.005725 [32000/345580]\n",
      "Train loss: 1.000736 [35200/345580]\n",
      "Train loss: 1.017317 [38400/345580]\n",
      "Train loss: 1.021270 [41600/345580]\n",
      "Train loss: 1.101602 [44800/345580]\n",
      "Train loss: 0.962362 [48000/345580]\n",
      "Train loss: 1.038360 [51200/345580]\n",
      "Train loss: 1.088483 [54400/345580]\n",
      "Train loss: 1.019860 [57600/345580]\n",
      "Train loss: 1.041482 [60800/345580]\n",
      "Train loss: 1.022970 [64000/345580]\n",
      "Train loss: 0.995930 [67200/345580]\n",
      "Train loss: 1.033462 [70400/345580]\n",
      "Train loss: 1.053937 [73600/345580]\n",
      "Train loss: 1.104217 [76800/345580]\n",
      "Train loss: 1.116058 [80000/345580]\n",
      "Train loss: 1.016832 [83200/345580]\n",
      "Train loss: 0.937873 [86400/345580]\n",
      "Train loss: 1.093230 [89600/345580]\n",
      "Train loss: 1.034847 [92800/345580]\n",
      "Train loss: 1.068860 [96000/345580]\n",
      "Train loss: 1.093481 [99200/345580]\n",
      "Train loss: 1.084920 [102400/345580]\n",
      "Train loss: 0.990332 [105600/345580]\n",
      "Train loss: 1.039097 [108800/345580]\n",
      "Train loss: 1.045511 [112000/345580]\n",
      "Train loss: 1.088848 [115200/345580]\n",
      "Train loss: 0.968364 [118400/345580]\n",
      "Train loss: 1.078189 [121600/345580]\n",
      "Train loss: 1.027359 [124800/345580]\n",
      "Train loss: 0.990511 [128000/345580]\n",
      "Train loss: 1.032712 [131200/345580]\n",
      "Train loss: 1.088694 [134400/345580]\n",
      "Train loss: 1.003882 [137600/345580]\n",
      "Train loss: 1.107767 [140800/345580]\n",
      "Train loss: 1.051737 [144000/345580]\n",
      "Train loss: 1.014454 [147200/345580]\n",
      "Train loss: 0.994649 [150400/345580]\n",
      "Train loss: 1.005147 [153600/345580]\n",
      "Train loss: 1.023729 [156800/345580]\n",
      "Train loss: 1.084837 [160000/345580]\n",
      "Train loss: 1.016034 [163200/345580]\n",
      "Train loss: 1.015327 [166400/345580]\n",
      "Train loss: 1.047780 [169600/345580]\n",
      "Train loss: 1.110833 [172800/345580]\n",
      "Train loss: 1.080254 [176000/345580]\n",
      "Train loss: 1.071511 [179200/345580]\n",
      "Train loss: 0.968455 [182400/345580]\n",
      "Train loss: 1.115711 [185600/345580]\n",
      "Train loss: 1.036166 [188800/345580]\n",
      "Train loss: 1.024438 [192000/345580]\n",
      "Train loss: 1.083778 [195200/345580]\n",
      "Train loss: 1.193545 [198400/345580]\n",
      "Train loss: 0.995479 [201600/345580]\n",
      "Train loss: 1.007681 [204800/345580]\n",
      "Train loss: 0.977567 [208000/345580]\n",
      "Train loss: 1.015260 [211200/345580]\n",
      "Train loss: 0.968404 [214400/345580]\n",
      "Train loss: 1.004786 [217600/345580]\n",
      "Train loss: 1.042699 [220800/345580]\n",
      "Train loss: 1.055316 [224000/345580]\n",
      "Train loss: 0.967711 [227200/345580]\n",
      "Train loss: 1.031568 [230400/345580]\n",
      "Train loss: 0.958024 [233600/345580]\n",
      "Train loss: 0.971049 [236800/345580]\n",
      "Train loss: 1.104828 [240000/345580]\n",
      "Train loss: 1.024736 [243200/345580]\n",
      "Train loss: 1.052138 [246400/345580]\n",
      "Train loss: 1.057632 [249600/345580]\n",
      "Train loss: 1.106253 [252800/345580]\n",
      "Train loss: 1.003855 [256000/345580]\n",
      "Train loss: 1.025253 [259200/345580]\n",
      "Train loss: 1.067089 [262400/345580]\n",
      "Train loss: 1.022573 [265600/345580]\n",
      "Train loss: 0.953674 [268800/345580]\n",
      "Train loss: 0.982066 [272000/345580]\n",
      "Train loss: 1.080589 [275200/345580]\n",
      "Train loss: 1.034124 [278400/345580]\n",
      "Train loss: 1.030370 [281600/345580]\n",
      "Train loss: 1.033208 [284800/345580]\n",
      "Train loss: 1.065565 [288000/345580]\n",
      "Train loss: 1.059014 [291200/345580]\n",
      "Train loss: 0.925949 [294400/345580]\n",
      "Train loss: 0.962971 [297600/345580]\n",
      "Train loss: 1.027780 [300800/345580]\n",
      "Train loss: 0.998241 [304000/345580]\n",
      "Train loss: 1.110911 [307200/345580]\n",
      "Train loss: 1.080032 [310400/345580]\n",
      "Train loss: 1.166732 [313600/345580]\n",
      "Train loss: 1.118735 [316800/345580]\n",
      "Train loss: 1.004766 [320000/345580]\n",
      "Train loss: 1.090756 [323200/345580]\n",
      "Train loss: 1.015979 [326400/345580]\n",
      "Train loss: 1.089588 [329600/345580]\n",
      "Train loss: 1.028762 [332800/345580]\n",
      "Train loss: 1.029373 [336000/345580]\n",
      "Train loss: 0.982325 [339200/345580]\n",
      "Train loss: 0.919293 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.041456 \n",
      "\n",
      "Epoch 2\n",
      " -------------------\n",
      "Train loss: 0.960946 [    0/345580]\n",
      "Train loss: 1.021019 [ 3200/345580]\n",
      "Train loss: 1.003759 [ 6400/345580]\n",
      "Train loss: 1.099809 [ 9600/345580]\n",
      "Train loss: 1.037040 [12800/345580]\n",
      "Train loss: 0.981192 [16000/345580]\n",
      "Train loss: 1.021193 [19200/345580]\n",
      "Train loss: 1.080684 [22400/345580]\n",
      "Train loss: 1.069486 [25600/345580]\n",
      "Train loss: 1.050650 [28800/345580]\n",
      "Train loss: 1.081381 [32000/345580]\n",
      "Train loss: 1.041598 [35200/345580]\n",
      "Train loss: 0.909138 [38400/345580]\n",
      "Train loss: 1.138286 [41600/345580]\n",
      "Train loss: 1.045550 [44800/345580]\n",
      "Train loss: 1.106653 [48000/345580]\n",
      "Train loss: 0.986098 [51200/345580]\n",
      "Train loss: 1.022876 [54400/345580]\n",
      "Train loss: 1.011085 [57600/345580]\n",
      "Train loss: 0.982818 [60800/345580]\n",
      "Train loss: 1.127050 [64000/345580]\n",
      "Train loss: 1.119740 [67200/345580]\n",
      "Train loss: 1.113494 [70400/345580]\n",
      "Train loss: 1.065823 [73600/345580]\n",
      "Train loss: 1.079758 [76800/345580]\n",
      "Train loss: 1.055109 [80000/345580]\n",
      "Train loss: 1.102286 [83200/345580]\n",
      "Train loss: 0.922220 [86400/345580]\n",
      "Train loss: 1.107249 [89600/345580]\n",
      "Train loss: 0.975608 [92800/345580]\n",
      "Train loss: 1.025462 [96000/345580]\n",
      "Train loss: 1.066117 [99200/345580]\n",
      "Train loss: 0.988656 [102400/345580]\n",
      "Train loss: 1.010052 [105600/345580]\n",
      "Train loss: 1.007516 [108800/345580]\n",
      "Train loss: 1.048822 [112000/345580]\n",
      "Train loss: 1.050594 [115200/345580]\n",
      "Train loss: 1.074482 [118400/345580]\n",
      "Train loss: 1.080988 [121600/345580]\n",
      "Train loss: 1.066576 [124800/345580]\n",
      "Train loss: 1.099974 [128000/345580]\n",
      "Train loss: 1.010611 [131200/345580]\n",
      "Train loss: 1.073950 [134400/345580]\n",
      "Train loss: 1.080561 [137600/345580]\n",
      "Train loss: 1.035708 [140800/345580]\n",
      "Train loss: 0.952215 [144000/345580]\n",
      "Train loss: 1.094393 [147200/345580]\n",
      "Train loss: 0.953042 [150400/345580]\n",
      "Train loss: 1.058913 [153600/345580]\n",
      "Train loss: 1.133351 [156800/345580]\n",
      "Train loss: 1.057768 [160000/345580]\n",
      "Train loss: 1.123927 [163200/345580]\n",
      "Train loss: 0.980487 [166400/345580]\n",
      "Train loss: 1.040726 [169600/345580]\n",
      "Train loss: 1.139299 [172800/345580]\n",
      "Train loss: 1.067991 [176000/345580]\n",
      "Train loss: 1.160647 [179200/345580]\n",
      "Train loss: 1.024761 [182400/345580]\n",
      "Train loss: 1.049931 [185600/345580]\n",
      "Train loss: 1.083155 [188800/345580]\n",
      "Train loss: 1.139524 [192000/345580]\n",
      "Train loss: 1.167103 [195200/345580]\n",
      "Train loss: 1.060725 [198400/345580]\n",
      "Train loss: 1.092889 [201600/345580]\n",
      "Train loss: 1.040651 [204800/345580]\n",
      "Train loss: 1.008327 [208000/345580]\n",
      "Train loss: 1.084715 [211200/345580]\n",
      "Train loss: 1.007282 [214400/345580]\n",
      "Train loss: 1.086392 [217600/345580]\n",
      "Train loss: 1.146903 [220800/345580]\n",
      "Train loss: 1.006163 [224000/345580]\n",
      "Train loss: 1.017191 [227200/345580]\n",
      "Train loss: 0.999961 [230400/345580]\n",
      "Train loss: 0.921738 [233600/345580]\n",
      "Train loss: 0.958599 [236800/345580]\n",
      "Train loss: 1.159922 [240000/345580]\n",
      "Train loss: 1.067038 [243200/345580]\n",
      "Train loss: 1.125261 [246400/345580]\n",
      "Train loss: 0.960413 [249600/345580]\n",
      "Train loss: 1.089372 [252800/345580]\n",
      "Train loss: 0.980489 [256000/345580]\n",
      "Train loss: 1.052979 [259200/345580]\n",
      "Train loss: 1.139073 [262400/345580]\n",
      "Train loss: 1.019421 [265600/345580]\n",
      "Train loss: 1.016102 [268800/345580]\n",
      "Train loss: 0.972476 [272000/345580]\n",
      "Train loss: 1.007975 [275200/345580]\n",
      "Train loss: 1.115885 [278400/345580]\n",
      "Train loss: 1.142330 [281600/345580]\n",
      "Train loss: 1.033528 [284800/345580]\n",
      "Train loss: 1.056930 [288000/345580]\n",
      "Train loss: 1.136332 [291200/345580]\n",
      "Train loss: 1.009554 [294400/345580]\n",
      "Train loss: 1.047302 [297600/345580]\n",
      "Train loss: 1.019922 [300800/345580]\n",
      "Train loss: 1.046596 [304000/345580]\n",
      "Train loss: 1.063344 [307200/345580]\n",
      "Train loss: 1.033664 [310400/345580]\n",
      "Train loss: 1.033339 [313600/345580]\n",
      "Train loss: 0.954869 [316800/345580]\n",
      "Train loss: 1.094493 [320000/345580]\n",
      "Train loss: 1.061610 [323200/345580]\n",
      "Train loss: 0.968177 [326400/345580]\n",
      "Train loss: 1.024225 [329600/345580]\n",
      "Train loss: 0.999219 [332800/345580]\n",
      "Train loss: 1.119752 [336000/345580]\n",
      "Train loss: 0.992943 [339200/345580]\n",
      "Train loss: 0.974371 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.042250 \n",
      "\n",
      "Epoch 3\n",
      " -------------------\n",
      "Train loss: 1.098436 [    0/345580]\n",
      "Train loss: 1.019166 [ 3200/345580]\n",
      "Train loss: 1.036157 [ 6400/345580]\n",
      "Train loss: 1.128572 [ 9600/345580]\n",
      "Train loss: 1.074022 [12800/345580]\n",
      "Train loss: 1.022867 [16000/345580]\n",
      "Train loss: 1.088809 [19200/345580]\n",
      "Train loss: 0.999657 [22400/345580]\n",
      "Train loss: 1.006245 [25600/345580]\n",
      "Train loss: 1.027864 [28800/345580]\n",
      "Train loss: 1.061078 [32000/345580]\n",
      "Train loss: 1.117107 [35200/345580]\n",
      "Train loss: 1.030488 [38400/345580]\n",
      "Train loss: 1.034887 [41600/345580]\n",
      "Train loss: 0.952133 [44800/345580]\n",
      "Train loss: 1.015596 [48000/345580]\n",
      "Train loss: 1.001576 [51200/345580]\n",
      "Train loss: 1.105400 [54400/345580]\n",
      "Train loss: 1.065495 [57600/345580]\n",
      "Train loss: 0.931800 [60800/345580]\n",
      "Train loss: 1.097831 [64000/345580]\n",
      "Train loss: 0.975559 [67200/345580]\n",
      "Train loss: 0.952040 [70400/345580]\n",
      "Train loss: 1.122333 [73600/345580]\n",
      "Train loss: 1.049866 [76800/345580]\n",
      "Train loss: 1.083133 [80000/345580]\n",
      "Train loss: 1.077120 [83200/345580]\n",
      "Train loss: 1.047888 [86400/345580]\n",
      "Train loss: 1.005658 [89600/345580]\n",
      "Train loss: 1.053522 [92800/345580]\n",
      "Train loss: 0.982745 [96000/345580]\n",
      "Train loss: 1.002678 [99200/345580]\n",
      "Train loss: 1.007465 [102400/345580]\n",
      "Train loss: 1.087367 [105600/345580]\n",
      "Train loss: 1.096923 [108800/345580]\n",
      "Train loss: 1.014113 [112000/345580]\n",
      "Train loss: 1.015175 [115200/345580]\n",
      "Train loss: 1.040788 [118400/345580]\n",
      "Train loss: 1.058028 [121600/345580]\n",
      "Train loss: 1.046078 [124800/345580]\n",
      "Train loss: 1.098679 [128000/345580]\n",
      "Train loss: 1.078378 [131200/345580]\n",
      "Train loss: 1.113996 [134400/345580]\n",
      "Train loss: 1.105707 [137600/345580]\n",
      "Train loss: 1.049473 [140800/345580]\n",
      "Train loss: 1.073546 [144000/345580]\n",
      "Train loss: 1.014467 [147200/345580]\n",
      "Train loss: 1.032332 [150400/345580]\n",
      "Train loss: 1.138311 [153600/345580]\n",
      "Train loss: 0.973407 [156800/345580]\n",
      "Train loss: 1.115962 [160000/345580]\n",
      "Train loss: 1.037965 [163200/345580]\n",
      "Train loss: 1.076492 [166400/345580]\n",
      "Train loss: 1.056755 [169600/345580]\n",
      "Train loss: 1.021465 [172800/345580]\n",
      "Train loss: 1.017895 [176000/345580]\n",
      "Train loss: 1.058082 [179200/345580]\n",
      "Train loss: 1.140569 [182400/345580]\n",
      "Train loss: 1.078675 [185600/345580]\n",
      "Train loss: 1.026927 [188800/345580]\n",
      "Train loss: 1.101235 [192000/345580]\n",
      "Train loss: 1.047233 [195200/345580]\n",
      "Train loss: 1.120383 [198400/345580]\n",
      "Train loss: 1.016105 [201600/345580]\n",
      "Train loss: 0.994439 [204800/345580]\n",
      "Train loss: 0.994129 [208000/345580]\n",
      "Train loss: 1.138320 [211200/345580]\n",
      "Train loss: 1.106256 [214400/345580]\n",
      "Train loss: 1.001219 [217600/345580]\n",
      "Train loss: 1.066439 [220800/345580]\n",
      "Train loss: 1.106942 [224000/345580]\n",
      "Train loss: 1.056833 [227200/345580]\n",
      "Train loss: 1.034500 [230400/345580]\n",
      "Train loss: 0.975367 [233600/345580]\n",
      "Train loss: 1.042245 [236800/345580]\n",
      "Train loss: 1.236539 [240000/345580]\n",
      "Train loss: 0.958645 [243200/345580]\n",
      "Train loss: 1.076033 [246400/345580]\n",
      "Train loss: 1.050270 [249600/345580]\n",
      "Train loss: 1.103866 [252800/345580]\n",
      "Train loss: 1.032634 [256000/345580]\n",
      "Train loss: 1.043265 [259200/345580]\n",
      "Train loss: 1.102617 [262400/345580]\n",
      "Train loss: 0.992194 [265600/345580]\n",
      "Train loss: 1.054485 [268800/345580]\n",
      "Train loss: 1.074501 [272000/345580]\n",
      "Train loss: 1.045236 [275200/345580]\n",
      "Train loss: 1.057027 [278400/345580]\n",
      "Train loss: 1.073151 [281600/345580]\n",
      "Train loss: 1.067546 [284800/345580]\n",
      "Train loss: 1.026810 [288000/345580]\n",
      "Train loss: 0.971217 [291200/345580]\n",
      "Train loss: 1.045730 [294400/345580]\n",
      "Train loss: 1.106395 [297600/345580]\n",
      "Train loss: 1.033312 [300800/345580]\n",
      "Train loss: 1.042799 [304000/345580]\n",
      "Train loss: 1.085147 [307200/345580]\n",
      "Train loss: 1.077414 [310400/345580]\n",
      "Train loss: 1.090867 [313600/345580]\n",
      "Train loss: 1.017723 [316800/345580]\n",
      "Train loss: 1.050595 [320000/345580]\n",
      "Train loss: 0.983088 [323200/345580]\n",
      "Train loss: 1.061057 [326400/345580]\n",
      "Train loss: 1.075288 [329600/345580]\n",
      "Train loss: 1.137706 [332800/345580]\n",
      "Train loss: 1.072478 [336000/345580]\n",
      "Train loss: 1.108552 [339200/345580]\n",
      "Train loss: 1.106340 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.042717 \n",
      "\n",
      "Epoch 4\n",
      " -------------------\n",
      "Train loss: 1.118915 [    0/345580]\n",
      "Train loss: 1.046931 [ 3200/345580]\n",
      "Train loss: 1.069585 [ 6400/345580]\n",
      "Train loss: 1.051433 [ 9600/345580]\n",
      "Train loss: 1.088219 [12800/345580]\n",
      "Train loss: 1.027202 [16000/345580]\n",
      "Train loss: 0.986752 [19200/345580]\n",
      "Train loss: 0.932882 [22400/345580]\n",
      "Train loss: 1.082040 [25600/345580]\n",
      "Train loss: 1.110245 [28800/345580]\n",
      "Train loss: 1.073617 [32000/345580]\n",
      "Train loss: 1.054259 [35200/345580]\n",
      "Train loss: 1.148838 [38400/345580]\n",
      "Train loss: 1.043486 [41600/345580]\n",
      "Train loss: 1.082938 [44800/345580]\n",
      "Train loss: 1.015748 [48000/345580]\n",
      "Train loss: 1.082823 [51200/345580]\n",
      "Train loss: 0.997736 [54400/345580]\n",
      "Train loss: 1.007031 [57600/345580]\n",
      "Train loss: 1.064743 [60800/345580]\n",
      "Train loss: 1.059702 [64000/345580]\n",
      "Train loss: 1.059918 [67200/345580]\n",
      "Train loss: 1.051481 [70400/345580]\n",
      "Train loss: 1.121218 [73600/345580]\n",
      "Train loss: 1.019603 [76800/345580]\n",
      "Train loss: 0.932009 [80000/345580]\n",
      "Train loss: 0.986609 [83200/345580]\n",
      "Train loss: 1.108521 [86400/345580]\n",
      "Train loss: 0.999290 [89600/345580]\n",
      "Train loss: 1.067324 [92800/345580]\n",
      "Train loss: 1.104929 [96000/345580]\n",
      "Train loss: 1.073239 [99200/345580]\n",
      "Train loss: 1.074995 [102400/345580]\n",
      "Train loss: 1.019177 [105600/345580]\n",
      "Train loss: 0.991743 [108800/345580]\n",
      "Train loss: 0.941340 [112000/345580]\n",
      "Train loss: 1.075192 [115200/345580]\n",
      "Train loss: 1.167292 [118400/345580]\n",
      "Train loss: 1.063632 [121600/345580]\n",
      "Train loss: 1.042307 [124800/345580]\n",
      "Train loss: 1.083224 [128000/345580]\n",
      "Train loss: 0.980109 [131200/345580]\n",
      "Train loss: 1.160971 [134400/345580]\n",
      "Train loss: 1.018860 [137600/345580]\n",
      "Train loss: 1.065871 [140800/345580]\n",
      "Train loss: 1.128388 [144000/345580]\n",
      "Train loss: 1.082384 [147200/345580]\n",
      "Train loss: 0.969936 [150400/345580]\n",
      "Train loss: 1.011814 [153600/345580]\n",
      "Train loss: 1.061754 [156800/345580]\n",
      "Train loss: 1.083003 [160000/345580]\n",
      "Train loss: 1.034658 [163200/345580]\n",
      "Train loss: 1.041688 [166400/345580]\n",
      "Train loss: 1.111849 [169600/345580]\n",
      "Train loss: 1.019531 [172800/345580]\n",
      "Train loss: 1.178723 [176000/345580]\n",
      "Train loss: 1.079159 [179200/345580]\n",
      "Train loss: 1.041814 [182400/345580]\n",
      "Train loss: 1.041779 [185600/345580]\n",
      "Train loss: 1.004578 [188800/345580]\n",
      "Train loss: 1.015952 [192000/345580]\n",
      "Train loss: 1.003481 [195200/345580]\n",
      "Train loss: 1.120936 [198400/345580]\n",
      "Train loss: 1.037067 [201600/345580]\n",
      "Train loss: 1.064621 [204800/345580]\n",
      "Train loss: 1.064600 [208000/345580]\n",
      "Train loss: 1.144760 [211200/345580]\n",
      "Train loss: 1.164604 [214400/345580]\n",
      "Train loss: 1.006220 [217600/345580]\n",
      "Train loss: 1.047178 [220800/345580]\n",
      "Train loss: 0.936842 [224000/345580]\n",
      "Train loss: 1.064996 [227200/345580]\n",
      "Train loss: 1.043233 [230400/345580]\n",
      "Train loss: 1.135990 [233600/345580]\n",
      "Train loss: 1.076189 [236800/345580]\n",
      "Train loss: 0.995290 [240000/345580]\n",
      "Train loss: 1.143525 [243200/345580]\n",
      "Train loss: 1.035658 [246400/345580]\n",
      "Train loss: 1.082765 [249600/345580]\n",
      "Train loss: 1.078352 [252800/345580]\n",
      "Train loss: 0.983343 [256000/345580]\n",
      "Train loss: 1.093713 [259200/345580]\n",
      "Train loss: 1.084154 [262400/345580]\n",
      "Train loss: 0.943362 [265600/345580]\n",
      "Train loss: 0.954795 [268800/345580]\n",
      "Train loss: 0.988106 [272000/345580]\n",
      "Train loss: 1.087485 [275200/345580]\n",
      "Train loss: 1.038743 [278400/345580]\n",
      "Train loss: 1.142407 [281600/345580]\n",
      "Train loss: 1.035808 [284800/345580]\n",
      "Train loss: 1.064547 [288000/345580]\n",
      "Train loss: 1.074747 [291200/345580]\n",
      "Train loss: 1.041050 [294400/345580]\n",
      "Train loss: 0.995289 [297600/345580]\n",
      "Train loss: 0.936123 [300800/345580]\n",
      "Train loss: 1.104722 [304000/345580]\n",
      "Train loss: 0.971576 [307200/345580]\n",
      "Train loss: 1.076329 [310400/345580]\n",
      "Train loss: 1.129569 [313600/345580]\n",
      "Train loss: 1.043614 [316800/345580]\n",
      "Train loss: 0.973086 [320000/345580]\n",
      "Train loss: 1.127038 [323200/345580]\n",
      "Train loss: 0.974172 [326400/345580]\n",
      "Train loss: 0.985228 [329600/345580]\n",
      "Train loss: 1.095684 [332800/345580]\n",
      "Train loss: 0.942969 [336000/345580]\n",
      "Train loss: 1.032985 [339200/345580]\n",
      "Train loss: 0.975648 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042168 \n",
      "\n",
      "Epoch 5\n",
      " -------------------\n",
      "Train loss: 0.966549 [    0/345580]\n",
      "Train loss: 0.986329 [ 3200/345580]\n",
      "Train loss: 1.120860 [ 6400/345580]\n",
      "Train loss: 0.985333 [ 9600/345580]\n",
      "Train loss: 1.036574 [12800/345580]\n",
      "Train loss: 1.137236 [16000/345580]\n",
      "Train loss: 1.058794 [19200/345580]\n",
      "Train loss: 0.986218 [22400/345580]\n",
      "Train loss: 1.030607 [25600/345580]\n",
      "Train loss: 0.966680 [28800/345580]\n",
      "Train loss: 1.058021 [32000/345580]\n",
      "Train loss: 1.135351 [35200/345580]\n",
      "Train loss: 1.010342 [38400/345580]\n",
      "Train loss: 1.131729 [41600/345580]\n",
      "Train loss: 0.954825 [44800/345580]\n",
      "Train loss: 0.960399 [48000/345580]\n",
      "Train loss: 0.982688 [51200/345580]\n",
      "Train loss: 1.098566 [54400/345580]\n",
      "Train loss: 0.961137 [57600/345580]\n",
      "Train loss: 0.929576 [60800/345580]\n",
      "Train loss: 1.095136 [64000/345580]\n",
      "Train loss: 1.000598 [67200/345580]\n",
      "Train loss: 1.103032 [70400/345580]\n",
      "Train loss: 1.016184 [73600/345580]\n",
      "Train loss: 1.028473 [76800/345580]\n",
      "Train loss: 0.961119 [80000/345580]\n",
      "Train loss: 1.043995 [83200/345580]\n",
      "Train loss: 1.061793 [86400/345580]\n",
      "Train loss: 1.009448 [89600/345580]\n",
      "Train loss: 1.031403 [92800/345580]\n",
      "Train loss: 1.116295 [96000/345580]\n",
      "Train loss: 1.043056 [99200/345580]\n",
      "Train loss: 1.092658 [102400/345580]\n",
      "Train loss: 1.006152 [105600/345580]\n",
      "Train loss: 0.922637 [108800/345580]\n",
      "Train loss: 1.098624 [112000/345580]\n",
      "Train loss: 0.963145 [115200/345580]\n",
      "Train loss: 1.086840 [118400/345580]\n",
      "Train loss: 1.068328 [121600/345580]\n",
      "Train loss: 1.043716 [124800/345580]\n",
      "Train loss: 1.060554 [128000/345580]\n",
      "Train loss: 1.010903 [131200/345580]\n",
      "Train loss: 0.991683 [134400/345580]\n",
      "Train loss: 1.074135 [137600/345580]\n",
      "Train loss: 1.030803 [140800/345580]\n",
      "Train loss: 1.058814 [144000/345580]\n",
      "Train loss: 1.074297 [147200/345580]\n",
      "Train loss: 1.130469 [150400/345580]\n",
      "Train loss: 1.033664 [153600/345580]\n",
      "Train loss: 1.118620 [156800/345580]\n",
      "Train loss: 0.959711 [160000/345580]\n",
      "Train loss: 1.078164 [163200/345580]\n",
      "Train loss: 0.984499 [166400/345580]\n",
      "Train loss: 1.116179 [169600/345580]\n",
      "Train loss: 1.036801 [172800/345580]\n",
      "Train loss: 1.026870 [176000/345580]\n",
      "Train loss: 1.098595 [179200/345580]\n",
      "Train loss: 1.039439 [182400/345580]\n",
      "Train loss: 1.066385 [185600/345580]\n",
      "Train loss: 1.090508 [188800/345580]\n",
      "Train loss: 0.945511 [192000/345580]\n",
      "Train loss: 1.063186 [195200/345580]\n",
      "Train loss: 1.149590 [198400/345580]\n",
      "Train loss: 1.005647 [201600/345580]\n",
      "Train loss: 1.086750 [204800/345580]\n",
      "Train loss: 1.007869 [208000/345580]\n",
      "Train loss: 1.018377 [211200/345580]\n",
      "Train loss: 1.069867 [214400/345580]\n",
      "Train loss: 1.077994 [217600/345580]\n",
      "Train loss: 1.053612 [220800/345580]\n",
      "Train loss: 0.945857 [224000/345580]\n",
      "Train loss: 1.094009 [227200/345580]\n",
      "Train loss: 1.051793 [230400/345580]\n",
      "Train loss: 1.015449 [233600/345580]\n",
      "Train loss: 0.988547 [236800/345580]\n",
      "Train loss: 1.062486 [240000/345580]\n",
      "Train loss: 0.987987 [243200/345580]\n",
      "Train loss: 0.968440 [246400/345580]\n",
      "Train loss: 1.013210 [249600/345580]\n",
      "Train loss: 0.970870 [252800/345580]\n",
      "Train loss: 1.051993 [256000/345580]\n",
      "Train loss: 1.092026 [259200/345580]\n",
      "Train loss: 0.974079 [262400/345580]\n",
      "Train loss: 1.069981 [265600/345580]\n",
      "Train loss: 1.136252 [268800/345580]\n",
      "Train loss: 1.064482 [272000/345580]\n",
      "Train loss: 0.954969 [275200/345580]\n",
      "Train loss: 1.096311 [278400/345580]\n",
      "Train loss: 1.003855 [281600/345580]\n",
      "Train loss: 1.088907 [284800/345580]\n",
      "Train loss: 1.005572 [288000/345580]\n",
      "Train loss: 1.075750 [291200/345580]\n",
      "Train loss: 1.019666 [294400/345580]\n",
      "Train loss: 0.955031 [297600/345580]\n",
      "Train loss: 1.177157 [300800/345580]\n",
      "Train loss: 0.986644 [304000/345580]\n",
      "Train loss: 1.117087 [307200/345580]\n",
      "Train loss: 0.987510 [310400/345580]\n",
      "Train loss: 1.012715 [313600/345580]\n",
      "Train loss: 1.113396 [316800/345580]\n",
      "Train loss: 1.128326 [320000/345580]\n",
      "Train loss: 1.003916 [323200/345580]\n",
      "Train loss: 0.959561 [326400/345580]\n",
      "Train loss: 1.110669 [329600/345580]\n",
      "Train loss: 1.011388 [332800/345580]\n",
      "Train loss: 1.085214 [336000/345580]\n",
      "Train loss: 0.997083 [339200/345580]\n",
      "Train loss: 1.082973 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.042076 \n",
      "\n",
      "Epoch 6\n",
      " -------------------\n",
      "Train loss: 1.222377 [    0/345580]\n",
      "Train loss: 0.997130 [ 3200/345580]\n",
      "Train loss: 1.010975 [ 6400/345580]\n",
      "Train loss: 0.985764 [ 9600/345580]\n",
      "Train loss: 1.053540 [12800/345580]\n",
      "Train loss: 1.090755 [16000/345580]\n",
      "Train loss: 1.041616 [19200/345580]\n",
      "Train loss: 0.975308 [22400/345580]\n",
      "Train loss: 1.002646 [25600/345580]\n",
      "Train loss: 1.045864 [28800/345580]\n",
      "Train loss: 0.984063 [32000/345580]\n",
      "Train loss: 1.012927 [35200/345580]\n",
      "Train loss: 1.044871 [38400/345580]\n",
      "Train loss: 1.042462 [41600/345580]\n",
      "Train loss: 1.043712 [44800/345580]\n",
      "Train loss: 1.048575 [48000/345580]\n",
      "Train loss: 1.013570 [51200/345580]\n",
      "Train loss: 1.122390 [54400/345580]\n",
      "Train loss: 1.010582 [57600/345580]\n",
      "Train loss: 1.087425 [60800/345580]\n",
      "Train loss: 1.044032 [64000/345580]\n",
      "Train loss: 1.098551 [67200/345580]\n",
      "Train loss: 1.169885 [70400/345580]\n",
      "Train loss: 1.125515 [73600/345580]\n",
      "Train loss: 1.075988 [76800/345580]\n",
      "Train loss: 1.108270 [80000/345580]\n",
      "Train loss: 1.119835 [83200/345580]\n",
      "Train loss: 1.028493 [86400/345580]\n",
      "Train loss: 1.028749 [89600/345580]\n",
      "Train loss: 0.900207 [92800/345580]\n",
      "Train loss: 0.948217 [96000/345580]\n",
      "Train loss: 1.032460 [99200/345580]\n",
      "Train loss: 1.034434 [102400/345580]\n",
      "Train loss: 1.026733 [105600/345580]\n",
      "Train loss: 1.016834 [108800/345580]\n",
      "Train loss: 1.097733 [112000/345580]\n",
      "Train loss: 1.037202 [115200/345580]\n",
      "Train loss: 1.098549 [118400/345580]\n",
      "Train loss: 1.065272 [121600/345580]\n",
      "Train loss: 0.995376 [124800/345580]\n",
      "Train loss: 1.096315 [128000/345580]\n",
      "Train loss: 1.128042 [131200/345580]\n",
      "Train loss: 1.079924 [134400/345580]\n",
      "Train loss: 1.030110 [137600/345580]\n",
      "Train loss: 0.999637 [140800/345580]\n",
      "Train loss: 1.057412 [144000/345580]\n",
      "Train loss: 1.112014 [147200/345580]\n",
      "Train loss: 0.974242 [150400/345580]\n",
      "Train loss: 1.082593 [153600/345580]\n",
      "Train loss: 1.118669 [156800/345580]\n",
      "Train loss: 1.037885 [160000/345580]\n",
      "Train loss: 1.015318 [163200/345580]\n",
      "Train loss: 1.122554 [166400/345580]\n",
      "Train loss: 0.960222 [169600/345580]\n",
      "Train loss: 1.052253 [172800/345580]\n",
      "Train loss: 1.049255 [176000/345580]\n",
      "Train loss: 1.035736 [179200/345580]\n",
      "Train loss: 0.983106 [182400/345580]\n",
      "Train loss: 1.072390 [185600/345580]\n",
      "Train loss: 0.996703 [188800/345580]\n",
      "Train loss: 1.070306 [192000/345580]\n",
      "Train loss: 0.898593 [195200/345580]\n",
      "Train loss: 1.010681 [198400/345580]\n",
      "Train loss: 1.030813 [201600/345580]\n",
      "Train loss: 1.088342 [204800/345580]\n",
      "Train loss: 0.943578 [208000/345580]\n",
      "Train loss: 0.959742 [211200/345580]\n",
      "Train loss: 1.109624 [214400/345580]\n",
      "Train loss: 1.064489 [217600/345580]\n",
      "Train loss: 1.026973 [220800/345580]\n",
      "Train loss: 1.034315 [224000/345580]\n",
      "Train loss: 1.015531 [227200/345580]\n",
      "Train loss: 0.962727 [230400/345580]\n",
      "Train loss: 1.048332 [233600/345580]\n",
      "Train loss: 0.992234 [236800/345580]\n",
      "Train loss: 0.972619 [240000/345580]\n",
      "Train loss: 1.090465 [243200/345580]\n",
      "Train loss: 0.908302 [246400/345580]\n",
      "Train loss: 1.078693 [249600/345580]\n",
      "Train loss: 1.128237 [252800/345580]\n",
      "Train loss: 1.039799 [256000/345580]\n",
      "Train loss: 1.010777 [259200/345580]\n",
      "Train loss: 1.092863 [262400/345580]\n",
      "Train loss: 1.133469 [265600/345580]\n",
      "Train loss: 1.087700 [268800/345580]\n",
      "Train loss: 1.029245 [272000/345580]\n",
      "Train loss: 1.047143 [275200/345580]\n",
      "Train loss: 0.993547 [278400/345580]\n",
      "Train loss: 0.957824 [281600/345580]\n",
      "Train loss: 0.983449 [284800/345580]\n",
      "Train loss: 0.967086 [288000/345580]\n",
      "Train loss: 1.197708 [291200/345580]\n",
      "Train loss: 1.116353 [294400/345580]\n",
      "Train loss: 1.084981 [297600/345580]\n",
      "Train loss: 1.024889 [300800/345580]\n",
      "Train loss: 1.054302 [304000/345580]\n",
      "Train loss: 1.072680 [307200/345580]\n",
      "Train loss: 1.077867 [310400/345580]\n",
      "Train loss: 1.181588 [313600/345580]\n",
      "Train loss: 1.133065 [316800/345580]\n",
      "Train loss: 1.104810 [320000/345580]\n",
      "Train loss: 1.218655 [323200/345580]\n",
      "Train loss: 1.066938 [326400/345580]\n",
      "Train loss: 0.971686 [329600/345580]\n",
      "Train loss: 1.097213 [332800/345580]\n",
      "Train loss: 1.005259 [336000/345580]\n",
      "Train loss: 0.982047 [339200/345580]\n",
      "Train loss: 1.073956 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.042386 \n",
      "\n",
      "Epoch 7\n",
      " -------------------\n",
      "Train loss: 0.954845 [    0/345580]\n",
      "Train loss: 1.030191 [ 3200/345580]\n",
      "Train loss: 1.028512 [ 6400/345580]\n",
      "Train loss: 1.022427 [ 9600/345580]\n",
      "Train loss: 1.069869 [12800/345580]\n",
      "Train loss: 1.022522 [16000/345580]\n",
      "Train loss: 1.036047 [19200/345580]\n",
      "Train loss: 1.051881 [22400/345580]\n",
      "Train loss: 1.019508 [25600/345580]\n",
      "Train loss: 1.043981 [28800/345580]\n",
      "Train loss: 0.959861 [32000/345580]\n",
      "Train loss: 0.999030 [35200/345580]\n",
      "Train loss: 0.994915 [38400/345580]\n",
      "Train loss: 1.014025 [41600/345580]\n",
      "Train loss: 1.060854 [44800/345580]\n",
      "Train loss: 1.109630 [48000/345580]\n",
      "Train loss: 1.128379 [51200/345580]\n",
      "Train loss: 1.074788 [54400/345580]\n",
      "Train loss: 1.084033 [57600/345580]\n",
      "Train loss: 1.047775 [60800/345580]\n",
      "Train loss: 1.065323 [64000/345580]\n",
      "Train loss: 1.058352 [67200/345580]\n",
      "Train loss: 1.087393 [70400/345580]\n",
      "Train loss: 0.959407 [73600/345580]\n",
      "Train loss: 1.006949 [76800/345580]\n",
      "Train loss: 1.023815 [80000/345580]\n",
      "Train loss: 1.018816 [83200/345580]\n",
      "Train loss: 1.050103 [86400/345580]\n",
      "Train loss: 1.077407 [89600/345580]\n",
      "Train loss: 1.032585 [92800/345580]\n",
      "Train loss: 1.088996 [96000/345580]\n",
      "Train loss: 1.081537 [99200/345580]\n",
      "Train loss: 1.114504 [102400/345580]\n",
      "Train loss: 1.031088 [105600/345580]\n",
      "Train loss: 1.040197 [108800/345580]\n",
      "Train loss: 1.101741 [112000/345580]\n",
      "Train loss: 1.106746 [115200/345580]\n",
      "Train loss: 1.134104 [118400/345580]\n",
      "Train loss: 1.101436 [121600/345580]\n",
      "Train loss: 1.073039 [124800/345580]\n",
      "Train loss: 0.987257 [128000/345580]\n",
      "Train loss: 1.156915 [131200/345580]\n",
      "Train loss: 0.929153 [134400/345580]\n",
      "Train loss: 1.045290 [137600/345580]\n",
      "Train loss: 0.944633 [140800/345580]\n",
      "Train loss: 0.987663 [144000/345580]\n",
      "Train loss: 1.024093 [147200/345580]\n",
      "Train loss: 1.025942 [150400/345580]\n",
      "Train loss: 0.938328 [153600/345580]\n",
      "Train loss: 1.009788 [156800/345580]\n",
      "Train loss: 1.107122 [160000/345580]\n",
      "Train loss: 1.086209 [163200/345580]\n",
      "Train loss: 1.035052 [166400/345580]\n",
      "Train loss: 1.037728 [169600/345580]\n",
      "Train loss: 1.079480 [172800/345580]\n",
      "Train loss: 1.100167 [176000/345580]\n",
      "Train loss: 1.002509 [179200/345580]\n",
      "Train loss: 1.008712 [182400/345580]\n",
      "Train loss: 1.034951 [185600/345580]\n",
      "Train loss: 1.009487 [188800/345580]\n",
      "Train loss: 1.055971 [192000/345580]\n",
      "Train loss: 1.096358 [195200/345580]\n",
      "Train loss: 0.945511 [198400/345580]\n",
      "Train loss: 1.046203 [201600/345580]\n",
      "Train loss: 1.011373 [204800/345580]\n",
      "Train loss: 0.952723 [208000/345580]\n",
      "Train loss: 1.016151 [211200/345580]\n",
      "Train loss: 1.075482 [214400/345580]\n",
      "Train loss: 1.072462 [217600/345580]\n",
      "Train loss: 1.164451 [220800/345580]\n",
      "Train loss: 0.999356 [224000/345580]\n",
      "Train loss: 0.991063 [227200/345580]\n",
      "Train loss: 1.114932 [230400/345580]\n",
      "Train loss: 1.128236 [233600/345580]\n",
      "Train loss: 1.089867 [236800/345580]\n",
      "Train loss: 1.043699 [240000/345580]\n",
      "Train loss: 1.217089 [243200/345580]\n",
      "Train loss: 1.027729 [246400/345580]\n",
      "Train loss: 1.140622 [249600/345580]\n",
      "Train loss: 0.958073 [252800/345580]\n",
      "Train loss: 1.058947 [256000/345580]\n",
      "Train loss: 1.016042 [259200/345580]\n",
      "Train loss: 1.071800 [262400/345580]\n",
      "Train loss: 1.053277 [265600/345580]\n",
      "Train loss: 1.056728 [268800/345580]\n",
      "Train loss: 1.076133 [272000/345580]\n",
      "Train loss: 0.948559 [275200/345580]\n",
      "Train loss: 1.004977 [278400/345580]\n",
      "Train loss: 1.218305 [281600/345580]\n",
      "Train loss: 1.055297 [284800/345580]\n",
      "Train loss: 1.057618 [288000/345580]\n",
      "Train loss: 1.068175 [291200/345580]\n",
      "Train loss: 1.005914 [294400/345580]\n",
      "Train loss: 1.030155 [297600/345580]\n",
      "Train loss: 1.060846 [300800/345580]\n",
      "Train loss: 0.937161 [304000/345580]\n",
      "Train loss: 1.056182 [307200/345580]\n",
      "Train loss: 1.001983 [310400/345580]\n",
      "Train loss: 1.122370 [313600/345580]\n",
      "Train loss: 1.133006 [316800/345580]\n",
      "Train loss: 1.002175 [320000/345580]\n",
      "Train loss: 0.999737 [323200/345580]\n",
      "Train loss: 0.962770 [326400/345580]\n",
      "Train loss: 1.134112 [329600/345580]\n",
      "Train loss: 1.009441 [332800/345580]\n",
      "Train loss: 0.962559 [336000/345580]\n",
      "Train loss: 1.010910 [339200/345580]\n",
      "Train loss: 1.021422 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042695 \n",
      "\n",
      "Epoch 8\n",
      " -------------------\n",
      "Train loss: 1.031574 [    0/345580]\n",
      "Train loss: 1.034211 [ 3200/345580]\n",
      "Train loss: 1.097012 [ 6400/345580]\n",
      "Train loss: 1.083876 [ 9600/345580]\n",
      "Train loss: 1.029674 [12800/345580]\n",
      "Train loss: 1.105476 [16000/345580]\n",
      "Train loss: 0.987660 [19200/345580]\n",
      "Train loss: 1.004824 [22400/345580]\n",
      "Train loss: 1.064349 [25600/345580]\n",
      "Train loss: 1.064249 [28800/345580]\n",
      "Train loss: 0.980477 [32000/345580]\n",
      "Train loss: 0.991596 [35200/345580]\n",
      "Train loss: 1.037192 [38400/345580]\n",
      "Train loss: 1.163687 [41600/345580]\n",
      "Train loss: 1.165800 [44800/345580]\n",
      "Train loss: 1.007388 [48000/345580]\n",
      "Train loss: 0.960836 [51200/345580]\n",
      "Train loss: 0.997590 [54400/345580]\n",
      "Train loss: 1.052742 [57600/345580]\n",
      "Train loss: 1.095344 [60800/345580]\n",
      "Train loss: 1.042507 [64000/345580]\n",
      "Train loss: 1.008568 [67200/345580]\n",
      "Train loss: 0.960807 [70400/345580]\n",
      "Train loss: 1.081139 [73600/345580]\n",
      "Train loss: 1.014913 [76800/345580]\n",
      "Train loss: 0.954623 [80000/345580]\n",
      "Train loss: 1.017093 [83200/345580]\n",
      "Train loss: 1.015888 [86400/345580]\n",
      "Train loss: 1.037047 [89600/345580]\n",
      "Train loss: 1.049269 [92800/345580]\n",
      "Train loss: 0.989453 [96000/345580]\n",
      "Train loss: 1.173834 [99200/345580]\n",
      "Train loss: 1.050211 [102400/345580]\n",
      "Train loss: 1.016848 [105600/345580]\n",
      "Train loss: 1.071513 [108800/345580]\n",
      "Train loss: 0.997253 [112000/345580]\n",
      "Train loss: 1.026781 [115200/345580]\n",
      "Train loss: 1.031621 [118400/345580]\n",
      "Train loss: 1.041086 [121600/345580]\n",
      "Train loss: 0.977674 [124800/345580]\n",
      "Train loss: 1.013195 [128000/345580]\n",
      "Train loss: 0.984383 [131200/345580]\n",
      "Train loss: 1.003573 [134400/345580]\n",
      "Train loss: 1.019843 [137600/345580]\n",
      "Train loss: 1.107459 [140800/345580]\n",
      "Train loss: 1.054572 [144000/345580]\n",
      "Train loss: 1.037139 [147200/345580]\n",
      "Train loss: 0.977344 [150400/345580]\n",
      "Train loss: 1.110084 [153600/345580]\n",
      "Train loss: 1.033834 [156800/345580]\n",
      "Train loss: 1.069358 [160000/345580]\n",
      "Train loss: 1.100481 [163200/345580]\n",
      "Train loss: 0.980120 [166400/345580]\n",
      "Train loss: 1.027448 [169600/345580]\n",
      "Train loss: 1.091282 [172800/345580]\n",
      "Train loss: 0.921670 [176000/345580]\n",
      "Train loss: 1.014588 [179200/345580]\n",
      "Train loss: 1.136027 [182400/345580]\n",
      "Train loss: 1.050840 [185600/345580]\n",
      "Train loss: 0.957153 [188800/345580]\n",
      "Train loss: 0.961618 [192000/345580]\n",
      "Train loss: 0.997421 [195200/345580]\n",
      "Train loss: 1.100327 [198400/345580]\n",
      "Train loss: 1.111475 [201600/345580]\n",
      "Train loss: 1.085751 [204800/345580]\n",
      "Train loss: 1.068085 [208000/345580]\n",
      "Train loss: 1.057111 [211200/345580]\n",
      "Train loss: 1.016178 [214400/345580]\n",
      "Train loss: 1.043138 [217600/345580]\n",
      "Train loss: 1.036424 [220800/345580]\n",
      "Train loss: 1.143091 [224000/345580]\n",
      "Train loss: 1.159347 [227200/345580]\n",
      "Train loss: 1.054286 [230400/345580]\n",
      "Train loss: 1.041887 [233600/345580]\n",
      "Train loss: 1.022484 [236800/345580]\n",
      "Train loss: 1.070615 [240000/345580]\n",
      "Train loss: 1.009306 [243200/345580]\n",
      "Train loss: 1.029856 [246400/345580]\n",
      "Train loss: 1.136521 [249600/345580]\n",
      "Train loss: 0.926259 [252800/345580]\n",
      "Train loss: 1.079362 [256000/345580]\n",
      "Train loss: 1.088892 [259200/345580]\n",
      "Train loss: 1.030609 [262400/345580]\n",
      "Train loss: 1.204731 [265600/345580]\n",
      "Train loss: 0.963515 [268800/345580]\n",
      "Train loss: 1.016477 [272000/345580]\n",
      "Train loss: 1.014539 [275200/345580]\n",
      "Train loss: 0.994703 [278400/345580]\n",
      "Train loss: 0.996668 [281600/345580]\n",
      "Train loss: 1.046079 [284800/345580]\n",
      "Train loss: 1.062413 [288000/345580]\n",
      "Train loss: 1.059482 [291200/345580]\n",
      "Train loss: 1.181069 [294400/345580]\n",
      "Train loss: 1.081298 [297600/345580]\n",
      "Train loss: 1.027617 [300800/345580]\n",
      "Train loss: 1.009878 [304000/345580]\n",
      "Train loss: 0.980044 [307200/345580]\n",
      "Train loss: 0.957421 [310400/345580]\n",
      "Train loss: 1.129277 [313600/345580]\n",
      "Train loss: 1.006819 [316800/345580]\n",
      "Train loss: 0.938282 [320000/345580]\n",
      "Train loss: 1.067646 [323200/345580]\n",
      "Train loss: 1.102839 [326400/345580]\n",
      "Train loss: 1.065609 [329600/345580]\n",
      "Train loss: 1.082604 [332800/345580]\n",
      "Train loss: 1.077885 [336000/345580]\n",
      "Train loss: 1.099971 [339200/345580]\n",
      "Train loss: 0.940684 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.041730 \n",
      "\n",
      "Epoch 9\n",
      " -------------------\n",
      "Train loss: 1.032657 [    0/345580]\n",
      "Train loss: 1.018158 [ 3200/345580]\n",
      "Train loss: 1.109162 [ 6400/345580]\n",
      "Train loss: 1.040778 [ 9600/345580]\n",
      "Train loss: 0.980242 [12800/345580]\n",
      "Train loss: 0.980681 [16000/345580]\n",
      "Train loss: 0.928752 [19200/345580]\n",
      "Train loss: 1.087183 [22400/345580]\n",
      "Train loss: 1.049943 [25600/345580]\n",
      "Train loss: 1.095212 [28800/345580]\n",
      "Train loss: 1.014428 [32000/345580]\n",
      "Train loss: 1.155512 [35200/345580]\n",
      "Train loss: 1.070101 [38400/345580]\n",
      "Train loss: 1.054624 [41600/345580]\n",
      "Train loss: 1.039570 [44800/345580]\n",
      "Train loss: 1.158717 [48000/345580]\n",
      "Train loss: 1.043889 [51200/345580]\n",
      "Train loss: 1.075050 [54400/345580]\n",
      "Train loss: 1.080841 [57600/345580]\n",
      "Train loss: 0.965171 [60800/345580]\n",
      "Train loss: 1.088564 [64000/345580]\n",
      "Train loss: 1.021173 [67200/345580]\n",
      "Train loss: 1.033207 [70400/345580]\n",
      "Train loss: 1.055931 [73600/345580]\n",
      "Train loss: 1.035738 [76800/345580]\n",
      "Train loss: 1.052751 [80000/345580]\n",
      "Train loss: 0.978463 [83200/345580]\n",
      "Train loss: 1.176438 [86400/345580]\n",
      "Train loss: 1.006664 [89600/345580]\n",
      "Train loss: 1.062818 [92800/345580]\n",
      "Train loss: 1.070787 [96000/345580]\n",
      "Train loss: 0.964460 [99200/345580]\n",
      "Train loss: 0.953883 [102400/345580]\n",
      "Train loss: 1.122023 [105600/345580]\n",
      "Train loss: 1.008574 [108800/345580]\n",
      "Train loss: 0.995981 [112000/345580]\n",
      "Train loss: 0.998698 [115200/345580]\n",
      "Train loss: 1.036537 [118400/345580]\n",
      "Train loss: 0.947013 [121600/345580]\n",
      "Train loss: 1.090106 [124800/345580]\n",
      "Train loss: 1.029114 [128000/345580]\n",
      "Train loss: 1.027865 [131200/345580]\n",
      "Train loss: 1.074574 [134400/345580]\n",
      "Train loss: 0.999606 [137600/345580]\n",
      "Train loss: 0.981199 [140800/345580]\n",
      "Train loss: 1.095370 [144000/345580]\n",
      "Train loss: 1.096673 [147200/345580]\n",
      "Train loss: 1.111233 [150400/345580]\n",
      "Train loss: 0.946030 [153600/345580]\n",
      "Train loss: 1.104138 [156800/345580]\n",
      "Train loss: 1.087207 [160000/345580]\n",
      "Train loss: 0.971228 [163200/345580]\n",
      "Train loss: 1.097754 [166400/345580]\n",
      "Train loss: 1.104063 [169600/345580]\n",
      "Train loss: 1.116794 [172800/345580]\n",
      "Train loss: 1.091637 [176000/345580]\n",
      "Train loss: 1.032078 [179200/345580]\n",
      "Train loss: 1.072984 [182400/345580]\n",
      "Train loss: 1.062471 [185600/345580]\n",
      "Train loss: 1.207557 [188800/345580]\n",
      "Train loss: 1.018805 [192000/345580]\n",
      "Train loss: 1.124577 [195200/345580]\n",
      "Train loss: 0.981158 [198400/345580]\n",
      "Train loss: 1.002834 [201600/345580]\n",
      "Train loss: 0.932997 [204800/345580]\n",
      "Train loss: 1.036853 [208000/345580]\n",
      "Train loss: 1.014709 [211200/345580]\n",
      "Train loss: 1.054164 [214400/345580]\n",
      "Train loss: 1.023821 [217600/345580]\n",
      "Train loss: 1.098207 [220800/345580]\n",
      "Train loss: 1.112814 [224000/345580]\n",
      "Train loss: 0.992026 [227200/345580]\n",
      "Train loss: 1.094813 [230400/345580]\n",
      "Train loss: 0.918613 [233600/345580]\n",
      "Train loss: 1.026046 [236800/345580]\n",
      "Train loss: 1.053001 [240000/345580]\n",
      "Train loss: 1.040592 [243200/345580]\n",
      "Train loss: 1.028737 [246400/345580]\n",
      "Train loss: 0.978202 [249600/345580]\n",
      "Train loss: 1.042632 [252800/345580]\n",
      "Train loss: 1.004703 [256000/345580]\n",
      "Train loss: 1.007304 [259200/345580]\n",
      "Train loss: 1.012260 [262400/345580]\n",
      "Train loss: 1.069914 [265600/345580]\n",
      "Train loss: 1.059500 [268800/345580]\n",
      "Train loss: 1.003982 [272000/345580]\n",
      "Train loss: 1.060547 [275200/345580]\n",
      "Train loss: 1.067032 [278400/345580]\n",
      "Train loss: 1.043211 [281600/345580]\n",
      "Train loss: 1.059686 [284800/345580]\n",
      "Train loss: 0.970719 [288000/345580]\n",
      "Train loss: 0.990379 [291200/345580]\n",
      "Train loss: 1.086867 [294400/345580]\n",
      "Train loss: 1.028295 [297600/345580]\n",
      "Train loss: 1.063837 [300800/345580]\n",
      "Train loss: 0.996901 [304000/345580]\n",
      "Train loss: 1.031322 [307200/345580]\n",
      "Train loss: 1.059924 [310400/345580]\n",
      "Train loss: 1.005443 [313600/345580]\n",
      "Train loss: 0.939587 [316800/345580]\n",
      "Train loss: 1.000706 [320000/345580]\n",
      "Train loss: 1.036377 [323200/345580]\n",
      "Train loss: 0.994898 [326400/345580]\n",
      "Train loss: 1.040745 [329600/345580]\n",
      "Train loss: 1.078068 [332800/345580]\n",
      "Train loss: 1.034942 [336000/345580]\n",
      "Train loss: 1.046308 [339200/345580]\n",
      "Train loss: 1.078772 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.041574 \n",
      "\n",
      "Epoch 10\n",
      " -------------------\n",
      "Train loss: 1.102191 [    0/345580]\n",
      "Train loss: 1.064550 [ 3200/345580]\n",
      "Train loss: 1.090735 [ 6400/345580]\n",
      "Train loss: 1.055344 [ 9600/345580]\n",
      "Train loss: 1.037544 [12800/345580]\n",
      "Train loss: 0.964446 [16000/345580]\n",
      "Train loss: 1.012362 [19200/345580]\n",
      "Train loss: 1.022795 [22400/345580]\n",
      "Train loss: 0.985054 [25600/345580]\n",
      "Train loss: 1.005055 [28800/345580]\n",
      "Train loss: 1.052484 [32000/345580]\n",
      "Train loss: 1.099026 [35200/345580]\n",
      "Train loss: 1.020110 [38400/345580]\n",
      "Train loss: 1.036756 [41600/345580]\n",
      "Train loss: 1.056395 [44800/345580]\n",
      "Train loss: 1.129634 [48000/345580]\n",
      "Train loss: 1.056702 [51200/345580]\n",
      "Train loss: 1.074243 [54400/345580]\n",
      "Train loss: 1.098561 [57600/345580]\n",
      "Train loss: 1.067167 [60800/345580]\n",
      "Train loss: 1.003249 [64000/345580]\n",
      "Train loss: 0.957781 [67200/345580]\n",
      "Train loss: 1.003000 [70400/345580]\n",
      "Train loss: 1.062180 [73600/345580]\n",
      "Train loss: 1.074738 [76800/345580]\n",
      "Train loss: 1.064358 [80000/345580]\n",
      "Train loss: 1.014050 [83200/345580]\n",
      "Train loss: 0.932391 [86400/345580]\n",
      "Train loss: 1.056859 [89600/345580]\n",
      "Train loss: 1.095300 [92800/345580]\n",
      "Train loss: 0.968661 [96000/345580]\n",
      "Train loss: 0.965160 [99200/345580]\n",
      "Train loss: 1.082388 [102400/345580]\n",
      "Train loss: 1.077792 [105600/345580]\n",
      "Train loss: 1.016536 [108800/345580]\n",
      "Train loss: 1.089440 [112000/345580]\n",
      "Train loss: 0.991407 [115200/345580]\n",
      "Train loss: 1.189853 [118400/345580]\n",
      "Train loss: 0.988837 [121600/345580]\n",
      "Train loss: 1.149744 [124800/345580]\n",
      "Train loss: 0.952755 [128000/345580]\n",
      "Train loss: 1.053142 [131200/345580]\n",
      "Train loss: 0.982210 [134400/345580]\n",
      "Train loss: 0.980091 [137600/345580]\n",
      "Train loss: 0.977385 [140800/345580]\n",
      "Train loss: 1.113974 [144000/345580]\n",
      "Train loss: 1.066854 [147200/345580]\n",
      "Train loss: 1.063254 [150400/345580]\n",
      "Train loss: 1.118654 [153600/345580]\n",
      "Train loss: 1.029639 [156800/345580]\n",
      "Train loss: 1.043807 [160000/345580]\n",
      "Train loss: 0.957441 [163200/345580]\n",
      "Train loss: 0.988330 [166400/345580]\n",
      "Train loss: 1.081136 [169600/345580]\n",
      "Train loss: 0.966878 [172800/345580]\n",
      "Train loss: 1.071410 [176000/345580]\n",
      "Train loss: 1.031215 [179200/345580]\n",
      "Train loss: 1.061404 [182400/345580]\n",
      "Train loss: 1.105636 [185600/345580]\n",
      "Train loss: 1.083254 [188800/345580]\n",
      "Train loss: 1.023687 [192000/345580]\n",
      "Train loss: 0.986717 [195200/345580]\n",
      "Train loss: 1.067333 [198400/345580]\n",
      "Train loss: 0.978487 [201600/345580]\n",
      "Train loss: 1.033351 [204800/345580]\n",
      "Train loss: 1.094364 [208000/345580]\n",
      "Train loss: 0.990412 [211200/345580]\n",
      "Train loss: 1.146419 [214400/345580]\n",
      "Train loss: 1.004125 [217600/345580]\n",
      "Train loss: 1.041424 [220800/345580]\n",
      "Train loss: 1.091062 [224000/345580]\n",
      "Train loss: 1.006723 [227200/345580]\n",
      "Train loss: 1.096206 [230400/345580]\n",
      "Train loss: 1.143387 [233600/345580]\n",
      "Train loss: 1.010121 [236800/345580]\n",
      "Train loss: 1.002998 [240000/345580]\n",
      "Train loss: 1.010779 [243200/345580]\n",
      "Train loss: 1.092942 [246400/345580]\n",
      "Train loss: 1.048787 [249600/345580]\n",
      "Train loss: 1.036663 [252800/345580]\n",
      "Train loss: 1.110648 [256000/345580]\n",
      "Train loss: 1.050629 [259200/345580]\n",
      "Train loss: 1.002211 [262400/345580]\n",
      "Train loss: 0.997606 [265600/345580]\n",
      "Train loss: 1.033668 [268800/345580]\n",
      "Train loss: 1.127538 [272000/345580]\n",
      "Train loss: 1.159992 [275200/345580]\n",
      "Train loss: 1.128772 [278400/345580]\n",
      "Train loss: 1.066345 [281600/345580]\n",
      "Train loss: 1.049270 [284800/345580]\n",
      "Train loss: 1.007557 [288000/345580]\n",
      "Train loss: 1.125411 [291200/345580]\n",
      "Train loss: 1.088692 [294400/345580]\n",
      "Train loss: 1.024578 [297600/345580]\n",
      "Train loss: 1.066823 [300800/345580]\n",
      "Train loss: 1.077629 [304000/345580]\n",
      "Train loss: 1.044855 [307200/345580]\n",
      "Train loss: 1.014970 [310400/345580]\n",
      "Train loss: 0.941105 [313600/345580]\n",
      "Train loss: 0.993787 [316800/345580]\n",
      "Train loss: 1.087251 [320000/345580]\n",
      "Train loss: 0.961684 [323200/345580]\n",
      "Train loss: 1.031257 [326400/345580]\n",
      "Train loss: 0.997740 [329600/345580]\n",
      "Train loss: 1.050310 [332800/345580]\n",
      "Train loss: 1.132788 [336000/345580]\n",
      "Train loss: 1.022006 [339200/345580]\n",
      "Train loss: 0.999133 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.042471 \n",
      "\n",
      "Epoch 11\n",
      " -------------------\n",
      "Train loss: 0.982630 [    0/345580]\n",
      "Train loss: 1.080805 [ 3200/345580]\n",
      "Train loss: 0.951486 [ 6400/345580]\n",
      "Train loss: 1.111588 [ 9600/345580]\n",
      "Train loss: 1.076040 [12800/345580]\n",
      "Train loss: 1.028505 [16000/345580]\n",
      "Train loss: 1.017481 [19200/345580]\n",
      "Train loss: 1.193906 [22400/345580]\n",
      "Train loss: 1.073156 [25600/345580]\n",
      "Train loss: 1.035089 [28800/345580]\n",
      "Train loss: 0.982120 [32000/345580]\n",
      "Train loss: 1.032314 [35200/345580]\n",
      "Train loss: 1.057829 [38400/345580]\n",
      "Train loss: 0.914583 [41600/345580]\n",
      "Train loss: 0.990955 [44800/345580]\n",
      "Train loss: 1.062584 [48000/345580]\n",
      "Train loss: 1.133626 [51200/345580]\n",
      "Train loss: 1.072948 [54400/345580]\n",
      "Train loss: 1.017491 [57600/345580]\n",
      "Train loss: 1.042503 [60800/345580]\n",
      "Train loss: 0.992779 [64000/345580]\n",
      "Train loss: 1.044288 [67200/345580]\n",
      "Train loss: 1.006195 [70400/345580]\n",
      "Train loss: 1.032610 [73600/345580]\n",
      "Train loss: 1.098853 [76800/345580]\n",
      "Train loss: 1.059262 [80000/345580]\n",
      "Train loss: 1.048599 [83200/345580]\n",
      "Train loss: 1.042850 [86400/345580]\n",
      "Train loss: 1.023522 [89600/345580]\n",
      "Train loss: 1.033388 [92800/345580]\n",
      "Train loss: 1.043448 [96000/345580]\n",
      "Train loss: 1.072371 [99200/345580]\n",
      "Train loss: 1.086867 [102400/345580]\n",
      "Train loss: 1.118682 [105600/345580]\n",
      "Train loss: 1.080461 [108800/345580]\n",
      "Train loss: 1.122815 [112000/345580]\n",
      "Train loss: 1.034111 [115200/345580]\n",
      "Train loss: 0.968671 [118400/345580]\n",
      "Train loss: 0.957286 [121600/345580]\n",
      "Train loss: 0.980800 [124800/345580]\n",
      "Train loss: 1.063001 [128000/345580]\n",
      "Train loss: 0.990994 [131200/345580]\n",
      "Train loss: 0.993700 [134400/345580]\n",
      "Train loss: 1.015909 [137600/345580]\n",
      "Train loss: 1.109632 [140800/345580]\n",
      "Train loss: 0.935881 [144000/345580]\n",
      "Train loss: 1.045537 [147200/345580]\n",
      "Train loss: 1.122705 [150400/345580]\n",
      "Train loss: 1.002182 [153600/345580]\n",
      "Train loss: 0.998482 [156800/345580]\n",
      "Train loss: 0.994083 [160000/345580]\n",
      "Train loss: 1.043983 [163200/345580]\n",
      "Train loss: 1.082915 [166400/345580]\n",
      "Train loss: 1.029306 [169600/345580]\n",
      "Train loss: 1.059544 [172800/345580]\n",
      "Train loss: 1.044152 [176000/345580]\n",
      "Train loss: 0.997416 [179200/345580]\n",
      "Train loss: 0.981473 [182400/345580]\n",
      "Train loss: 1.041754 [185600/345580]\n",
      "Train loss: 1.093073 [188800/345580]\n",
      "Train loss: 1.091609 [192000/345580]\n",
      "Train loss: 1.024757 [195200/345580]\n",
      "Train loss: 1.030205 [198400/345580]\n",
      "Train loss: 1.127253 [201600/345580]\n",
      "Train loss: 1.011428 [204800/345580]\n",
      "Train loss: 0.946581 [208000/345580]\n",
      "Train loss: 1.042827 [211200/345580]\n",
      "Train loss: 1.095091 [214400/345580]\n",
      "Train loss: 0.998962 [217600/345580]\n",
      "Train loss: 1.034741 [220800/345580]\n",
      "Train loss: 1.083756 [224000/345580]\n",
      "Train loss: 1.013319 [227200/345580]\n",
      "Train loss: 1.018966 [230400/345580]\n",
      "Train loss: 0.982037 [233600/345580]\n",
      "Train loss: 1.032919 [236800/345580]\n",
      "Train loss: 1.119036 [240000/345580]\n",
      "Train loss: 1.044569 [243200/345580]\n",
      "Train loss: 1.042246 [246400/345580]\n",
      "Train loss: 1.028660 [249600/345580]\n",
      "Train loss: 1.046382 [252800/345580]\n",
      "Train loss: 1.095152 [256000/345580]\n",
      "Train loss: 1.055174 [259200/345580]\n",
      "Train loss: 1.085299 [262400/345580]\n",
      "Train loss: 1.121127 [265600/345580]\n",
      "Train loss: 1.225987 [268800/345580]\n",
      "Train loss: 1.001431 [272000/345580]\n",
      "Train loss: 1.103297 [275200/345580]\n",
      "Train loss: 1.163143 [278400/345580]\n",
      "Train loss: 0.946051 [281600/345580]\n",
      "Train loss: 0.954218 [284800/345580]\n",
      "Train loss: 1.036721 [288000/345580]\n",
      "Train loss: 1.058629 [291200/345580]\n",
      "Train loss: 0.957670 [294400/345580]\n",
      "Train loss: 1.069032 [297600/345580]\n",
      "Train loss: 1.048755 [300800/345580]\n",
      "Train loss: 1.078648 [304000/345580]\n",
      "Train loss: 0.984855 [307200/345580]\n",
      "Train loss: 0.988808 [310400/345580]\n",
      "Train loss: 1.019307 [313600/345580]\n",
      "Train loss: 0.959731 [316800/345580]\n",
      "Train loss: 0.989591 [320000/345580]\n",
      "Train loss: 1.103290 [323200/345580]\n",
      "Train loss: 1.043722 [326400/345580]\n",
      "Train loss: 1.139706 [329600/345580]\n",
      "Train loss: 1.065475 [332800/345580]\n",
      "Train loss: 1.015027 [336000/345580]\n",
      "Train loss: 0.988733 [339200/345580]\n",
      "Train loss: 1.170614 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 42.9%, Avg loss: 1.042094 \n",
      "\n",
      "Epoch 12\n",
      " -------------------\n",
      "Train loss: 1.096670 [    0/345580]\n",
      "Train loss: 1.041061 [ 3200/345580]\n",
      "Train loss: 1.034061 [ 6400/345580]\n",
      "Train loss: 1.205688 [ 9600/345580]\n",
      "Train loss: 1.042721 [12800/345580]\n",
      "Train loss: 1.066805 [16000/345580]\n",
      "Train loss: 0.961047 [19200/345580]\n",
      "Train loss: 1.105792 [22400/345580]\n",
      "Train loss: 0.946702 [25600/345580]\n",
      "Train loss: 1.013558 [28800/345580]\n",
      "Train loss: 0.970414 [32000/345580]\n",
      "Train loss: 0.958940 [35200/345580]\n",
      "Train loss: 1.030852 [38400/345580]\n",
      "Train loss: 1.000358 [41600/345580]\n",
      "Train loss: 1.164667 [44800/345580]\n",
      "Train loss: 1.121359 [48000/345580]\n",
      "Train loss: 1.109732 [51200/345580]\n",
      "Train loss: 1.057635 [54400/345580]\n",
      "Train loss: 0.935081 [57600/345580]\n",
      "Train loss: 1.060228 [60800/345580]\n",
      "Train loss: 1.146348 [64000/345580]\n",
      "Train loss: 1.085221 [67200/345580]\n",
      "Train loss: 0.983727 [70400/345580]\n",
      "Train loss: 1.064117 [73600/345580]\n",
      "Train loss: 1.015921 [76800/345580]\n",
      "Train loss: 0.971451 [80000/345580]\n",
      "Train loss: 1.148278 [83200/345580]\n",
      "Train loss: 0.898387 [86400/345580]\n",
      "Train loss: 1.104259 [89600/345580]\n",
      "Train loss: 1.053214 [92800/345580]\n",
      "Train loss: 1.105088 [96000/345580]\n",
      "Train loss: 1.011889 [99200/345580]\n",
      "Train loss: 0.986697 [102400/345580]\n",
      "Train loss: 1.106274 [105600/345580]\n",
      "Train loss: 1.085795 [108800/345580]\n",
      "Train loss: 1.095280 [112000/345580]\n",
      "Train loss: 1.071934 [115200/345580]\n",
      "Train loss: 1.081760 [118400/345580]\n",
      "Train loss: 0.979823 [121600/345580]\n",
      "Train loss: 1.039169 [124800/345580]\n",
      "Train loss: 1.066974 [128000/345580]\n",
      "Train loss: 0.991701 [131200/345580]\n",
      "Train loss: 1.064852 [134400/345580]\n",
      "Train loss: 1.042147 [137600/345580]\n",
      "Train loss: 1.012022 [140800/345580]\n",
      "Train loss: 0.939855 [144000/345580]\n",
      "Train loss: 1.001742 [147200/345580]\n",
      "Train loss: 1.015788 [150400/345580]\n",
      "Train loss: 1.024494 [153600/345580]\n",
      "Train loss: 1.031327 [156800/345580]\n",
      "Train loss: 1.058133 [160000/345580]\n",
      "Train loss: 1.112963 [163200/345580]\n",
      "Train loss: 1.063613 [166400/345580]\n",
      "Train loss: 1.008920 [169600/345580]\n",
      "Train loss: 0.944559 [172800/345580]\n",
      "Train loss: 1.001383 [176000/345580]\n",
      "Train loss: 0.993719 [179200/345580]\n",
      "Train loss: 1.102108 [182400/345580]\n",
      "Train loss: 1.060689 [185600/345580]\n",
      "Train loss: 1.028482 [188800/345580]\n",
      "Train loss: 1.006469 [192000/345580]\n",
      "Train loss: 0.998550 [195200/345580]\n",
      "Train loss: 1.028802 [198400/345580]\n",
      "Train loss: 1.212086 [201600/345580]\n",
      "Train loss: 1.031397 [204800/345580]\n",
      "Train loss: 0.915578 [208000/345580]\n",
      "Train loss: 1.079204 [211200/345580]\n",
      "Train loss: 0.906165 [214400/345580]\n",
      "Train loss: 0.953720 [217600/345580]\n",
      "Train loss: 1.115554 [220800/345580]\n",
      "Train loss: 1.041166 [224000/345580]\n",
      "Train loss: 1.142074 [227200/345580]\n",
      "Train loss: 0.996660 [230400/345580]\n",
      "Train loss: 1.093069 [233600/345580]\n",
      "Train loss: 1.046638 [236800/345580]\n",
      "Train loss: 1.031418 [240000/345580]\n",
      "Train loss: 1.088012 [243200/345580]\n",
      "Train loss: 1.083555 [246400/345580]\n",
      "Train loss: 1.101237 [249600/345580]\n",
      "Train loss: 1.014376 [252800/345580]\n",
      "Train loss: 0.965708 [256000/345580]\n",
      "Train loss: 1.038892 [259200/345580]\n",
      "Train loss: 1.142372 [262400/345580]\n",
      "Train loss: 1.019696 [265600/345580]\n",
      "Train loss: 1.088855 [268800/345580]\n",
      "Train loss: 1.046241 [272000/345580]\n",
      "Train loss: 0.906803 [275200/345580]\n",
      "Train loss: 1.022309 [278400/345580]\n",
      "Train loss: 0.990230 [281600/345580]\n",
      "Train loss: 1.019891 [284800/345580]\n",
      "Train loss: 1.012786 [288000/345580]\n",
      "Train loss: 0.951875 [291200/345580]\n",
      "Train loss: 0.965090 [294400/345580]\n",
      "Train loss: 1.050627 [297600/345580]\n",
      "Train loss: 1.043302 [300800/345580]\n",
      "Train loss: 1.081875 [304000/345580]\n",
      "Train loss: 1.043430 [307200/345580]\n",
      "Train loss: 1.112556 [310400/345580]\n",
      "Train loss: 1.049661 [313600/345580]\n",
      "Train loss: 0.983022 [316800/345580]\n",
      "Train loss: 1.066800 [320000/345580]\n",
      "Train loss: 1.090223 [323200/345580]\n",
      "Train loss: 1.067020 [326400/345580]\n",
      "Train loss: 1.070341 [329600/345580]\n",
      "Train loss: 1.007826 [332800/345580]\n",
      "Train loss: 1.077830 [336000/345580]\n",
      "Train loss: 1.076559 [339200/345580]\n",
      "Train loss: 1.059378 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.041671 \n",
      "\n",
      "Epoch 13\n",
      " -------------------\n",
      "Train loss: 0.977077 [    0/345580]\n",
      "Train loss: 1.083290 [ 3200/345580]\n",
      "Train loss: 1.083273 [ 6400/345580]\n",
      "Train loss: 1.017400 [ 9600/345580]\n",
      "Train loss: 0.951389 [12800/345580]\n",
      "Train loss: 1.089166 [16000/345580]\n",
      "Train loss: 1.105946 [19200/345580]\n",
      "Train loss: 1.018707 [22400/345580]\n",
      "Train loss: 1.097648 [25600/345580]\n",
      "Train loss: 1.056347 [28800/345580]\n",
      "Train loss: 0.991905 [32000/345580]\n",
      "Train loss: 1.038397 [35200/345580]\n",
      "Train loss: 1.026738 [38400/345580]\n",
      "Train loss: 0.986932 [41600/345580]\n",
      "Train loss: 1.120440 [44800/345580]\n",
      "Train loss: 1.158173 [48000/345580]\n",
      "Train loss: 1.070707 [51200/345580]\n",
      "Train loss: 1.135986 [54400/345580]\n",
      "Train loss: 1.019257 [57600/345580]\n",
      "Train loss: 1.048832 [60800/345580]\n",
      "Train loss: 0.968895 [64000/345580]\n",
      "Train loss: 0.979393 [67200/345580]\n",
      "Train loss: 1.082312 [70400/345580]\n",
      "Train loss: 1.012862 [73600/345580]\n",
      "Train loss: 1.077158 [76800/345580]\n",
      "Train loss: 1.123381 [80000/345580]\n",
      "Train loss: 0.996292 [83200/345580]\n",
      "Train loss: 1.058676 [86400/345580]\n",
      "Train loss: 1.013649 [89600/345580]\n",
      "Train loss: 1.063099 [92800/345580]\n",
      "Train loss: 1.021864 [96000/345580]\n",
      "Train loss: 1.072489 [99200/345580]\n",
      "Train loss: 1.104608 [102400/345580]\n",
      "Train loss: 1.107632 [105600/345580]\n",
      "Train loss: 1.036471 [108800/345580]\n",
      "Train loss: 1.182249 [112000/345580]\n",
      "Train loss: 0.954982 [115200/345580]\n",
      "Train loss: 1.054903 [118400/345580]\n",
      "Train loss: 1.010674 [121600/345580]\n",
      "Train loss: 0.973545 [124800/345580]\n",
      "Train loss: 1.009078 [128000/345580]\n",
      "Train loss: 0.972827 [131200/345580]\n",
      "Train loss: 1.000771 [134400/345580]\n",
      "Train loss: 1.099570 [137600/345580]\n",
      "Train loss: 1.054230 [140800/345580]\n",
      "Train loss: 1.105532 [144000/345580]\n",
      "Train loss: 1.052855 [147200/345580]\n",
      "Train loss: 0.979202 [150400/345580]\n",
      "Train loss: 1.030490 [153600/345580]\n",
      "Train loss: 0.972352 [156800/345580]\n",
      "Train loss: 0.994008 [160000/345580]\n",
      "Train loss: 1.059618 [163200/345580]\n",
      "Train loss: 0.983270 [166400/345580]\n",
      "Train loss: 1.047719 [169600/345580]\n",
      "Train loss: 1.106039 [172800/345580]\n",
      "Train loss: 1.169697 [176000/345580]\n",
      "Train loss: 1.063862 [179200/345580]\n",
      "Train loss: 1.017619 [182400/345580]\n",
      "Train loss: 1.023610 [185600/345580]\n",
      "Train loss: 1.020055 [188800/345580]\n",
      "Train loss: 1.012181 [192000/345580]\n",
      "Train loss: 1.102543 [195200/345580]\n",
      "Train loss: 1.029495 [198400/345580]\n",
      "Train loss: 1.036360 [201600/345580]\n",
      "Train loss: 1.111055 [204800/345580]\n",
      "Train loss: 0.976333 [208000/345580]\n",
      "Train loss: 1.067251 [211200/345580]\n",
      "Train loss: 1.006306 [214400/345580]\n",
      "Train loss: 1.193083 [217600/345580]\n",
      "Train loss: 1.052195 [220800/345580]\n",
      "Train loss: 0.963099 [224000/345580]\n",
      "Train loss: 0.997564 [227200/345580]\n",
      "Train loss: 1.043231 [230400/345580]\n",
      "Train loss: 1.041660 [233600/345580]\n",
      "Train loss: 0.951074 [236800/345580]\n",
      "Train loss: 1.011772 [240000/345580]\n",
      "Train loss: 1.104610 [243200/345580]\n",
      "Train loss: 1.067766 [246400/345580]\n",
      "Train loss: 1.052702 [249600/345580]\n",
      "Train loss: 1.060629 [252800/345580]\n",
      "Train loss: 1.072982 [256000/345580]\n",
      "Train loss: 0.944685 [259200/345580]\n",
      "Train loss: 1.020398 [262400/345580]\n",
      "Train loss: 1.030566 [265600/345580]\n",
      "Train loss: 1.121988 [268800/345580]\n",
      "Train loss: 1.170547 [272000/345580]\n",
      "Train loss: 1.090602 [275200/345580]\n",
      "Train loss: 1.069178 [278400/345580]\n",
      "Train loss: 1.052100 [281600/345580]\n",
      "Train loss: 0.988239 [284800/345580]\n",
      "Train loss: 1.120067 [288000/345580]\n",
      "Train loss: 1.140913 [291200/345580]\n",
      "Train loss: 1.109651 [294400/345580]\n",
      "Train loss: 1.047420 [297600/345580]\n",
      "Train loss: 0.972028 [300800/345580]\n",
      "Train loss: 1.030926 [304000/345580]\n",
      "Train loss: 0.947126 [307200/345580]\n",
      "Train loss: 0.999367 [310400/345580]\n",
      "Train loss: 1.085546 [313600/345580]\n",
      "Train loss: 0.977237 [316800/345580]\n",
      "Train loss: 0.986147 [320000/345580]\n",
      "Train loss: 1.002784 [323200/345580]\n",
      "Train loss: 1.039982 [326400/345580]\n",
      "Train loss: 1.025797 [329600/345580]\n",
      "Train loss: 1.005755 [332800/345580]\n",
      "Train loss: 1.094345 [336000/345580]\n",
      "Train loss: 0.992416 [339200/345580]\n",
      "Train loss: 1.062353 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.3%, Avg loss: 1.042507 \n",
      "\n",
      "Epoch 14\n",
      " -------------------\n",
      "Train loss: 1.084508 [    0/345580]\n",
      "Train loss: 1.001748 [ 3200/345580]\n",
      "Train loss: 1.023098 [ 6400/345580]\n",
      "Train loss: 1.098283 [ 9600/345580]\n",
      "Train loss: 1.011074 [12800/345580]\n",
      "Train loss: 1.103846 [16000/345580]\n",
      "Train loss: 1.007240 [19200/345580]\n",
      "Train loss: 1.027232 [22400/345580]\n",
      "Train loss: 0.977784 [25600/345580]\n",
      "Train loss: 1.001541 [28800/345580]\n",
      "Train loss: 1.069792 [32000/345580]\n",
      "Train loss: 0.945868 [35200/345580]\n",
      "Train loss: 1.008403 [38400/345580]\n",
      "Train loss: 1.116593 [41600/345580]\n",
      "Train loss: 1.094893 [44800/345580]\n",
      "Train loss: 1.081278 [48000/345580]\n",
      "Train loss: 1.150178 [51200/345580]\n",
      "Train loss: 0.950545 [54400/345580]\n",
      "Train loss: 0.991419 [57600/345580]\n",
      "Train loss: 1.051971 [60800/345580]\n",
      "Train loss: 1.001205 [64000/345580]\n",
      "Train loss: 1.083451 [67200/345580]\n",
      "Train loss: 1.056511 [70400/345580]\n",
      "Train loss: 1.009233 [73600/345580]\n",
      "Train loss: 1.026254 [76800/345580]\n",
      "Train loss: 0.947848 [80000/345580]\n",
      "Train loss: 1.081257 [83200/345580]\n",
      "Train loss: 1.016122 [86400/345580]\n",
      "Train loss: 0.945886 [89600/345580]\n",
      "Train loss: 1.071838 [92800/345580]\n",
      "Train loss: 1.026940 [96000/345580]\n",
      "Train loss: 1.069312 [99200/345580]\n",
      "Train loss: 0.994500 [102400/345580]\n",
      "Train loss: 1.032575 [105600/345580]\n",
      "Train loss: 1.045524 [108800/345580]\n",
      "Train loss: 1.135867 [112000/345580]\n",
      "Train loss: 1.042142 [115200/345580]\n",
      "Train loss: 1.048843 [118400/345580]\n",
      "Train loss: 0.997303 [121600/345580]\n",
      "Train loss: 1.021606 [124800/345580]\n",
      "Train loss: 1.095414 [128000/345580]\n",
      "Train loss: 1.040217 [131200/345580]\n",
      "Train loss: 1.043653 [134400/345580]\n",
      "Train loss: 1.019071 [137600/345580]\n",
      "Train loss: 1.037501 [140800/345580]\n",
      "Train loss: 1.108430 [144000/345580]\n",
      "Train loss: 1.029393 [147200/345580]\n",
      "Train loss: 1.048373 [150400/345580]\n",
      "Train loss: 1.075985 [153600/345580]\n",
      "Train loss: 1.034062 [156800/345580]\n",
      "Train loss: 1.039094 [160000/345580]\n",
      "Train loss: 0.991776 [163200/345580]\n",
      "Train loss: 1.087797 [166400/345580]\n",
      "Train loss: 1.003572 [169600/345580]\n",
      "Train loss: 1.034723 [172800/345580]\n",
      "Train loss: 1.027526 [176000/345580]\n",
      "Train loss: 1.073739 [179200/345580]\n",
      "Train loss: 1.055725 [182400/345580]\n",
      "Train loss: 1.061764 [185600/345580]\n",
      "Train loss: 1.111188 [188800/345580]\n",
      "Train loss: 1.062889 [192000/345580]\n",
      "Train loss: 1.022366 [195200/345580]\n",
      "Train loss: 1.028748 [198400/345580]\n",
      "Train loss: 1.094121 [201600/345580]\n",
      "Train loss: 1.024094 [204800/345580]\n",
      "Train loss: 1.011815 [208000/345580]\n",
      "Train loss: 0.959409 [211200/345580]\n",
      "Train loss: 1.052938 [214400/345580]\n",
      "Train loss: 0.967260 [217600/345580]\n",
      "Train loss: 1.053335 [220800/345580]\n",
      "Train loss: 1.080784 [224000/345580]\n",
      "Train loss: 1.093981 [227200/345580]\n",
      "Train loss: 1.166538 [230400/345580]\n",
      "Train loss: 1.110747 [233600/345580]\n",
      "Train loss: 1.035024 [236800/345580]\n",
      "Train loss: 1.090142 [240000/345580]\n",
      "Train loss: 1.062982 [243200/345580]\n",
      "Train loss: 1.032811 [246400/345580]\n",
      "Train loss: 1.043952 [249600/345580]\n",
      "Train loss: 1.086207 [252800/345580]\n",
      "Train loss: 1.084394 [256000/345580]\n",
      "Train loss: 1.033349 [259200/345580]\n",
      "Train loss: 1.069219 [262400/345580]\n",
      "Train loss: 1.127476 [265600/345580]\n",
      "Train loss: 0.970807 [268800/345580]\n",
      "Train loss: 1.069505 [272000/345580]\n",
      "Train loss: 1.015337 [275200/345580]\n",
      "Train loss: 1.107376 [278400/345580]\n",
      "Train loss: 1.063300 [281600/345580]\n",
      "Train loss: 1.105286 [284800/345580]\n",
      "Train loss: 1.102014 [288000/345580]\n",
      "Train loss: 1.035575 [291200/345580]\n",
      "Train loss: 1.070128 [294400/345580]\n",
      "Train loss: 1.039117 [297600/345580]\n",
      "Train loss: 1.019791 [300800/345580]\n",
      "Train loss: 1.057954 [304000/345580]\n",
      "Train loss: 1.044327 [307200/345580]\n",
      "Train loss: 1.002535 [310400/345580]\n",
      "Train loss: 0.976473 [313600/345580]\n",
      "Train loss: 1.077762 [316800/345580]\n",
      "Train loss: 1.051773 [320000/345580]\n",
      "Train loss: 0.989904 [323200/345580]\n",
      "Train loss: 1.104368 [326400/345580]\n",
      "Train loss: 1.007646 [329600/345580]\n",
      "Train loss: 1.036795 [332800/345580]\n",
      "Train loss: 1.095521 [336000/345580]\n",
      "Train loss: 1.019734 [339200/345580]\n",
      "Train loss: 0.997471 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.042121 \n",
      "\n",
      "Epoch 15\n",
      " -------------------\n",
      "Train loss: 0.990732 [    0/345580]\n",
      "Train loss: 1.140773 [ 3200/345580]\n",
      "Train loss: 1.058173 [ 6400/345580]\n",
      "Train loss: 1.126055 [ 9600/345580]\n",
      "Train loss: 0.964166 [12800/345580]\n",
      "Train loss: 0.999696 [16000/345580]\n",
      "Train loss: 1.094122 [19200/345580]\n",
      "Train loss: 0.954241 [22400/345580]\n",
      "Train loss: 1.003304 [25600/345580]\n",
      "Train loss: 1.073192 [28800/345580]\n",
      "Train loss: 1.111680 [32000/345580]\n",
      "Train loss: 1.180276 [35200/345580]\n",
      "Train loss: 1.065343 [38400/345580]\n",
      "Train loss: 0.967645 [41600/345580]\n",
      "Train loss: 1.130888 [44800/345580]\n",
      "Train loss: 1.210425 [48000/345580]\n",
      "Train loss: 1.062149 [51200/345580]\n",
      "Train loss: 1.054559 [54400/345580]\n",
      "Train loss: 1.060198 [57600/345580]\n",
      "Train loss: 0.985737 [60800/345580]\n",
      "Train loss: 1.015448 [64000/345580]\n",
      "Train loss: 1.025273 [67200/345580]\n",
      "Train loss: 1.048583 [70400/345580]\n",
      "Train loss: 1.009049 [73600/345580]\n",
      "Train loss: 1.024663 [76800/345580]\n",
      "Train loss: 1.007373 [80000/345580]\n",
      "Train loss: 0.932793 [83200/345580]\n",
      "Train loss: 1.029711 [86400/345580]\n",
      "Train loss: 1.002357 [89600/345580]\n",
      "Train loss: 1.008951 [92800/345580]\n",
      "Train loss: 0.980761 [96000/345580]\n",
      "Train loss: 1.142729 [99200/345580]\n",
      "Train loss: 1.080786 [102400/345580]\n",
      "Train loss: 1.016599 [105600/345580]\n",
      "Train loss: 1.081607 [108800/345580]\n",
      "Train loss: 1.096894 [112000/345580]\n",
      "Train loss: 1.030600 [115200/345580]\n",
      "Train loss: 1.072989 [118400/345580]\n",
      "Train loss: 1.086297 [121600/345580]\n",
      "Train loss: 1.041877 [124800/345580]\n",
      "Train loss: 0.945244 [128000/345580]\n",
      "Train loss: 1.044999 [131200/345580]\n",
      "Train loss: 1.011129 [134400/345580]\n",
      "Train loss: 1.030923 [137600/345580]\n",
      "Train loss: 1.004270 [140800/345580]\n",
      "Train loss: 1.076742 [144000/345580]\n",
      "Train loss: 1.112359 [147200/345580]\n",
      "Train loss: 0.960426 [150400/345580]\n",
      "Train loss: 1.152046 [153600/345580]\n",
      "Train loss: 1.037157 [156800/345580]\n",
      "Train loss: 1.124142 [160000/345580]\n",
      "Train loss: 1.026991 [163200/345580]\n",
      "Train loss: 1.037393 [166400/345580]\n",
      "Train loss: 1.049169 [169600/345580]\n",
      "Train loss: 1.104141 [172800/345580]\n",
      "Train loss: 1.169228 [176000/345580]\n",
      "Train loss: 1.033941 [179200/345580]\n",
      "Train loss: 1.033356 [182400/345580]\n",
      "Train loss: 0.977684 [185600/345580]\n",
      "Train loss: 1.158647 [188800/345580]\n",
      "Train loss: 1.062231 [192000/345580]\n",
      "Train loss: 0.945825 [195200/345580]\n",
      "Train loss: 1.070559 [198400/345580]\n",
      "Train loss: 1.138945 [201600/345580]\n",
      "Train loss: 1.097688 [204800/345580]\n",
      "Train loss: 1.062242 [208000/345580]\n",
      "Train loss: 0.974330 [211200/345580]\n",
      "Train loss: 1.023293 [214400/345580]\n",
      "Train loss: 1.100644 [217600/345580]\n",
      "Train loss: 0.945821 [220800/345580]\n",
      "Train loss: 1.140671 [224000/345580]\n",
      "Train loss: 0.990782 [227200/345580]\n",
      "Train loss: 0.988055 [230400/345580]\n",
      "Train loss: 1.156164 [233600/345580]\n",
      "Train loss: 1.048127 [236800/345580]\n",
      "Train loss: 1.092295 [240000/345580]\n",
      "Train loss: 1.032487 [243200/345580]\n",
      "Train loss: 1.108990 [246400/345580]\n",
      "Train loss: 1.125067 [249600/345580]\n",
      "Train loss: 1.026111 [252800/345580]\n",
      "Train loss: 1.137236 [256000/345580]\n",
      "Train loss: 1.064299 [259200/345580]\n",
      "Train loss: 1.083361 [262400/345580]\n",
      "Train loss: 1.065596 [265600/345580]\n",
      "Train loss: 1.127647 [268800/345580]\n",
      "Train loss: 1.013120 [272000/345580]\n",
      "Train loss: 1.069275 [275200/345580]\n",
      "Train loss: 1.038680 [278400/345580]\n",
      "Train loss: 1.009671 [281600/345580]\n",
      "Train loss: 1.070261 [284800/345580]\n",
      "Train loss: 1.018584 [288000/345580]\n",
      "Train loss: 1.082936 [291200/345580]\n",
      "Train loss: 1.015530 [294400/345580]\n",
      "Train loss: 0.999440 [297600/345580]\n",
      "Train loss: 1.075483 [300800/345580]\n",
      "Train loss: 1.135044 [304000/345580]\n",
      "Train loss: 1.117057 [307200/345580]\n",
      "Train loss: 1.060439 [310400/345580]\n",
      "Train loss: 0.973335 [313600/345580]\n",
      "Train loss: 1.211777 [316800/345580]\n",
      "Train loss: 1.073409 [320000/345580]\n",
      "Train loss: 1.060492 [323200/345580]\n",
      "Train loss: 1.074141 [326400/345580]\n",
      "Train loss: 0.943976 [329600/345580]\n",
      "Train loss: 1.089152 [332800/345580]\n",
      "Train loss: 1.015567 [336000/345580]\n",
      "Train loss: 1.074733 [339200/345580]\n",
      "Train loss: 1.152700 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.043322 \n",
      "\n",
      "Epoch 16\n",
      " -------------------\n",
      "Train loss: 1.006758 [    0/345580]\n",
      "Train loss: 1.024192 [ 3200/345580]\n",
      "Train loss: 1.022889 [ 6400/345580]\n",
      "Train loss: 0.981103 [ 9600/345580]\n",
      "Train loss: 1.070879 [12800/345580]\n",
      "Train loss: 1.022072 [16000/345580]\n",
      "Train loss: 1.083398 [19200/345580]\n",
      "Train loss: 1.137807 [22400/345580]\n",
      "Train loss: 0.962754 [25600/345580]\n",
      "Train loss: 1.097357 [28800/345580]\n",
      "Train loss: 0.971473 [32000/345580]\n",
      "Train loss: 1.048038 [35200/345580]\n",
      "Train loss: 0.982831 [38400/345580]\n",
      "Train loss: 1.093199 [41600/345580]\n",
      "Train loss: 1.120792 [44800/345580]\n",
      "Train loss: 1.053828 [48000/345580]\n",
      "Train loss: 1.112675 [51200/345580]\n",
      "Train loss: 1.036145 [54400/345580]\n",
      "Train loss: 1.048964 [57600/345580]\n",
      "Train loss: 0.984594 [60800/345580]\n",
      "Train loss: 0.977188 [64000/345580]\n",
      "Train loss: 1.099723 [67200/345580]\n",
      "Train loss: 1.139864 [70400/345580]\n",
      "Train loss: 0.999842 [73600/345580]\n",
      "Train loss: 0.951303 [76800/345580]\n",
      "Train loss: 1.038386 [80000/345580]\n",
      "Train loss: 0.954521 [83200/345580]\n",
      "Train loss: 0.992211 [86400/345580]\n",
      "Train loss: 1.068047 [89600/345580]\n",
      "Train loss: 0.997270 [92800/345580]\n",
      "Train loss: 1.005180 [96000/345580]\n",
      "Train loss: 1.051715 [99200/345580]\n",
      "Train loss: 1.046261 [102400/345580]\n",
      "Train loss: 1.076385 [105600/345580]\n",
      "Train loss: 1.075314 [108800/345580]\n",
      "Train loss: 1.117799 [112000/345580]\n",
      "Train loss: 1.062605 [115200/345580]\n",
      "Train loss: 1.005028 [118400/345580]\n",
      "Train loss: 1.010854 [121600/345580]\n",
      "Train loss: 1.040254 [124800/345580]\n",
      "Train loss: 1.036084 [128000/345580]\n",
      "Train loss: 1.018696 [131200/345580]\n",
      "Train loss: 0.993482 [134400/345580]\n",
      "Train loss: 1.039162 [137600/345580]\n",
      "Train loss: 0.991255 [140800/345580]\n",
      "Train loss: 0.985980 [144000/345580]\n",
      "Train loss: 1.017434 [147200/345580]\n",
      "Train loss: 1.047910 [150400/345580]\n",
      "Train loss: 0.959427 [153600/345580]\n",
      "Train loss: 1.053261 [156800/345580]\n",
      "Train loss: 1.019288 [160000/345580]\n",
      "Train loss: 1.027147 [163200/345580]\n",
      "Train loss: 1.014187 [166400/345580]\n",
      "Train loss: 1.006014 [169600/345580]\n",
      "Train loss: 1.022045 [172800/345580]\n",
      "Train loss: 1.047015 [176000/345580]\n",
      "Train loss: 1.015951 [179200/345580]\n",
      "Train loss: 1.051784 [182400/345580]\n",
      "Train loss: 1.113880 [185600/345580]\n",
      "Train loss: 0.934143 [188800/345580]\n",
      "Train loss: 1.019841 [192000/345580]\n",
      "Train loss: 1.041098 [195200/345580]\n",
      "Train loss: 1.062244 [198400/345580]\n",
      "Train loss: 0.980909 [201600/345580]\n",
      "Train loss: 0.993577 [204800/345580]\n",
      "Train loss: 1.061364 [208000/345580]\n",
      "Train loss: 1.099979 [211200/345580]\n",
      "Train loss: 0.941017 [214400/345580]\n",
      "Train loss: 1.078920 [217600/345580]\n",
      "Train loss: 1.009751 [220800/345580]\n",
      "Train loss: 1.076819 [224000/345580]\n",
      "Train loss: 1.029201 [227200/345580]\n",
      "Train loss: 1.153847 [230400/345580]\n",
      "Train loss: 1.006723 [233600/345580]\n",
      "Train loss: 1.052826 [236800/345580]\n",
      "Train loss: 1.009970 [240000/345580]\n",
      "Train loss: 1.028020 [243200/345580]\n",
      "Train loss: 0.988374 [246400/345580]\n",
      "Train loss: 1.025476 [249600/345580]\n",
      "Train loss: 1.027302 [252800/345580]\n",
      "Train loss: 1.144527 [256000/345580]\n",
      "Train loss: 1.020179 [259200/345580]\n",
      "Train loss: 1.127612 [262400/345580]\n",
      "Train loss: 1.026878 [265600/345580]\n",
      "Train loss: 1.037067 [268800/345580]\n",
      "Train loss: 1.026037 [272000/345580]\n",
      "Train loss: 1.040550 [275200/345580]\n",
      "Train loss: 1.024523 [278400/345580]\n",
      "Train loss: 0.969594 [281600/345580]\n",
      "Train loss: 1.073528 [284800/345580]\n",
      "Train loss: 0.992164 [288000/345580]\n",
      "Train loss: 1.053161 [291200/345580]\n",
      "Train loss: 1.039551 [294400/345580]\n",
      "Train loss: 1.029607 [297600/345580]\n",
      "Train loss: 0.933016 [300800/345580]\n",
      "Train loss: 1.036161 [304000/345580]\n",
      "Train loss: 1.036673 [307200/345580]\n",
      "Train loss: 0.951559 [310400/345580]\n",
      "Train loss: 1.062827 [313600/345580]\n",
      "Train loss: 1.066186 [316800/345580]\n",
      "Train loss: 1.022208 [320000/345580]\n",
      "Train loss: 1.007523 [323200/345580]\n",
      "Train loss: 0.990584 [326400/345580]\n",
      "Train loss: 0.991064 [329600/345580]\n",
      "Train loss: 0.968738 [332800/345580]\n",
      "Train loss: 1.094074 [336000/345580]\n",
      "Train loss: 1.011479 [339200/345580]\n",
      "Train loss: 1.088676 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.042880 \n",
      "\n",
      "Epoch 17\n",
      " -------------------\n",
      "Train loss: 1.083606 [    0/345580]\n",
      "Train loss: 0.988965 [ 3200/345580]\n",
      "Train loss: 1.135972 [ 6400/345580]\n",
      "Train loss: 1.159343 [ 9600/345580]\n",
      "Train loss: 1.023853 [12800/345580]\n",
      "Train loss: 1.007084 [16000/345580]\n",
      "Train loss: 1.059441 [19200/345580]\n",
      "Train loss: 1.104812 [22400/345580]\n",
      "Train loss: 1.111971 [25600/345580]\n",
      "Train loss: 1.100934 [28800/345580]\n",
      "Train loss: 1.021983 [32000/345580]\n",
      "Train loss: 0.976074 [35200/345580]\n",
      "Train loss: 0.963723 [38400/345580]\n",
      "Train loss: 1.005365 [41600/345580]\n",
      "Train loss: 1.046820 [44800/345580]\n",
      "Train loss: 1.093726 [48000/345580]\n",
      "Train loss: 1.017848 [51200/345580]\n",
      "Train loss: 0.998541 [54400/345580]\n",
      "Train loss: 0.962674 [57600/345580]\n",
      "Train loss: 0.996652 [60800/345580]\n",
      "Train loss: 1.030677 [64000/345580]\n",
      "Train loss: 1.147744 [67200/345580]\n",
      "Train loss: 0.981816 [70400/345580]\n",
      "Train loss: 1.032196 [73600/345580]\n",
      "Train loss: 0.929186 [76800/345580]\n",
      "Train loss: 1.021979 [80000/345580]\n",
      "Train loss: 1.000489 [83200/345580]\n",
      "Train loss: 1.039558 [86400/345580]\n",
      "Train loss: 1.023518 [89600/345580]\n",
      "Train loss: 1.088451 [92800/345580]\n",
      "Train loss: 1.065478 [96000/345580]\n",
      "Train loss: 0.961193 [99200/345580]\n",
      "Train loss: 1.106032 [102400/345580]\n",
      "Train loss: 1.051360 [105600/345580]\n",
      "Train loss: 1.047819 [108800/345580]\n",
      "Train loss: 0.998016 [112000/345580]\n",
      "Train loss: 1.169621 [115200/345580]\n",
      "Train loss: 1.168899 [118400/345580]\n",
      "Train loss: 1.096519 [121600/345580]\n",
      "Train loss: 1.126811 [124800/345580]\n",
      "Train loss: 0.900611 [128000/345580]\n",
      "Train loss: 1.050924 [131200/345580]\n",
      "Train loss: 1.031411 [134400/345580]\n",
      "Train loss: 1.071698 [137600/345580]\n",
      "Train loss: 1.025778 [140800/345580]\n",
      "Train loss: 0.977448 [144000/345580]\n",
      "Train loss: 0.981025 [147200/345580]\n",
      "Train loss: 1.006304 [150400/345580]\n",
      "Train loss: 1.063951 [153600/345580]\n",
      "Train loss: 1.008762 [156800/345580]\n",
      "Train loss: 1.046762 [160000/345580]\n",
      "Train loss: 1.130880 [163200/345580]\n",
      "Train loss: 1.081338 [166400/345580]\n",
      "Train loss: 1.053741 [169600/345580]\n",
      "Train loss: 1.002884 [172800/345580]\n",
      "Train loss: 1.025711 [176000/345580]\n",
      "Train loss: 1.080890 [179200/345580]\n",
      "Train loss: 1.049678 [182400/345580]\n",
      "Train loss: 1.064127 [185600/345580]\n",
      "Train loss: 1.024703 [188800/345580]\n",
      "Train loss: 1.012767 [192000/345580]\n",
      "Train loss: 1.120412 [195200/345580]\n",
      "Train loss: 1.003307 [198400/345580]\n",
      "Train loss: 1.005750 [201600/345580]\n",
      "Train loss: 1.077325 [204800/345580]\n",
      "Train loss: 1.035940 [208000/345580]\n",
      "Train loss: 1.025294 [211200/345580]\n",
      "Train loss: 1.063355 [214400/345580]\n",
      "Train loss: 0.982682 [217600/345580]\n",
      "Train loss: 1.035070 [220800/345580]\n",
      "Train loss: 1.061804 [224000/345580]\n",
      "Train loss: 1.105177 [227200/345580]\n",
      "Train loss: 1.047360 [230400/345580]\n",
      "Train loss: 1.078702 [233600/345580]\n",
      "Train loss: 1.057051 [236800/345580]\n",
      "Train loss: 1.013564 [240000/345580]\n",
      "Train loss: 1.041745 [243200/345580]\n",
      "Train loss: 1.083316 [246400/345580]\n",
      "Train loss: 0.924319 [249600/345580]\n",
      "Train loss: 0.972036 [252800/345580]\n",
      "Train loss: 1.015978 [256000/345580]\n",
      "Train loss: 1.003209 [259200/345580]\n",
      "Train loss: 1.054180 [262400/345580]\n",
      "Train loss: 1.074560 [265600/345580]\n",
      "Train loss: 1.045796 [268800/345580]\n",
      "Train loss: 1.147229 [272000/345580]\n",
      "Train loss: 1.045921 [275200/345580]\n",
      "Train loss: 0.992048 [278400/345580]\n",
      "Train loss: 1.007063 [281600/345580]\n",
      "Train loss: 1.043376 [284800/345580]\n",
      "Train loss: 1.028245 [288000/345580]\n",
      "Train loss: 1.019503 [291200/345580]\n",
      "Train loss: 1.097458 [294400/345580]\n",
      "Train loss: 1.035394 [297600/345580]\n",
      "Train loss: 1.059830 [300800/345580]\n",
      "Train loss: 0.979501 [304000/345580]\n",
      "Train loss: 1.048376 [307200/345580]\n",
      "Train loss: 1.064437 [310400/345580]\n",
      "Train loss: 1.029787 [313600/345580]\n",
      "Train loss: 1.075580 [316800/345580]\n",
      "Train loss: 1.000325 [320000/345580]\n",
      "Train loss: 1.211928 [323200/345580]\n",
      "Train loss: 1.263168 [326400/345580]\n",
      "Train loss: 0.984651 [329600/345580]\n",
      "Train loss: 1.005393 [332800/345580]\n",
      "Train loss: 1.150625 [336000/345580]\n",
      "Train loss: 1.076882 [339200/345580]\n",
      "Train loss: 0.958156 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.041576 \n",
      "\n",
      "Epoch 18\n",
      " -------------------\n",
      "Train loss: 1.041309 [    0/345580]\n",
      "Train loss: 0.964756 [ 3200/345580]\n",
      "Train loss: 1.109048 [ 6400/345580]\n",
      "Train loss: 1.007561 [ 9600/345580]\n",
      "Train loss: 1.098181 [12800/345580]\n",
      "Train loss: 1.094645 [16000/345580]\n",
      "Train loss: 1.065070 [19200/345580]\n",
      "Train loss: 1.066478 [22400/345580]\n",
      "Train loss: 1.054609 [25600/345580]\n",
      "Train loss: 1.027634 [28800/345580]\n",
      "Train loss: 1.048312 [32000/345580]\n",
      "Train loss: 1.081686 [35200/345580]\n",
      "Train loss: 0.998719 [38400/345580]\n",
      "Train loss: 1.076165 [41600/345580]\n",
      "Train loss: 1.049744 [44800/345580]\n",
      "Train loss: 1.087974 [48000/345580]\n",
      "Train loss: 0.905255 [51200/345580]\n",
      "Train loss: 1.030398 [54400/345580]\n",
      "Train loss: 1.045090 [57600/345580]\n",
      "Train loss: 1.141813 [60800/345580]\n",
      "Train loss: 1.045357 [64000/345580]\n",
      "Train loss: 1.032804 [67200/345580]\n",
      "Train loss: 0.978379 [70400/345580]\n",
      "Train loss: 0.984220 [73600/345580]\n",
      "Train loss: 0.995366 [76800/345580]\n",
      "Train loss: 0.976706 [80000/345580]\n",
      "Train loss: 1.150549 [83200/345580]\n",
      "Train loss: 1.056401 [86400/345580]\n",
      "Train loss: 1.000062 [89600/345580]\n",
      "Train loss: 1.098985 [92800/345580]\n",
      "Train loss: 1.045921 [96000/345580]\n",
      "Train loss: 0.933747 [99200/345580]\n",
      "Train loss: 1.069207 [102400/345580]\n",
      "Train loss: 0.968569 [105600/345580]\n",
      "Train loss: 1.072896 [108800/345580]\n",
      "Train loss: 1.082109 [112000/345580]\n",
      "Train loss: 1.126498 [115200/345580]\n",
      "Train loss: 1.046598 [118400/345580]\n",
      "Train loss: 1.062786 [121600/345580]\n",
      "Train loss: 1.128836 [124800/345580]\n",
      "Train loss: 1.086790 [128000/345580]\n",
      "Train loss: 1.014703 [131200/345580]\n",
      "Train loss: 1.045285 [134400/345580]\n",
      "Train loss: 1.035804 [137600/345580]\n",
      "Train loss: 1.033853 [140800/345580]\n",
      "Train loss: 1.132770 [144000/345580]\n",
      "Train loss: 1.022609 [147200/345580]\n",
      "Train loss: 1.013010 [150400/345580]\n",
      "Train loss: 1.044142 [153600/345580]\n",
      "Train loss: 0.976030 [156800/345580]\n",
      "Train loss: 1.066034 [160000/345580]\n",
      "Train loss: 1.007290 [163200/345580]\n",
      "Train loss: 1.095348 [166400/345580]\n",
      "Train loss: 0.993463 [169600/345580]\n",
      "Train loss: 1.049019 [172800/345580]\n",
      "Train loss: 1.039770 [176000/345580]\n",
      "Train loss: 1.098660 [179200/345580]\n",
      "Train loss: 1.009620 [182400/345580]\n",
      "Train loss: 1.015609 [185600/345580]\n",
      "Train loss: 1.034078 [188800/345580]\n",
      "Train loss: 0.989155 [192000/345580]\n",
      "Train loss: 1.016429 [195200/345580]\n",
      "Train loss: 1.088851 [198400/345580]\n",
      "Train loss: 1.039486 [201600/345580]\n",
      "Train loss: 1.013813 [204800/345580]\n",
      "Train loss: 1.056507 [208000/345580]\n",
      "Train loss: 1.111200 [211200/345580]\n",
      "Train loss: 1.153818 [214400/345580]\n",
      "Train loss: 0.949550 [217600/345580]\n",
      "Train loss: 1.037981 [220800/345580]\n",
      "Train loss: 1.101021 [224000/345580]\n",
      "Train loss: 1.087954 [227200/345580]\n",
      "Train loss: 1.085608 [230400/345580]\n",
      "Train loss: 0.975145 [233600/345580]\n",
      "Train loss: 1.072731 [236800/345580]\n",
      "Train loss: 0.943727 [240000/345580]\n",
      "Train loss: 1.125578 [243200/345580]\n",
      "Train loss: 0.967413 [246400/345580]\n",
      "Train loss: 0.944129 [249600/345580]\n",
      "Train loss: 1.064213 [252800/345580]\n",
      "Train loss: 1.014889 [256000/345580]\n",
      "Train loss: 1.084516 [259200/345580]\n",
      "Train loss: 1.070945 [262400/345580]\n",
      "Train loss: 1.161503 [265600/345580]\n",
      "Train loss: 1.042543 [268800/345580]\n",
      "Train loss: 1.041741 [272000/345580]\n",
      "Train loss: 1.083713 [275200/345580]\n",
      "Train loss: 1.003721 [278400/345580]\n",
      "Train loss: 1.024935 [281600/345580]\n",
      "Train loss: 1.158934 [284800/345580]\n",
      "Train loss: 1.009872 [288000/345580]\n",
      "Train loss: 0.967779 [291200/345580]\n",
      "Train loss: 1.046031 [294400/345580]\n",
      "Train loss: 1.037354 [297600/345580]\n",
      "Train loss: 1.052971 [300800/345580]\n",
      "Train loss: 1.070153 [304000/345580]\n",
      "Train loss: 1.080328 [307200/345580]\n",
      "Train loss: 1.165731 [310400/345580]\n",
      "Train loss: 1.011710 [313600/345580]\n",
      "Train loss: 1.020674 [316800/345580]\n",
      "Train loss: 1.007031 [320000/345580]\n",
      "Train loss: 0.996358 [323200/345580]\n",
      "Train loss: 1.077902 [326400/345580]\n",
      "Train loss: 1.158648 [329600/345580]\n",
      "Train loss: 1.036193 [332800/345580]\n",
      "Train loss: 0.959853 [336000/345580]\n",
      "Train loss: 0.969851 [339200/345580]\n",
      "Train loss: 1.079814 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.043194 \n",
      "\n",
      "Epoch 19\n",
      " -------------------\n",
      "Train loss: 0.999021 [    0/345580]\n",
      "Train loss: 1.050786 [ 3200/345580]\n",
      "Train loss: 1.412471 [ 6400/345580]\n",
      "Train loss: 0.990808 [ 9600/345580]\n",
      "Train loss: 0.982913 [12800/345580]\n",
      "Train loss: 1.111198 [16000/345580]\n",
      "Train loss: 1.054030 [19200/345580]\n",
      "Train loss: 1.036450 [22400/345580]\n",
      "Train loss: 1.015456 [25600/345580]\n",
      "Train loss: 1.110525 [28800/345580]\n",
      "Train loss: 0.900193 [32000/345580]\n",
      "Train loss: 1.021274 [35200/345580]\n",
      "Train loss: 1.037679 [38400/345580]\n",
      "Train loss: 1.036990 [41600/345580]\n",
      "Train loss: 1.122980 [44800/345580]\n",
      "Train loss: 0.971327 [48000/345580]\n",
      "Train loss: 1.042978 [51200/345580]\n",
      "Train loss: 1.046921 [54400/345580]\n",
      "Train loss: 0.979648 [57600/345580]\n",
      "Train loss: 1.082139 [60800/345580]\n",
      "Train loss: 1.190806 [64000/345580]\n",
      "Train loss: 1.093492 [67200/345580]\n",
      "Train loss: 1.020771 [70400/345580]\n",
      "Train loss: 1.085378 [73600/345580]\n",
      "Train loss: 1.069920 [76800/345580]\n",
      "Train loss: 1.169525 [80000/345580]\n",
      "Train loss: 1.006431 [83200/345580]\n",
      "Train loss: 1.019473 [86400/345580]\n",
      "Train loss: 0.991526 [89600/345580]\n",
      "Train loss: 1.057287 [92800/345580]\n",
      "Train loss: 0.960825 [96000/345580]\n",
      "Train loss: 1.083555 [99200/345580]\n",
      "Train loss: 1.064538 [102400/345580]\n",
      "Train loss: 0.967638 [105600/345580]\n",
      "Train loss: 1.047533 [108800/345580]\n",
      "Train loss: 0.959190 [112000/345580]\n",
      "Train loss: 0.975374 [115200/345580]\n",
      "Train loss: 1.049026 [118400/345580]\n",
      "Train loss: 1.087540 [121600/345580]\n",
      "Train loss: 1.125383 [124800/345580]\n",
      "Train loss: 0.986796 [128000/345580]\n",
      "Train loss: 1.102446 [131200/345580]\n",
      "Train loss: 1.084748 [134400/345580]\n",
      "Train loss: 1.004774 [137600/345580]\n",
      "Train loss: 1.049689 [140800/345580]\n",
      "Train loss: 0.991163 [144000/345580]\n",
      "Train loss: 1.017944 [147200/345580]\n",
      "Train loss: 1.022164 [150400/345580]\n",
      "Train loss: 1.088007 [153600/345580]\n",
      "Train loss: 1.052388 [156800/345580]\n",
      "Train loss: 1.087718 [160000/345580]\n",
      "Train loss: 1.076568 [163200/345580]\n",
      "Train loss: 1.004940 [166400/345580]\n",
      "Train loss: 0.996509 [169600/345580]\n",
      "Train loss: 1.003291 [172800/345580]\n",
      "Train loss: 1.036131 [176000/345580]\n",
      "Train loss: 1.067032 [179200/345580]\n",
      "Train loss: 0.997680 [182400/345580]\n",
      "Train loss: 1.033587 [185600/345580]\n",
      "Train loss: 1.054394 [188800/345580]\n",
      "Train loss: 0.978591 [192000/345580]\n",
      "Train loss: 1.091067 [195200/345580]\n",
      "Train loss: 1.075227 [198400/345580]\n",
      "Train loss: 0.971709 [201600/345580]\n",
      "Train loss: 1.122159 [204800/345580]\n",
      "Train loss: 1.084115 [208000/345580]\n",
      "Train loss: 1.033460 [211200/345580]\n",
      "Train loss: 1.018504 [214400/345580]\n",
      "Train loss: 0.977436 [217600/345580]\n",
      "Train loss: 0.966198 [220800/345580]\n",
      "Train loss: 0.946932 [224000/345580]\n",
      "Train loss: 1.058941 [227200/345580]\n",
      "Train loss: 1.016806 [230400/345580]\n",
      "Train loss: 0.959386 [233600/345580]\n",
      "Train loss: 1.115489 [236800/345580]\n",
      "Train loss: 1.009773 [240000/345580]\n",
      "Train loss: 0.968010 [243200/345580]\n",
      "Train loss: 1.091355 [246400/345580]\n",
      "Train loss: 1.141256 [249600/345580]\n",
      "Train loss: 0.943834 [252800/345580]\n",
      "Train loss: 1.113392 [256000/345580]\n",
      "Train loss: 1.127310 [259200/345580]\n",
      "Train loss: 0.999275 [262400/345580]\n",
      "Train loss: 1.038176 [265600/345580]\n",
      "Train loss: 1.173464 [268800/345580]\n",
      "Train loss: 0.914119 [272000/345580]\n",
      "Train loss: 1.067504 [275200/345580]\n",
      "Train loss: 1.040301 [278400/345580]\n",
      "Train loss: 0.987022 [281600/345580]\n",
      "Train loss: 1.091485 [284800/345580]\n",
      "Train loss: 1.046378 [288000/345580]\n",
      "Train loss: 1.053901 [291200/345580]\n",
      "Train loss: 1.114700 [294400/345580]\n",
      "Train loss: 1.106065 [297600/345580]\n",
      "Train loss: 1.068593 [300800/345580]\n",
      "Train loss: 1.018995 [304000/345580]\n",
      "Train loss: 1.019581 [307200/345580]\n",
      "Train loss: 1.146183 [310400/345580]\n",
      "Train loss: 1.049877 [313600/345580]\n",
      "Train loss: 1.004991 [316800/345580]\n",
      "Train loss: 1.045818 [320000/345580]\n",
      "Train loss: 1.021396 [323200/345580]\n",
      "Train loss: 0.979063 [326400/345580]\n",
      "Train loss: 1.026499 [329600/345580]\n",
      "Train loss: 1.150240 [332800/345580]\n",
      "Train loss: 1.025270 [336000/345580]\n",
      "Train loss: 1.100385 [339200/345580]\n",
      "Train loss: 1.040014 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042750 \n",
      "\n",
      "Epoch 20\n",
      " -------------------\n",
      "Train loss: 0.968563 [    0/345580]\n",
      "Train loss: 1.045293 [ 3200/345580]\n",
      "Train loss: 1.075059 [ 6400/345580]\n",
      "Train loss: 1.042571 [ 9600/345580]\n",
      "Train loss: 1.063673 [12800/345580]\n",
      "Train loss: 1.122503 [16000/345580]\n",
      "Train loss: 1.023722 [19200/345580]\n",
      "Train loss: 1.019981 [22400/345580]\n",
      "Train loss: 1.032084 [25600/345580]\n",
      "Train loss: 1.059812 [28800/345580]\n",
      "Train loss: 1.004953 [32000/345580]\n",
      "Train loss: 0.950164 [35200/345580]\n",
      "Train loss: 0.989522 [38400/345580]\n",
      "Train loss: 1.078220 [41600/345580]\n",
      "Train loss: 0.970565 [44800/345580]\n",
      "Train loss: 1.121105 [48000/345580]\n",
      "Train loss: 1.097933 [51200/345580]\n",
      "Train loss: 1.062985 [54400/345580]\n",
      "Train loss: 0.985700 [57600/345580]\n",
      "Train loss: 1.012783 [60800/345580]\n",
      "Train loss: 1.072250 [64000/345580]\n",
      "Train loss: 0.958687 [67200/345580]\n",
      "Train loss: 1.138562 [70400/345580]\n",
      "Train loss: 0.890528 [73600/345580]\n",
      "Train loss: 1.042939 [76800/345580]\n",
      "Train loss: 1.023556 [80000/345580]\n",
      "Train loss: 0.996895 [83200/345580]\n",
      "Train loss: 1.064132 [86400/345580]\n",
      "Train loss: 1.013256 [89600/345580]\n",
      "Train loss: 1.219390 [92800/345580]\n",
      "Train loss: 1.105999 [96000/345580]\n",
      "Train loss: 0.974629 [99200/345580]\n",
      "Train loss: 0.941123 [102400/345580]\n",
      "Train loss: 1.081655 [105600/345580]\n",
      "Train loss: 1.056044 [108800/345580]\n",
      "Train loss: 1.106361 [112000/345580]\n",
      "Train loss: 1.008072 [115200/345580]\n",
      "Train loss: 1.044323 [118400/345580]\n",
      "Train loss: 1.121827 [121600/345580]\n",
      "Train loss: 1.150369 [124800/345580]\n",
      "Train loss: 1.048377 [128000/345580]\n",
      "Train loss: 1.021473 [131200/345580]\n",
      "Train loss: 1.003670 [134400/345580]\n",
      "Train loss: 1.108686 [137600/345580]\n",
      "Train loss: 1.027043 [140800/345580]\n",
      "Train loss: 1.043458 [144000/345580]\n",
      "Train loss: 1.054525 [147200/345580]\n",
      "Train loss: 1.104268 [150400/345580]\n",
      "Train loss: 1.104619 [153600/345580]\n",
      "Train loss: 0.978309 [156800/345580]\n",
      "Train loss: 0.995055 [160000/345580]\n",
      "Train loss: 1.033028 [163200/345580]\n",
      "Train loss: 1.032034 [166400/345580]\n",
      "Train loss: 1.192191 [169600/345580]\n",
      "Train loss: 1.085361 [172800/345580]\n",
      "Train loss: 1.016985 [176000/345580]\n",
      "Train loss: 1.068078 [179200/345580]\n",
      "Train loss: 0.915004 [182400/345580]\n",
      "Train loss: 1.019302 [185600/345580]\n",
      "Train loss: 1.072564 [188800/345580]\n",
      "Train loss: 1.020773 [192000/345580]\n",
      "Train loss: 0.994913 [195200/345580]\n",
      "Train loss: 1.008995 [198400/345580]\n",
      "Train loss: 1.014783 [201600/345580]\n",
      "Train loss: 0.937228 [204800/345580]\n",
      "Train loss: 0.989539 [208000/345580]\n",
      "Train loss: 1.117660 [211200/345580]\n",
      "Train loss: 0.970188 [214400/345580]\n",
      "Train loss: 1.006713 [217600/345580]\n",
      "Train loss: 0.947840 [220800/345580]\n",
      "Train loss: 1.040223 [224000/345580]\n",
      "Train loss: 0.983048 [227200/345580]\n",
      "Train loss: 0.980148 [230400/345580]\n",
      "Train loss: 1.024548 [233600/345580]\n",
      "Train loss: 1.061361 [236800/345580]\n",
      "Train loss: 0.942060 [240000/345580]\n",
      "Train loss: 1.077163 [243200/345580]\n",
      "Train loss: 0.986638 [246400/345580]\n",
      "Train loss: 1.008458 [249600/345580]\n",
      "Train loss: 1.035379 [252800/345580]\n",
      "Train loss: 1.117001 [256000/345580]\n",
      "Train loss: 0.980460 [259200/345580]\n",
      "Train loss: 1.115659 [262400/345580]\n",
      "Train loss: 1.003758 [265600/345580]\n",
      "Train loss: 1.060021 [268800/345580]\n",
      "Train loss: 1.042943 [272000/345580]\n",
      "Train loss: 0.982710 [275200/345580]\n",
      "Train loss: 0.995765 [278400/345580]\n",
      "Train loss: 1.027247 [281600/345580]\n",
      "Train loss: 1.136634 [284800/345580]\n",
      "Train loss: 1.046751 [288000/345580]\n",
      "Train loss: 0.900050 [291200/345580]\n",
      "Train loss: 1.099101 [294400/345580]\n",
      "Train loss: 0.974755 [297600/345580]\n",
      "Train loss: 1.015876 [300800/345580]\n",
      "Train loss: 1.027527 [304000/345580]\n",
      "Train loss: 1.020783 [307200/345580]\n",
      "Train loss: 1.072094 [310400/345580]\n",
      "Train loss: 1.004826 [313600/345580]\n",
      "Train loss: 1.008996 [316800/345580]\n",
      "Train loss: 1.062238 [320000/345580]\n",
      "Train loss: 1.065060 [323200/345580]\n",
      "Train loss: 1.027640 [326400/345580]\n",
      "Train loss: 0.992470 [329600/345580]\n",
      "Train loss: 0.986643 [332800/345580]\n",
      "Train loss: 0.978605 [336000/345580]\n",
      "Train loss: 1.051583 [339200/345580]\n",
      "Train loss: 1.063412 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042640 \n",
      "\n",
      "Epoch 21\n",
      " -------------------\n",
      "Train loss: 1.016484 [    0/345580]\n",
      "Train loss: 1.007179 [ 3200/345580]\n",
      "Train loss: 0.995714 [ 6400/345580]\n",
      "Train loss: 1.042874 [ 9600/345580]\n",
      "Train loss: 0.961171 [12800/345580]\n",
      "Train loss: 1.029760 [16000/345580]\n",
      "Train loss: 1.041857 [19200/345580]\n",
      "Train loss: 1.005583 [22400/345580]\n",
      "Train loss: 1.071828 [25600/345580]\n",
      "Train loss: 0.982050 [28800/345580]\n",
      "Train loss: 0.976082 [32000/345580]\n",
      "Train loss: 1.129665 [35200/345580]\n",
      "Train loss: 1.003742 [38400/345580]\n",
      "Train loss: 1.035877 [41600/345580]\n",
      "Train loss: 1.100851 [44800/345580]\n",
      "Train loss: 1.136612 [48000/345580]\n",
      "Train loss: 1.054874 [51200/345580]\n",
      "Train loss: 1.113465 [54400/345580]\n",
      "Train loss: 1.065906 [57600/345580]\n",
      "Train loss: 1.126117 [60800/345580]\n",
      "Train loss: 0.970946 [64000/345580]\n",
      "Train loss: 1.000130 [67200/345580]\n",
      "Train loss: 0.906480 [70400/345580]\n",
      "Train loss: 0.993772 [73600/345580]\n",
      "Train loss: 1.048664 [76800/345580]\n",
      "Train loss: 0.944930 [80000/345580]\n",
      "Train loss: 1.015666 [83200/345580]\n",
      "Train loss: 1.067942 [86400/345580]\n",
      "Train loss: 0.970259 [89600/345580]\n",
      "Train loss: 1.110088 [92800/345580]\n",
      "Train loss: 1.093475 [96000/345580]\n",
      "Train loss: 1.122847 [99200/345580]\n",
      "Train loss: 1.033399 [102400/345580]\n",
      "Train loss: 1.144175 [105600/345580]\n",
      "Train loss: 1.049901 [108800/345580]\n",
      "Train loss: 1.088017 [112000/345580]\n",
      "Train loss: 1.052459 [115200/345580]\n",
      "Train loss: 1.039596 [118400/345580]\n",
      "Train loss: 1.028541 [121600/345580]\n",
      "Train loss: 1.062777 [124800/345580]\n",
      "Train loss: 0.977168 [128000/345580]\n",
      "Train loss: 1.095543 [131200/345580]\n",
      "Train loss: 0.953514 [134400/345580]\n",
      "Train loss: 1.026523 [137600/345580]\n",
      "Train loss: 0.993668 [140800/345580]\n",
      "Train loss: 1.008158 [144000/345580]\n",
      "Train loss: 1.045237 [147200/345580]\n",
      "Train loss: 0.936778 [150400/345580]\n",
      "Train loss: 1.024077 [153600/345580]\n",
      "Train loss: 1.114873 [156800/345580]\n",
      "Train loss: 1.089913 [160000/345580]\n",
      "Train loss: 1.138221 [163200/345580]\n",
      "Train loss: 1.021600 [166400/345580]\n",
      "Train loss: 1.069960 [169600/345580]\n",
      "Train loss: 1.078242 [172800/345580]\n",
      "Train loss: 1.109316 [176000/345580]\n",
      "Train loss: 1.177843 [179200/345580]\n",
      "Train loss: 1.026979 [182400/345580]\n",
      "Train loss: 1.030199 [185600/345580]\n",
      "Train loss: 1.052366 [188800/345580]\n",
      "Train loss: 1.032005 [192000/345580]\n",
      "Train loss: 1.102844 [195200/345580]\n",
      "Train loss: 1.015434 [198400/345580]\n",
      "Train loss: 1.050348 [201600/345580]\n",
      "Train loss: 0.984200 [204800/345580]\n",
      "Train loss: 1.045111 [208000/345580]\n",
      "Train loss: 1.051560 [211200/345580]\n",
      "Train loss: 1.114855 [214400/345580]\n",
      "Train loss: 0.999484 [217600/345580]\n",
      "Train loss: 0.987656 [220800/345580]\n",
      "Train loss: 1.049902 [224000/345580]\n",
      "Train loss: 0.976539 [227200/345580]\n",
      "Train loss: 1.061648 [230400/345580]\n",
      "Train loss: 1.062200 [233600/345580]\n",
      "Train loss: 1.072435 [236800/345580]\n",
      "Train loss: 1.052305 [240000/345580]\n",
      "Train loss: 1.023160 [243200/345580]\n",
      "Train loss: 1.091128 [246400/345580]\n",
      "Train loss: 1.106602 [249600/345580]\n",
      "Train loss: 1.091496 [252800/345580]\n",
      "Train loss: 1.007591 [256000/345580]\n",
      "Train loss: 1.110315 [259200/345580]\n",
      "Train loss: 1.015414 [262400/345580]\n",
      "Train loss: 1.113702 [265600/345580]\n",
      "Train loss: 0.998960 [268800/345580]\n",
      "Train loss: 1.015570 [272000/345580]\n",
      "Train loss: 1.045627 [275200/345580]\n",
      "Train loss: 1.049412 [278400/345580]\n",
      "Train loss: 1.116830 [281600/345580]\n",
      "Train loss: 1.095255 [284800/345580]\n",
      "Train loss: 1.047179 [288000/345580]\n",
      "Train loss: 1.071287 [291200/345580]\n",
      "Train loss: 1.050031 [294400/345580]\n",
      "Train loss: 1.011151 [297600/345580]\n",
      "Train loss: 0.969587 [300800/345580]\n",
      "Train loss: 1.066452 [304000/345580]\n",
      "Train loss: 1.048457 [307200/345580]\n",
      "Train loss: 1.137211 [310400/345580]\n",
      "Train loss: 1.080079 [313600/345580]\n",
      "Train loss: 1.125675 [316800/345580]\n",
      "Train loss: 1.249310 [320000/345580]\n",
      "Train loss: 1.023634 [323200/345580]\n",
      "Train loss: 1.010495 [326400/345580]\n",
      "Train loss: 1.126705 [329600/345580]\n",
      "Train loss: 1.088885 [332800/345580]\n",
      "Train loss: 1.129939 [336000/345580]\n",
      "Train loss: 1.082882 [339200/345580]\n",
      "Train loss: 1.019187 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042051 \n",
      "\n",
      "Epoch 22\n",
      " -------------------\n",
      "Train loss: 0.938501 [    0/345580]\n",
      "Train loss: 0.980103 [ 3200/345580]\n",
      "Train loss: 0.993025 [ 6400/345580]\n",
      "Train loss: 1.056887 [ 9600/345580]\n",
      "Train loss: 1.061037 [12800/345580]\n",
      "Train loss: 1.084117 [16000/345580]\n",
      "Train loss: 0.996469 [19200/345580]\n",
      "Train loss: 0.993514 [22400/345580]\n",
      "Train loss: 1.048166 [25600/345580]\n",
      "Train loss: 0.977963 [28800/345580]\n",
      "Train loss: 0.973288 [32000/345580]\n",
      "Train loss: 1.064722 [35200/345580]\n",
      "Train loss: 1.063567 [38400/345580]\n",
      "Train loss: 0.911630 [41600/345580]\n",
      "Train loss: 1.091532 [44800/345580]\n",
      "Train loss: 1.105170 [48000/345580]\n",
      "Train loss: 1.023720 [51200/345580]\n",
      "Train loss: 1.029277 [54400/345580]\n",
      "Train loss: 1.072947 [57600/345580]\n",
      "Train loss: 1.071810 [60800/345580]\n",
      "Train loss: 1.107360 [64000/345580]\n",
      "Train loss: 1.128512 [67200/345580]\n",
      "Train loss: 1.024570 [70400/345580]\n",
      "Train loss: 1.016275 [73600/345580]\n",
      "Train loss: 1.004893 [76800/345580]\n",
      "Train loss: 1.089073 [80000/345580]\n",
      "Train loss: 1.008658 [83200/345580]\n",
      "Train loss: 1.105924 [86400/345580]\n",
      "Train loss: 0.971462 [89600/345580]\n",
      "Train loss: 1.034328 [92800/345580]\n",
      "Train loss: 0.945311 [96000/345580]\n",
      "Train loss: 1.033213 [99200/345580]\n",
      "Train loss: 1.107806 [102400/345580]\n",
      "Train loss: 1.089616 [105600/345580]\n",
      "Train loss: 1.089051 [108800/345580]\n",
      "Train loss: 1.042975 [112000/345580]\n",
      "Train loss: 1.025451 [115200/345580]\n",
      "Train loss: 1.042155 [118400/345580]\n",
      "Train loss: 1.062235 [121600/345580]\n",
      "Train loss: 1.084849 [124800/345580]\n",
      "Train loss: 1.087135 [128000/345580]\n",
      "Train loss: 1.076184 [131200/345580]\n",
      "Train loss: 1.000367 [134400/345580]\n",
      "Train loss: 1.085315 [137600/345580]\n",
      "Train loss: 0.945575 [140800/345580]\n",
      "Train loss: 1.038167 [144000/345580]\n",
      "Train loss: 1.048233 [147200/345580]\n",
      "Train loss: 1.046414 [150400/345580]\n",
      "Train loss: 1.223865 [153600/345580]\n",
      "Train loss: 0.967719 [156800/345580]\n",
      "Train loss: 1.174232 [160000/345580]\n",
      "Train loss: 0.996844 [163200/345580]\n",
      "Train loss: 1.068006 [166400/345580]\n",
      "Train loss: 0.937324 [169600/345580]\n",
      "Train loss: 0.964666 [172800/345580]\n",
      "Train loss: 1.008606 [176000/345580]\n",
      "Train loss: 1.065773 [179200/345580]\n",
      "Train loss: 1.042033 [182400/345580]\n",
      "Train loss: 0.995143 [185600/345580]\n",
      "Train loss: 0.993567 [188800/345580]\n",
      "Train loss: 1.038910 [192000/345580]\n",
      "Train loss: 1.023333 [195200/345580]\n",
      "Train loss: 0.998249 [198400/345580]\n",
      "Train loss: 1.049109 [201600/345580]\n",
      "Train loss: 1.119714 [204800/345580]\n",
      "Train loss: 1.051258 [208000/345580]\n",
      "Train loss: 1.092881 [211200/345580]\n",
      "Train loss: 1.179274 [214400/345580]\n",
      "Train loss: 1.041608 [217600/345580]\n",
      "Train loss: 1.039873 [220800/345580]\n",
      "Train loss: 0.925995 [224000/345580]\n",
      "Train loss: 0.949176 [227200/345580]\n",
      "Train loss: 1.087754 [230400/345580]\n",
      "Train loss: 0.990872 [233600/345580]\n",
      "Train loss: 1.003682 [236800/345580]\n",
      "Train loss: 1.015586 [240000/345580]\n",
      "Train loss: 0.953014 [243200/345580]\n",
      "Train loss: 1.029665 [246400/345580]\n",
      "Train loss: 0.936908 [249600/345580]\n",
      "Train loss: 1.151514 [252800/345580]\n",
      "Train loss: 1.025151 [256000/345580]\n",
      "Train loss: 1.018261 [259200/345580]\n",
      "Train loss: 1.024869 [262400/345580]\n",
      "Train loss: 1.045453 [265600/345580]\n",
      "Train loss: 0.991188 [268800/345580]\n",
      "Train loss: 1.000057 [272000/345580]\n",
      "Train loss: 1.105706 [275200/345580]\n",
      "Train loss: 1.126585 [278400/345580]\n",
      "Train loss: 1.031438 [281600/345580]\n",
      "Train loss: 1.028376 [284800/345580]\n",
      "Train loss: 1.024410 [288000/345580]\n",
      "Train loss: 1.037295 [291200/345580]\n",
      "Train loss: 1.037058 [294400/345580]\n",
      "Train loss: 1.030690 [297600/345580]\n",
      "Train loss: 1.012038 [300800/345580]\n",
      "Train loss: 1.028530 [304000/345580]\n",
      "Train loss: 1.124650 [307200/345580]\n",
      "Train loss: 1.073203 [310400/345580]\n",
      "Train loss: 1.029427 [313600/345580]\n",
      "Train loss: 1.069239 [316800/345580]\n",
      "Train loss: 1.051355 [320000/345580]\n",
      "Train loss: 1.078425 [323200/345580]\n",
      "Train loss: 1.037534 [326400/345580]\n",
      "Train loss: 1.049481 [329600/345580]\n",
      "Train loss: 1.140821 [332800/345580]\n",
      "Train loss: 1.115462 [336000/345580]\n",
      "Train loss: 1.018896 [339200/345580]\n",
      "Train loss: 1.006766 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.041924 \n",
      "\n",
      "Epoch 23\n",
      " -------------------\n",
      "Train loss: 1.101301 [    0/345580]\n",
      "Train loss: 1.023934 [ 3200/345580]\n",
      "Train loss: 1.044451 [ 6400/345580]\n",
      "Train loss: 0.968598 [ 9600/345580]\n",
      "Train loss: 1.069137 [12800/345580]\n",
      "Train loss: 1.097516 [16000/345580]\n",
      "Train loss: 1.233289 [19200/345580]\n",
      "Train loss: 1.066622 [22400/345580]\n",
      "Train loss: 0.975481 [25600/345580]\n",
      "Train loss: 1.043651 [28800/345580]\n",
      "Train loss: 1.093750 [32000/345580]\n",
      "Train loss: 1.093865 [35200/345580]\n",
      "Train loss: 1.185895 [38400/345580]\n",
      "Train loss: 1.051290 [41600/345580]\n",
      "Train loss: 1.023232 [44800/345580]\n",
      "Train loss: 1.060596 [48000/345580]\n",
      "Train loss: 1.077826 [51200/345580]\n",
      "Train loss: 1.075224 [54400/345580]\n",
      "Train loss: 1.099181 [57600/345580]\n",
      "Train loss: 1.019449 [60800/345580]\n",
      "Train loss: 0.959327 [64000/345580]\n",
      "Train loss: 0.964772 [67200/345580]\n",
      "Train loss: 1.018620 [70400/345580]\n",
      "Train loss: 1.011214 [73600/345580]\n",
      "Train loss: 1.076409 [76800/345580]\n",
      "Train loss: 1.050197 [80000/345580]\n",
      "Train loss: 0.934965 [83200/345580]\n",
      "Train loss: 1.070138 [86400/345580]\n",
      "Train loss: 1.022340 [89600/345580]\n",
      "Train loss: 1.096986 [92800/345580]\n",
      "Train loss: 1.016076 [96000/345580]\n",
      "Train loss: 1.073755 [99200/345580]\n",
      "Train loss: 1.085162 [102400/345580]\n",
      "Train loss: 1.062571 [105600/345580]\n",
      "Train loss: 0.980070 [108800/345580]\n",
      "Train loss: 1.095987 [112000/345580]\n",
      "Train loss: 1.009213 [115200/345580]\n",
      "Train loss: 0.988046 [118400/345580]\n",
      "Train loss: 1.018896 [121600/345580]\n",
      "Train loss: 0.939753 [124800/345580]\n",
      "Train loss: 0.971100 [128000/345580]\n",
      "Train loss: 1.088709 [131200/345580]\n",
      "Train loss: 1.055918 [134400/345580]\n",
      "Train loss: 1.004660 [137600/345580]\n",
      "Train loss: 1.020509 [140800/345580]\n",
      "Train loss: 1.045893 [144000/345580]\n",
      "Train loss: 1.022885 [147200/345580]\n",
      "Train loss: 1.036990 [150400/345580]\n",
      "Train loss: 1.030710 [153600/345580]\n",
      "Train loss: 0.974152 [156800/345580]\n",
      "Train loss: 1.055850 [160000/345580]\n",
      "Train loss: 1.003796 [163200/345580]\n",
      "Train loss: 1.042870 [166400/345580]\n",
      "Train loss: 1.028415 [169600/345580]\n",
      "Train loss: 0.992495 [172800/345580]\n",
      "Train loss: 1.144372 [176000/345580]\n",
      "Train loss: 1.061658 [179200/345580]\n",
      "Train loss: 1.081530 [182400/345580]\n",
      "Train loss: 1.113461 [185600/345580]\n",
      "Train loss: 0.931239 [188800/345580]\n",
      "Train loss: 1.024480 [192000/345580]\n",
      "Train loss: 1.082562 [195200/345580]\n",
      "Train loss: 0.998729 [198400/345580]\n",
      "Train loss: 1.085523 [201600/345580]\n",
      "Train loss: 1.072653 [204800/345580]\n",
      "Train loss: 0.948564 [208000/345580]\n",
      "Train loss: 1.090588 [211200/345580]\n",
      "Train loss: 1.009468 [214400/345580]\n",
      "Train loss: 1.084953 [217600/345580]\n",
      "Train loss: 0.957171 [220800/345580]\n",
      "Train loss: 1.045330 [224000/345580]\n",
      "Train loss: 0.960442 [227200/345580]\n",
      "Train loss: 1.019636 [230400/345580]\n",
      "Train loss: 0.962753 [233600/345580]\n",
      "Train loss: 0.998105 [236800/345580]\n",
      "Train loss: 1.045314 [240000/345580]\n",
      "Train loss: 1.077076 [243200/345580]\n",
      "Train loss: 1.015105 [246400/345580]\n",
      "Train loss: 1.048755 [249600/345580]\n",
      "Train loss: 0.948346 [252800/345580]\n",
      "Train loss: 1.005709 [256000/345580]\n",
      "Train loss: 1.057569 [259200/345580]\n",
      "Train loss: 1.002355 [262400/345580]\n",
      "Train loss: 0.983409 [265600/345580]\n",
      "Train loss: 0.975126 [268800/345580]\n",
      "Train loss: 0.970618 [272000/345580]\n",
      "Train loss: 1.047910 [275200/345580]\n",
      "Train loss: 1.011791 [278400/345580]\n",
      "Train loss: 1.041528 [281600/345580]\n",
      "Train loss: 0.985287 [284800/345580]\n",
      "Train loss: 1.117316 [288000/345580]\n",
      "Train loss: 1.076400 [291200/345580]\n",
      "Train loss: 0.939599 [294400/345580]\n",
      "Train loss: 0.987075 [297600/345580]\n",
      "Train loss: 0.994791 [300800/345580]\n",
      "Train loss: 1.020075 [304000/345580]\n",
      "Train loss: 0.957293 [307200/345580]\n",
      "Train loss: 1.003726 [310400/345580]\n",
      "Train loss: 0.926985 [313600/345580]\n",
      "Train loss: 1.015894 [316800/345580]\n",
      "Train loss: 0.983817 [320000/345580]\n",
      "Train loss: 1.047054 [323200/345580]\n",
      "Train loss: 1.105245 [326400/345580]\n",
      "Train loss: 0.903236 [329600/345580]\n",
      "Train loss: 1.021537 [332800/345580]\n",
      "Train loss: 1.188577 [336000/345580]\n",
      "Train loss: 0.961890 [339200/345580]\n",
      "Train loss: 1.050681 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.042509 \n",
      "\n",
      "Epoch 24\n",
      " -------------------\n",
      "Train loss: 1.054908 [    0/345580]\n",
      "Train loss: 1.063187 [ 3200/345580]\n",
      "Train loss: 1.070671 [ 6400/345580]\n",
      "Train loss: 0.987179 [ 9600/345580]\n",
      "Train loss: 0.976382 [12800/345580]\n",
      "Train loss: 1.050774 [16000/345580]\n",
      "Train loss: 1.024163 [19200/345580]\n",
      "Train loss: 1.026898 [22400/345580]\n",
      "Train loss: 0.991382 [25600/345580]\n",
      "Train loss: 0.948026 [28800/345580]\n",
      "Train loss: 0.945511 [32000/345580]\n",
      "Train loss: 1.073638 [35200/345580]\n",
      "Train loss: 1.005821 [38400/345580]\n",
      "Train loss: 1.118753 [41600/345580]\n",
      "Train loss: 1.069081 [44800/345580]\n",
      "Train loss: 1.031606 [48000/345580]\n",
      "Train loss: 0.948201 [51200/345580]\n",
      "Train loss: 1.035945 [54400/345580]\n",
      "Train loss: 1.139515 [57600/345580]\n",
      "Train loss: 1.051124 [60800/345580]\n",
      "Train loss: 1.082490 [64000/345580]\n",
      "Train loss: 0.981554 [67200/345580]\n",
      "Train loss: 1.100266 [70400/345580]\n",
      "Train loss: 1.078871 [73600/345580]\n",
      "Train loss: 0.940367 [76800/345580]\n",
      "Train loss: 1.083647 [80000/345580]\n",
      "Train loss: 1.038718 [83200/345580]\n",
      "Train loss: 1.118800 [86400/345580]\n",
      "Train loss: 1.021721 [89600/345580]\n",
      "Train loss: 1.111704 [92800/345580]\n",
      "Train loss: 1.068302 [96000/345580]\n",
      "Train loss: 1.043506 [99200/345580]\n",
      "Train loss: 0.959989 [102400/345580]\n",
      "Train loss: 1.064358 [105600/345580]\n",
      "Train loss: 1.092481 [108800/345580]\n",
      "Train loss: 1.029954 [112000/345580]\n",
      "Train loss: 1.146073 [115200/345580]\n",
      "Train loss: 1.024951 [118400/345580]\n",
      "Train loss: 1.060140 [121600/345580]\n",
      "Train loss: 1.014851 [124800/345580]\n",
      "Train loss: 1.070250 [128000/345580]\n",
      "Train loss: 0.963802 [131200/345580]\n",
      "Train loss: 1.008628 [134400/345580]\n",
      "Train loss: 1.017840 [137600/345580]\n",
      "Train loss: 1.016892 [140800/345580]\n",
      "Train loss: 1.046937 [144000/345580]\n",
      "Train loss: 1.076880 [147200/345580]\n",
      "Train loss: 0.984097 [150400/345580]\n",
      "Train loss: 1.090197 [153600/345580]\n",
      "Train loss: 1.060501 [156800/345580]\n",
      "Train loss: 1.034360 [160000/345580]\n",
      "Train loss: 0.998816 [163200/345580]\n",
      "Train loss: 1.033536 [166400/345580]\n",
      "Train loss: 1.049905 [169600/345580]\n",
      "Train loss: 1.046274 [172800/345580]\n",
      "Train loss: 0.994491 [176000/345580]\n",
      "Train loss: 1.029398 [179200/345580]\n",
      "Train loss: 1.088901 [182400/345580]\n",
      "Train loss: 1.007209 [185600/345580]\n",
      "Train loss: 1.093211 [188800/345580]\n",
      "Train loss: 1.064568 [192000/345580]\n",
      "Train loss: 0.943641 [195200/345580]\n",
      "Train loss: 1.089390 [198400/345580]\n",
      "Train loss: 1.052616 [201600/345580]\n",
      "Train loss: 1.137282 [204800/345580]\n",
      "Train loss: 1.016566 [208000/345580]\n",
      "Train loss: 1.000132 [211200/345580]\n",
      "Train loss: 1.048559 [214400/345580]\n",
      "Train loss: 1.030425 [217600/345580]\n",
      "Train loss: 0.952240 [220800/345580]\n",
      "Train loss: 1.099067 [224000/345580]\n",
      "Train loss: 1.024855 [227200/345580]\n",
      "Train loss: 1.005843 [230400/345580]\n",
      "Train loss: 0.995973 [233600/345580]\n",
      "Train loss: 1.020369 [236800/345580]\n",
      "Train loss: 1.057917 [240000/345580]\n",
      "Train loss: 1.096130 [243200/345580]\n",
      "Train loss: 0.976117 [246400/345580]\n",
      "Train loss: 1.028690 [249600/345580]\n",
      "Train loss: 1.069671 [252800/345580]\n",
      "Train loss: 1.185453 [256000/345580]\n",
      "Train loss: 1.074494 [259200/345580]\n",
      "Train loss: 0.961624 [262400/345580]\n",
      "Train loss: 1.075423 [265600/345580]\n",
      "Train loss: 1.025165 [268800/345580]\n",
      "Train loss: 1.146379 [272000/345580]\n",
      "Train loss: 1.141370 [275200/345580]\n",
      "Train loss: 1.064269 [278400/345580]\n",
      "Train loss: 1.129143 [281600/345580]\n",
      "Train loss: 1.016162 [284800/345580]\n",
      "Train loss: 1.065732 [288000/345580]\n",
      "Train loss: 1.010763 [291200/345580]\n",
      "Train loss: 1.034423 [294400/345580]\n",
      "Train loss: 0.971867 [297600/345580]\n",
      "Train loss: 0.970387 [300800/345580]\n",
      "Train loss: 0.924497 [304000/345580]\n",
      "Train loss: 0.997882 [307200/345580]\n",
      "Train loss: 0.969899 [310400/345580]\n",
      "Train loss: 0.926586 [313600/345580]\n",
      "Train loss: 1.060778 [316800/345580]\n",
      "Train loss: 0.948981 [320000/345580]\n",
      "Train loss: 0.972315 [323200/345580]\n",
      "Train loss: 1.084611 [326400/345580]\n",
      "Train loss: 1.051382 [329600/345580]\n",
      "Train loss: 1.055462 [332800/345580]\n",
      "Train loss: 1.065520 [336000/345580]\n",
      "Train loss: 1.074267 [339200/345580]\n",
      "Train loss: 1.020880 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 42.9%, Avg loss: 1.042428 \n",
      "\n",
      "Epoch 25\n",
      " -------------------\n",
      "Train loss: 1.044533 [    0/345580]\n",
      "Train loss: 1.024117 [ 3200/345580]\n",
      "Train loss: 1.051642 [ 6400/345580]\n",
      "Train loss: 1.037436 [ 9600/345580]\n",
      "Train loss: 0.946943 [12800/345580]\n",
      "Train loss: 1.105588 [16000/345580]\n",
      "Train loss: 1.039215 [19200/345580]\n",
      "Train loss: 0.979632 [22400/345580]\n",
      "Train loss: 1.067844 [25600/345580]\n",
      "Train loss: 1.011306 [28800/345580]\n",
      "Train loss: 1.060645 [32000/345580]\n",
      "Train loss: 1.164645 [35200/345580]\n",
      "Train loss: 1.025437 [38400/345580]\n",
      "Train loss: 0.996341 [41600/345580]\n",
      "Train loss: 0.979255 [44800/345580]\n",
      "Train loss: 1.077548 [48000/345580]\n",
      "Train loss: 1.070135 [51200/345580]\n",
      "Train loss: 1.000946 [54400/345580]\n",
      "Train loss: 1.004695 [57600/345580]\n",
      "Train loss: 0.982048 [60800/345580]\n",
      "Train loss: 1.062015 [64000/345580]\n",
      "Train loss: 1.034754 [67200/345580]\n",
      "Train loss: 1.007796 [70400/345580]\n",
      "Train loss: 1.092366 [73600/345580]\n",
      "Train loss: 1.038646 [76800/345580]\n",
      "Train loss: 1.170913 [80000/345580]\n",
      "Train loss: 1.025361 [83200/345580]\n",
      "Train loss: 1.088019 [86400/345580]\n",
      "Train loss: 1.032527 [89600/345580]\n",
      "Train loss: 1.036994 [92800/345580]\n",
      "Train loss: 1.036659 [96000/345580]\n",
      "Train loss: 1.094422 [99200/345580]\n",
      "Train loss: 0.965214 [102400/345580]\n",
      "Train loss: 1.031952 [105600/345580]\n",
      "Train loss: 1.038306 [108800/345580]\n",
      "Train loss: 1.083331 [112000/345580]\n",
      "Train loss: 1.116693 [115200/345580]\n",
      "Train loss: 1.136057 [118400/345580]\n",
      "Train loss: 1.118557 [121600/345580]\n",
      "Train loss: 1.017410 [124800/345580]\n",
      "Train loss: 1.145863 [128000/345580]\n",
      "Train loss: 1.023166 [131200/345580]\n",
      "Train loss: 1.154264 [134400/345580]\n",
      "Train loss: 0.997600 [137600/345580]\n",
      "Train loss: 1.071284 [140800/345580]\n",
      "Train loss: 0.987850 [144000/345580]\n",
      "Train loss: 1.005765 [147200/345580]\n",
      "Train loss: 1.001948 [150400/345580]\n",
      "Train loss: 1.053663 [153600/345580]\n",
      "Train loss: 0.969322 [156800/345580]\n",
      "Train loss: 1.052845 [160000/345580]\n",
      "Train loss: 0.955079 [163200/345580]\n",
      "Train loss: 1.109859 [166400/345580]\n",
      "Train loss: 1.070371 [169600/345580]\n",
      "Train loss: 1.051866 [172800/345580]\n",
      "Train loss: 1.114151 [176000/345580]\n",
      "Train loss: 1.006588 [179200/345580]\n",
      "Train loss: 1.084814 [182400/345580]\n",
      "Train loss: 1.061681 [185600/345580]\n",
      "Train loss: 1.070133 [188800/345580]\n",
      "Train loss: 1.076061 [192000/345580]\n",
      "Train loss: 0.962291 [195200/345580]\n",
      "Train loss: 1.158204 [198400/345580]\n",
      "Train loss: 1.139847 [201600/345580]\n",
      "Train loss: 1.017985 [204800/345580]\n",
      "Train loss: 0.979755 [208000/345580]\n",
      "Train loss: 1.129850 [211200/345580]\n",
      "Train loss: 1.110204 [214400/345580]\n",
      "Train loss: 1.000984 [217600/345580]\n",
      "Train loss: 1.024834 [220800/345580]\n",
      "Train loss: 1.019080 [224000/345580]\n",
      "Train loss: 1.062355 [227200/345580]\n",
      "Train loss: 1.136328 [230400/345580]\n",
      "Train loss: 1.063445 [233600/345580]\n",
      "Train loss: 1.061873 [236800/345580]\n",
      "Train loss: 1.043396 [240000/345580]\n",
      "Train loss: 1.091555 [243200/345580]\n",
      "Train loss: 0.915934 [246400/345580]\n",
      "Train loss: 0.960688 [249600/345580]\n",
      "Train loss: 0.978972 [252800/345580]\n",
      "Train loss: 0.985623 [256000/345580]\n",
      "Train loss: 0.970231 [259200/345580]\n",
      "Train loss: 1.000497 [262400/345580]\n",
      "Train loss: 1.077380 [265600/345580]\n",
      "Train loss: 0.972206 [268800/345580]\n",
      "Train loss: 1.011594 [272000/345580]\n",
      "Train loss: 1.052210 [275200/345580]\n",
      "Train loss: 0.985967 [278400/345580]\n",
      "Train loss: 0.995632 [281600/345580]\n",
      "Train loss: 1.074461 [284800/345580]\n",
      "Train loss: 1.012863 [288000/345580]\n",
      "Train loss: 1.028362 [291200/345580]\n",
      "Train loss: 1.009802 [294400/345580]\n",
      "Train loss: 1.008470 [297600/345580]\n",
      "Train loss: 1.003428 [300800/345580]\n",
      "Train loss: 0.998701 [304000/345580]\n",
      "Train loss: 1.093873 [307200/345580]\n",
      "Train loss: 0.982942 [310400/345580]\n",
      "Train loss: 1.066597 [313600/345580]\n",
      "Train loss: 1.072841 [316800/345580]\n",
      "Train loss: 1.005485 [320000/345580]\n",
      "Train loss: 0.976678 [323200/345580]\n",
      "Train loss: 1.053906 [326400/345580]\n",
      "Train loss: 1.089425 [329600/345580]\n",
      "Train loss: 1.084140 [332800/345580]\n",
      "Train loss: 0.942624 [336000/345580]\n",
      "Train loss: 1.008923 [339200/345580]\n",
      "Train loss: 1.093445 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042752 \n",
      "\n",
      "Epoch 26\n",
      " -------------------\n",
      "Train loss: 1.092559 [    0/345580]\n",
      "Train loss: 1.011948 [ 3200/345580]\n",
      "Train loss: 1.066843 [ 6400/345580]\n",
      "Train loss: 1.103924 [ 9600/345580]\n",
      "Train loss: 1.022489 [12800/345580]\n",
      "Train loss: 1.078497 [16000/345580]\n",
      "Train loss: 1.090526 [19200/345580]\n",
      "Train loss: 1.073762 [22400/345580]\n",
      "Train loss: 0.894412 [25600/345580]\n",
      "Train loss: 0.993983 [28800/345580]\n",
      "Train loss: 1.024631 [32000/345580]\n",
      "Train loss: 1.115226 [35200/345580]\n",
      "Train loss: 1.005849 [38400/345580]\n",
      "Train loss: 1.100973 [41600/345580]\n",
      "Train loss: 1.037305 [44800/345580]\n",
      "Train loss: 1.022710 [48000/345580]\n",
      "Train loss: 0.993390 [51200/345580]\n",
      "Train loss: 1.004023 [54400/345580]\n",
      "Train loss: 1.104456 [57600/345580]\n",
      "Train loss: 0.951839 [60800/345580]\n",
      "Train loss: 1.069036 [64000/345580]\n",
      "Train loss: 1.056999 [67200/345580]\n",
      "Train loss: 1.097087 [70400/345580]\n",
      "Train loss: 1.021083 [73600/345580]\n",
      "Train loss: 1.007856 [76800/345580]\n",
      "Train loss: 1.015496 [80000/345580]\n",
      "Train loss: 1.031729 [83200/345580]\n",
      "Train loss: 1.050903 [86400/345580]\n",
      "Train loss: 1.000135 [89600/345580]\n",
      "Train loss: 1.074899 [92800/345580]\n",
      "Train loss: 0.918978 [96000/345580]\n",
      "Train loss: 1.048271 [99200/345580]\n",
      "Train loss: 1.053915 [102400/345580]\n",
      "Train loss: 1.074065 [105600/345580]\n",
      "Train loss: 1.012745 [108800/345580]\n",
      "Train loss: 1.057708 [112000/345580]\n",
      "Train loss: 1.052830 [115200/345580]\n",
      "Train loss: 1.028817 [118400/345580]\n",
      "Train loss: 0.975868 [121600/345580]\n",
      "Train loss: 0.914129 [124800/345580]\n",
      "Train loss: 0.969402 [128000/345580]\n",
      "Train loss: 1.099025 [131200/345580]\n",
      "Train loss: 1.014973 [134400/345580]\n",
      "Train loss: 1.073760 [137600/345580]\n",
      "Train loss: 1.062901 [140800/345580]\n",
      "Train loss: 1.069908 [144000/345580]\n",
      "Train loss: 0.969663 [147200/345580]\n",
      "Train loss: 1.166772 [150400/345580]\n",
      "Train loss: 1.023476 [153600/345580]\n",
      "Train loss: 1.091380 [156800/345580]\n",
      "Train loss: 1.076063 [160000/345580]\n",
      "Train loss: 1.115536 [163200/345580]\n",
      "Train loss: 0.946955 [166400/345580]\n",
      "Train loss: 1.009205 [169600/345580]\n",
      "Train loss: 1.096579 [172800/345580]\n",
      "Train loss: 1.189021 [176000/345580]\n",
      "Train loss: 1.008351 [179200/345580]\n",
      "Train loss: 1.129376 [182400/345580]\n",
      "Train loss: 1.031215 [185600/345580]\n",
      "Train loss: 1.044312 [188800/345580]\n",
      "Train loss: 1.007771 [192000/345580]\n",
      "Train loss: 0.976411 [195200/345580]\n",
      "Train loss: 1.020682 [198400/345580]\n",
      "Train loss: 0.985169 [201600/345580]\n",
      "Train loss: 1.070593 [204800/345580]\n",
      "Train loss: 1.090659 [208000/345580]\n",
      "Train loss: 1.150194 [211200/345580]\n",
      "Train loss: 1.048797 [214400/345580]\n",
      "Train loss: 1.062131 [217600/345580]\n",
      "Train loss: 0.986700 [220800/345580]\n",
      "Train loss: 1.087927 [224000/345580]\n",
      "Train loss: 0.994928 [227200/345580]\n",
      "Train loss: 1.096132 [230400/345580]\n",
      "Train loss: 1.041145 [233600/345580]\n",
      "Train loss: 0.965333 [236800/345580]\n",
      "Train loss: 1.015502 [240000/345580]\n",
      "Train loss: 1.086590 [243200/345580]\n",
      "Train loss: 1.002026 [246400/345580]\n",
      "Train loss: 1.034325 [249600/345580]\n",
      "Train loss: 1.030202 [252800/345580]\n",
      "Train loss: 0.874876 [256000/345580]\n",
      "Train loss: 1.096868 [259200/345580]\n",
      "Train loss: 1.013521 [262400/345580]\n",
      "Train loss: 1.002174 [265600/345580]\n",
      "Train loss: 1.044383 [268800/345580]\n",
      "Train loss: 1.049603 [272000/345580]\n",
      "Train loss: 1.062973 [275200/345580]\n",
      "Train loss: 0.965305 [278400/345580]\n",
      "Train loss: 0.945676 [281600/345580]\n",
      "Train loss: 1.018910 [284800/345580]\n",
      "Train loss: 1.002401 [288000/345580]\n",
      "Train loss: 1.056966 [291200/345580]\n",
      "Train loss: 1.022579 [294400/345580]\n",
      "Train loss: 1.068200 [297600/345580]\n",
      "Train loss: 1.000996 [300800/345580]\n",
      "Train loss: 1.065699 [304000/345580]\n",
      "Train loss: 1.018104 [307200/345580]\n",
      "Train loss: 0.966682 [310400/345580]\n",
      "Train loss: 1.107697 [313600/345580]\n",
      "Train loss: 0.982267 [316800/345580]\n",
      "Train loss: 1.096016 [320000/345580]\n",
      "Train loss: 1.096764 [323200/345580]\n",
      "Train loss: 1.052046 [326400/345580]\n",
      "Train loss: 1.232249 [329600/345580]\n",
      "Train loss: 0.970022 [332800/345580]\n",
      "Train loss: 1.036302 [336000/345580]\n",
      "Train loss: 0.971542 [339200/345580]\n",
      "Train loss: 1.131327 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.041650 \n",
      "\n",
      "Epoch 27\n",
      " -------------------\n",
      "Train loss: 0.968216 [    0/345580]\n",
      "Train loss: 1.033450 [ 3200/345580]\n",
      "Train loss: 1.045594 [ 6400/345580]\n",
      "Train loss: 1.017736 [ 9600/345580]\n",
      "Train loss: 1.128390 [12800/345580]\n",
      "Train loss: 1.081404 [16000/345580]\n",
      "Train loss: 1.040421 [19200/345580]\n",
      "Train loss: 0.944465 [22400/345580]\n",
      "Train loss: 0.986676 [25600/345580]\n",
      "Train loss: 1.007600 [28800/345580]\n",
      "Train loss: 1.077281 [32000/345580]\n",
      "Train loss: 0.992040 [35200/345580]\n",
      "Train loss: 1.062281 [38400/345580]\n",
      "Train loss: 0.988759 [41600/345580]\n",
      "Train loss: 1.072634 [44800/345580]\n",
      "Train loss: 1.055182 [48000/345580]\n",
      "Train loss: 1.033381 [51200/345580]\n",
      "Train loss: 1.046772 [54400/345580]\n",
      "Train loss: 1.047064 [57600/345580]\n",
      "Train loss: 1.000109 [60800/345580]\n",
      "Train loss: 1.032369 [64000/345580]\n",
      "Train loss: 1.003716 [67200/345580]\n",
      "Train loss: 0.988170 [70400/345580]\n",
      "Train loss: 0.941268 [73600/345580]\n",
      "Train loss: 1.042593 [76800/345580]\n",
      "Train loss: 1.066974 [80000/345580]\n",
      "Train loss: 1.024990 [83200/345580]\n",
      "Train loss: 1.070600 [86400/345580]\n",
      "Train loss: 1.077810 [89600/345580]\n",
      "Train loss: 1.092516 [92800/345580]\n",
      "Train loss: 1.032273 [96000/345580]\n",
      "Train loss: 1.020828 [99200/345580]\n",
      "Train loss: 1.012971 [102400/345580]\n",
      "Train loss: 1.057311 [105600/345580]\n",
      "Train loss: 1.109872 [108800/345580]\n",
      "Train loss: 1.182122 [112000/345580]\n",
      "Train loss: 0.963535 [115200/345580]\n",
      "Train loss: 1.087903 [118400/345580]\n",
      "Train loss: 1.088877 [121600/345580]\n",
      "Train loss: 1.041785 [124800/345580]\n",
      "Train loss: 1.005145 [128000/345580]\n",
      "Train loss: 0.979638 [131200/345580]\n",
      "Train loss: 1.097887 [134400/345580]\n",
      "Train loss: 1.112427 [137600/345580]\n",
      "Train loss: 1.043791 [140800/345580]\n",
      "Train loss: 1.082765 [144000/345580]\n",
      "Train loss: 1.016906 [147200/345580]\n",
      "Train loss: 1.045183 [150400/345580]\n",
      "Train loss: 1.073322 [153600/345580]\n",
      "Train loss: 0.918163 [156800/345580]\n",
      "Train loss: 1.024258 [160000/345580]\n",
      "Train loss: 1.119246 [163200/345580]\n",
      "Train loss: 1.064546 [166400/345580]\n",
      "Train loss: 1.062332 [169600/345580]\n",
      "Train loss: 0.973433 [172800/345580]\n",
      "Train loss: 1.000803 [176000/345580]\n",
      "Train loss: 1.175176 [179200/345580]\n",
      "Train loss: 1.082918 [182400/345580]\n",
      "Train loss: 0.933554 [185600/345580]\n",
      "Train loss: 1.079112 [188800/345580]\n",
      "Train loss: 1.037733 [192000/345580]\n",
      "Train loss: 1.009452 [195200/345580]\n",
      "Train loss: 1.040105 [198400/345580]\n",
      "Train loss: 1.088136 [201600/345580]\n",
      "Train loss: 1.057221 [204800/345580]\n",
      "Train loss: 1.027456 [208000/345580]\n",
      "Train loss: 1.126609 [211200/345580]\n",
      "Train loss: 1.121395 [214400/345580]\n",
      "Train loss: 1.095407 [217600/345580]\n",
      "Train loss: 1.044420 [220800/345580]\n",
      "Train loss: 1.048745 [224000/345580]\n",
      "Train loss: 1.129823 [227200/345580]\n",
      "Train loss: 1.054559 [230400/345580]\n",
      "Train loss: 1.095518 [233600/345580]\n",
      "Train loss: 1.107556 [236800/345580]\n",
      "Train loss: 1.072121 [240000/345580]\n",
      "Train loss: 1.047362 [243200/345580]\n",
      "Train loss: 1.007306 [246400/345580]\n",
      "Train loss: 1.067552 [249600/345580]\n",
      "Train loss: 1.041033 [252800/345580]\n",
      "Train loss: 1.113161 [256000/345580]\n",
      "Train loss: 1.037701 [259200/345580]\n",
      "Train loss: 0.966034 [262400/345580]\n",
      "Train loss: 1.089046 [265600/345580]\n",
      "Train loss: 1.065171 [268800/345580]\n",
      "Train loss: 1.133662 [272000/345580]\n",
      "Train loss: 1.038617 [275200/345580]\n",
      "Train loss: 1.015526 [278400/345580]\n",
      "Train loss: 1.088979 [281600/345580]\n",
      "Train loss: 1.005531 [284800/345580]\n",
      "Train loss: 1.081995 [288000/345580]\n",
      "Train loss: 1.029646 [291200/345580]\n",
      "Train loss: 1.073731 [294400/345580]\n",
      "Train loss: 1.062274 [297600/345580]\n",
      "Train loss: 1.106433 [300800/345580]\n",
      "Train loss: 1.128365 [304000/345580]\n",
      "Train loss: 1.047948 [307200/345580]\n",
      "Train loss: 1.086773 [310400/345580]\n",
      "Train loss: 1.054428 [313600/345580]\n",
      "Train loss: 1.016464 [316800/345580]\n",
      "Train loss: 1.054837 [320000/345580]\n",
      "Train loss: 0.957656 [323200/345580]\n",
      "Train loss: 1.032788 [326400/345580]\n",
      "Train loss: 1.084516 [329600/345580]\n",
      "Train loss: 1.053025 [332800/345580]\n",
      "Train loss: 0.943870 [336000/345580]\n",
      "Train loss: 0.952439 [339200/345580]\n",
      "Train loss: 1.012784 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042061 \n",
      "\n",
      "Epoch 28\n",
      " -------------------\n",
      "Train loss: 1.045776 [    0/345580]\n",
      "Train loss: 1.018169 [ 3200/345580]\n",
      "Train loss: 1.035765 [ 6400/345580]\n",
      "Train loss: 1.115217 [ 9600/345580]\n",
      "Train loss: 1.076404 [12800/345580]\n",
      "Train loss: 1.041310 [16000/345580]\n",
      "Train loss: 1.042135 [19200/345580]\n",
      "Train loss: 1.015371 [22400/345580]\n",
      "Train loss: 1.062340 [25600/345580]\n",
      "Train loss: 1.017660 [28800/345580]\n",
      "Train loss: 1.039576 [32000/345580]\n",
      "Train loss: 1.086958 [35200/345580]\n",
      "Train loss: 1.038906 [38400/345580]\n",
      "Train loss: 1.069132 [41600/345580]\n",
      "Train loss: 1.012749 [44800/345580]\n",
      "Train loss: 1.029444 [48000/345580]\n",
      "Train loss: 1.043495 [51200/345580]\n",
      "Train loss: 1.036047 [54400/345580]\n",
      "Train loss: 1.080660 [57600/345580]\n",
      "Train loss: 0.976275 [60800/345580]\n",
      "Train loss: 1.076496 [64000/345580]\n",
      "Train loss: 1.067425 [67200/345580]\n",
      "Train loss: 1.081173 [70400/345580]\n",
      "Train loss: 1.065686 [73600/345580]\n",
      "Train loss: 1.163643 [76800/345580]\n",
      "Train loss: 1.029173 [80000/345580]\n",
      "Train loss: 1.035605 [83200/345580]\n",
      "Train loss: 0.946930 [86400/345580]\n",
      "Train loss: 1.119963 [89600/345580]\n",
      "Train loss: 1.090531 [92800/345580]\n",
      "Train loss: 1.029469 [96000/345580]\n",
      "Train loss: 1.122039 [99200/345580]\n",
      "Train loss: 1.004092 [102400/345580]\n",
      "Train loss: 1.071002 [105600/345580]\n",
      "Train loss: 1.117704 [108800/345580]\n",
      "Train loss: 0.990265 [112000/345580]\n",
      "Train loss: 1.160209 [115200/345580]\n",
      "Train loss: 0.940620 [118400/345580]\n",
      "Train loss: 1.063693 [121600/345580]\n",
      "Train loss: 1.061086 [124800/345580]\n",
      "Train loss: 1.022678 [128000/345580]\n",
      "Train loss: 1.033035 [131200/345580]\n",
      "Train loss: 1.050732 [134400/345580]\n",
      "Train loss: 0.984960 [137600/345580]\n",
      "Train loss: 1.094447 [140800/345580]\n",
      "Train loss: 1.068428 [144000/345580]\n",
      "Train loss: 1.040586 [147200/345580]\n",
      "Train loss: 1.015559 [150400/345580]\n",
      "Train loss: 0.931426 [153600/345580]\n",
      "Train loss: 0.993663 [156800/345580]\n",
      "Train loss: 1.124152 [160000/345580]\n",
      "Train loss: 1.023165 [163200/345580]\n",
      "Train loss: 1.034265 [166400/345580]\n",
      "Train loss: 1.038562 [169600/345580]\n",
      "Train loss: 1.190315 [172800/345580]\n",
      "Train loss: 1.059863 [176000/345580]\n",
      "Train loss: 1.011928 [179200/345580]\n",
      "Train loss: 0.954558 [182400/345580]\n",
      "Train loss: 1.110726 [185600/345580]\n",
      "Train loss: 1.057034 [188800/345580]\n",
      "Train loss: 1.051259 [192000/345580]\n",
      "Train loss: 0.996922 [195200/345580]\n",
      "Train loss: 0.988265 [198400/345580]\n",
      "Train loss: 1.013282 [201600/345580]\n",
      "Train loss: 0.978334 [204800/345580]\n",
      "Train loss: 1.120482 [208000/345580]\n",
      "Train loss: 1.146056 [211200/345580]\n",
      "Train loss: 0.980305 [214400/345580]\n",
      "Train loss: 0.916110 [217600/345580]\n",
      "Train loss: 1.041949 [220800/345580]\n",
      "Train loss: 1.009439 [224000/345580]\n",
      "Train loss: 0.965756 [227200/345580]\n",
      "Train loss: 1.007291 [230400/345580]\n",
      "Train loss: 0.944395 [233600/345580]\n",
      "Train loss: 1.150860 [236800/345580]\n",
      "Train loss: 0.959744 [240000/345580]\n",
      "Train loss: 1.000535 [243200/345580]\n",
      "Train loss: 0.904071 [246400/345580]\n",
      "Train loss: 1.069795 [249600/345580]\n",
      "Train loss: 0.966162 [252800/345580]\n",
      "Train loss: 1.131229 [256000/345580]\n",
      "Train loss: 1.075025 [259200/345580]\n",
      "Train loss: 0.974883 [262400/345580]\n",
      "Train loss: 1.009690 [265600/345580]\n",
      "Train loss: 1.056141 [268800/345580]\n",
      "Train loss: 1.198728 [272000/345580]\n",
      "Train loss: 0.942876 [275200/345580]\n",
      "Train loss: 1.079090 [278400/345580]\n",
      "Train loss: 0.995189 [281600/345580]\n",
      "Train loss: 1.057549 [284800/345580]\n",
      "Train loss: 1.113699 [288000/345580]\n",
      "Train loss: 0.984943 [291200/345580]\n",
      "Train loss: 1.011019 [294400/345580]\n",
      "Train loss: 0.996486 [297600/345580]\n",
      "Train loss: 1.014474 [300800/345580]\n",
      "Train loss: 0.962895 [304000/345580]\n",
      "Train loss: 1.071718 [307200/345580]\n",
      "Train loss: 1.076899 [310400/345580]\n",
      "Train loss: 1.103155 [313600/345580]\n",
      "Train loss: 1.086217 [316800/345580]\n",
      "Train loss: 1.046449 [320000/345580]\n",
      "Train loss: 1.018634 [323200/345580]\n",
      "Train loss: 0.893339 [326400/345580]\n",
      "Train loss: 1.014418 [329600/345580]\n",
      "Train loss: 1.076139 [332800/345580]\n",
      "Train loss: 1.033576 [336000/345580]\n",
      "Train loss: 1.096219 [339200/345580]\n",
      "Train loss: 0.996615 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.042359 \n",
      "\n",
      "Epoch 29\n",
      " -------------------\n",
      "Train loss: 0.923508 [    0/345580]\n",
      "Train loss: 1.073377 [ 3200/345580]\n",
      "Train loss: 1.144745 [ 6400/345580]\n",
      "Train loss: 1.043505 [ 9600/345580]\n",
      "Train loss: 1.039434 [12800/345580]\n",
      "Train loss: 0.971440 [16000/345580]\n",
      "Train loss: 1.128991 [19200/345580]\n",
      "Train loss: 1.069107 [22400/345580]\n",
      "Train loss: 1.073721 [25600/345580]\n",
      "Train loss: 1.039305 [28800/345580]\n",
      "Train loss: 1.054493 [32000/345580]\n",
      "Train loss: 1.035377 [35200/345580]\n",
      "Train loss: 0.915509 [38400/345580]\n",
      "Train loss: 0.971914 [41600/345580]\n",
      "Train loss: 1.078674 [44800/345580]\n",
      "Train loss: 0.994131 [48000/345580]\n",
      "Train loss: 0.995405 [51200/345580]\n",
      "Train loss: 1.130181 [54400/345580]\n",
      "Train loss: 1.022845 [57600/345580]\n",
      "Train loss: 1.008974 [60800/345580]\n",
      "Train loss: 1.012240 [64000/345580]\n",
      "Train loss: 1.089242 [67200/345580]\n",
      "Train loss: 1.040487 [70400/345580]\n",
      "Train loss: 1.007824 [73600/345580]\n",
      "Train loss: 1.063027 [76800/345580]\n",
      "Train loss: 1.221109 [80000/345580]\n",
      "Train loss: 0.934519 [83200/345580]\n",
      "Train loss: 1.180941 [86400/345580]\n",
      "Train loss: 0.995022 [89600/345580]\n",
      "Train loss: 0.968218 [92800/345580]\n",
      "Train loss: 1.006839 [96000/345580]\n",
      "Train loss: 1.028458 [99200/345580]\n",
      "Train loss: 1.130493 [102400/345580]\n",
      "Train loss: 1.011559 [105600/345580]\n",
      "Train loss: 0.921026 [108800/345580]\n",
      "Train loss: 1.119478 [112000/345580]\n",
      "Train loss: 1.244515 [115200/345580]\n",
      "Train loss: 1.105157 [118400/345580]\n",
      "Train loss: 0.985936 [121600/345580]\n",
      "Train loss: 1.085133 [124800/345580]\n",
      "Train loss: 1.089657 [128000/345580]\n",
      "Train loss: 0.932222 [131200/345580]\n",
      "Train loss: 1.153618 [134400/345580]\n",
      "Train loss: 1.040363 [137600/345580]\n",
      "Train loss: 1.080113 [140800/345580]\n",
      "Train loss: 1.013749 [144000/345580]\n",
      "Train loss: 1.040576 [147200/345580]\n",
      "Train loss: 0.989415 [150400/345580]\n",
      "Train loss: 1.106057 [153600/345580]\n",
      "Train loss: 1.078330 [156800/345580]\n",
      "Train loss: 1.045323 [160000/345580]\n",
      "Train loss: 1.009251 [163200/345580]\n",
      "Train loss: 1.082676 [166400/345580]\n",
      "Train loss: 0.931185 [169600/345580]\n",
      "Train loss: 0.996475 [172800/345580]\n",
      "Train loss: 0.999174 [176000/345580]\n",
      "Train loss: 1.035836 [179200/345580]\n",
      "Train loss: 1.164988 [182400/345580]\n",
      "Train loss: 1.003267 [185600/345580]\n",
      "Train loss: 1.005618 [188800/345580]\n",
      "Train loss: 1.079307 [192000/345580]\n",
      "Train loss: 1.015292 [195200/345580]\n",
      "Train loss: 1.032961 [198400/345580]\n",
      "Train loss: 1.075166 [201600/345580]\n",
      "Train loss: 1.070354 [204800/345580]\n",
      "Train loss: 1.134134 [208000/345580]\n",
      "Train loss: 0.957966 [211200/345580]\n",
      "Train loss: 0.991160 [214400/345580]\n",
      "Train loss: 1.018307 [217600/345580]\n",
      "Train loss: 1.031618 [220800/345580]\n",
      "Train loss: 0.991881 [224000/345580]\n",
      "Train loss: 1.046767 [227200/345580]\n",
      "Train loss: 1.023322 [230400/345580]\n",
      "Train loss: 0.989579 [233600/345580]\n",
      "Train loss: 1.013928 [236800/345580]\n",
      "Train loss: 1.031952 [240000/345580]\n",
      "Train loss: 1.058275 [243200/345580]\n",
      "Train loss: 1.092874 [246400/345580]\n",
      "Train loss: 1.067077 [249600/345580]\n",
      "Train loss: 0.975577 [252800/345580]\n",
      "Train loss: 1.100055 [256000/345580]\n",
      "Train loss: 1.077611 [259200/345580]\n",
      "Train loss: 0.974514 [262400/345580]\n",
      "Train loss: 1.103022 [265600/345580]\n",
      "Train loss: 0.973911 [268800/345580]\n",
      "Train loss: 1.133044 [272000/345580]\n",
      "Train loss: 1.128075 [275200/345580]\n",
      "Train loss: 1.000686 [278400/345580]\n",
      "Train loss: 1.024443 [281600/345580]\n",
      "Train loss: 1.010155 [284800/345580]\n",
      "Train loss: 1.050393 [288000/345580]\n",
      "Train loss: 0.970549 [291200/345580]\n",
      "Train loss: 0.934564 [294400/345580]\n",
      "Train loss: 1.070759 [297600/345580]\n",
      "Train loss: 1.103739 [300800/345580]\n",
      "Train loss: 1.056662 [304000/345580]\n",
      "Train loss: 0.961847 [307200/345580]\n",
      "Train loss: 1.114825 [310400/345580]\n",
      "Train loss: 0.991325 [313600/345580]\n",
      "Train loss: 0.950314 [316800/345580]\n",
      "Train loss: 1.031833 [320000/345580]\n",
      "Train loss: 1.089366 [323200/345580]\n",
      "Train loss: 1.118167 [326400/345580]\n",
      "Train loss: 1.002106 [329600/345580]\n",
      "Train loss: 1.001669 [332800/345580]\n",
      "Train loss: 1.045794 [336000/345580]\n",
      "Train loss: 0.999047 [339200/345580]\n",
      "Train loss: 1.138309 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042015 \n",
      "\n",
      "Epoch 30\n",
      " -------------------\n",
      "Train loss: 1.062208 [    0/345580]\n",
      "Train loss: 1.105117 [ 3200/345580]\n",
      "Train loss: 1.075743 [ 6400/345580]\n",
      "Train loss: 1.078568 [ 9600/345580]\n",
      "Train loss: 1.070529 [12800/345580]\n",
      "Train loss: 1.080934 [16000/345580]\n",
      "Train loss: 1.007440 [19200/345580]\n",
      "Train loss: 1.085721 [22400/345580]\n",
      "Train loss: 1.067818 [25600/345580]\n",
      "Train loss: 1.180983 [28800/345580]\n",
      "Train loss: 1.117643 [32000/345580]\n",
      "Train loss: 1.067664 [35200/345580]\n",
      "Train loss: 1.176110 [38400/345580]\n",
      "Train loss: 0.996617 [41600/345580]\n",
      "Train loss: 1.025233 [44800/345580]\n",
      "Train loss: 1.093874 [48000/345580]\n",
      "Train loss: 1.054455 [51200/345580]\n",
      "Train loss: 1.138947 [54400/345580]\n",
      "Train loss: 1.076391 [57600/345580]\n",
      "Train loss: 1.081129 [60800/345580]\n",
      "Train loss: 1.011248 [64000/345580]\n",
      "Train loss: 1.043950 [67200/345580]\n",
      "Train loss: 0.967722 [70400/345580]\n",
      "Train loss: 1.041882 [73600/345580]\n",
      "Train loss: 1.000963 [76800/345580]\n",
      "Train loss: 1.181667 [80000/345580]\n",
      "Train loss: 1.098031 [83200/345580]\n",
      "Train loss: 0.975453 [86400/345580]\n",
      "Train loss: 1.036744 [89600/345580]\n",
      "Train loss: 1.121760 [92800/345580]\n",
      "Train loss: 1.107365 [96000/345580]\n",
      "Train loss: 1.091243 [99200/345580]\n",
      "Train loss: 1.076394 [102400/345580]\n",
      "Train loss: 1.102846 [105600/345580]\n",
      "Train loss: 1.073661 [108800/345580]\n",
      "Train loss: 1.074990 [112000/345580]\n",
      "Train loss: 1.089091 [115200/345580]\n",
      "Train loss: 1.073433 [118400/345580]\n",
      "Train loss: 1.008263 [121600/345580]\n",
      "Train loss: 1.062125 [124800/345580]\n",
      "Train loss: 1.042975 [128000/345580]\n",
      "Train loss: 1.016196 [131200/345580]\n",
      "Train loss: 1.071228 [134400/345580]\n",
      "Train loss: 1.046456 [137600/345580]\n",
      "Train loss: 1.046526 [140800/345580]\n",
      "Train loss: 1.015247 [144000/345580]\n",
      "Train loss: 0.993349 [147200/345580]\n",
      "Train loss: 1.011777 [150400/345580]\n",
      "Train loss: 1.014149 [153600/345580]\n",
      "Train loss: 1.009478 [156800/345580]\n",
      "Train loss: 0.981007 [160000/345580]\n",
      "Train loss: 1.042164 [163200/345580]\n",
      "Train loss: 1.086702 [166400/345580]\n",
      "Train loss: 1.004348 [169600/345580]\n",
      "Train loss: 1.034914 [172800/345580]\n",
      "Train loss: 1.035596 [176000/345580]\n",
      "Train loss: 1.013805 [179200/345580]\n",
      "Train loss: 0.960355 [182400/345580]\n",
      "Train loss: 1.079216 [185600/345580]\n",
      "Train loss: 1.056948 [188800/345580]\n",
      "Train loss: 1.025468 [192000/345580]\n",
      "Train loss: 1.014473 [195200/345580]\n",
      "Train loss: 1.028522 [198400/345580]\n",
      "Train loss: 1.042343 [201600/345580]\n",
      "Train loss: 1.063873 [204800/345580]\n",
      "Train loss: 0.990840 [208000/345580]\n",
      "Train loss: 1.069974 [211200/345580]\n",
      "Train loss: 0.975172 [214400/345580]\n",
      "Train loss: 1.089066 [217600/345580]\n",
      "Train loss: 1.156525 [220800/345580]\n",
      "Train loss: 1.018452 [224000/345580]\n",
      "Train loss: 1.074278 [227200/345580]\n",
      "Train loss: 1.057374 [230400/345580]\n",
      "Train loss: 1.114824 [233600/345580]\n",
      "Train loss: 0.963275 [236800/345580]\n",
      "Train loss: 1.044374 [240000/345580]\n",
      "Train loss: 1.048330 [243200/345580]\n",
      "Train loss: 1.117839 [246400/345580]\n",
      "Train loss: 0.976272 [249600/345580]\n",
      "Train loss: 0.966896 [252800/345580]\n",
      "Train loss: 1.013303 [256000/345580]\n",
      "Train loss: 1.130189 [259200/345580]\n",
      "Train loss: 1.002789 [262400/345580]\n",
      "Train loss: 1.060512 [265600/345580]\n",
      "Train loss: 1.066308 [268800/345580]\n",
      "Train loss: 1.070864 [272000/345580]\n",
      "Train loss: 0.992990 [275200/345580]\n",
      "Train loss: 1.042922 [278400/345580]\n",
      "Train loss: 1.046652 [281600/345580]\n",
      "Train loss: 1.080243 [284800/345580]\n",
      "Train loss: 1.108601 [288000/345580]\n",
      "Train loss: 1.034722 [291200/345580]\n",
      "Train loss: 1.084097 [294400/345580]\n",
      "Train loss: 0.999896 [297600/345580]\n",
      "Train loss: 1.028483 [300800/345580]\n",
      "Train loss: 1.114561 [304000/345580]\n",
      "Train loss: 1.032917 [307200/345580]\n",
      "Train loss: 1.095834 [310400/345580]\n",
      "Train loss: 1.067856 [313600/345580]\n",
      "Train loss: 0.893806 [316800/345580]\n",
      "Train loss: 0.954965 [320000/345580]\n",
      "Train loss: 0.987531 [323200/345580]\n",
      "Train loss: 1.041841 [326400/345580]\n",
      "Train loss: 1.039278 [329600/345580]\n",
      "Train loss: 0.982879 [332800/345580]\n",
      "Train loss: 1.055670 [336000/345580]\n",
      "Train loss: 1.068218 [339200/345580]\n",
      "Train loss: 1.030534 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.042352 \n",
      "\n",
      "Epoch 31\n",
      " -------------------\n",
      "Train loss: 1.089435 [    0/345580]\n",
      "Train loss: 1.022112 [ 3200/345580]\n",
      "Train loss: 1.034521 [ 6400/345580]\n",
      "Train loss: 1.074469 [ 9600/345580]\n",
      "Train loss: 1.039798 [12800/345580]\n",
      "Train loss: 1.044114 [16000/345580]\n",
      "Train loss: 1.034352 [19200/345580]\n",
      "Train loss: 1.044495 [22400/345580]\n",
      "Train loss: 1.075988 [25600/345580]\n",
      "Train loss: 1.085212 [28800/345580]\n",
      "Train loss: 1.001734 [32000/345580]\n",
      "Train loss: 1.088585 [35200/345580]\n",
      "Train loss: 1.051347 [38400/345580]\n",
      "Train loss: 1.005953 [41600/345580]\n",
      "Train loss: 1.022317 [44800/345580]\n",
      "Train loss: 1.049343 [48000/345580]\n",
      "Train loss: 0.995076 [51200/345580]\n",
      "Train loss: 1.063179 [54400/345580]\n",
      "Train loss: 0.941157 [57600/345580]\n",
      "Train loss: 1.073918 [60800/345580]\n",
      "Train loss: 1.049077 [64000/345580]\n",
      "Train loss: 1.093586 [67200/345580]\n",
      "Train loss: 1.069105 [70400/345580]\n",
      "Train loss: 1.111774 [73600/345580]\n",
      "Train loss: 0.995411 [76800/345580]\n",
      "Train loss: 1.031649 [80000/345580]\n",
      "Train loss: 1.111305 [83200/345580]\n",
      "Train loss: 1.003553 [86400/345580]\n",
      "Train loss: 1.057779 [89600/345580]\n",
      "Train loss: 1.071729 [92800/345580]\n",
      "Train loss: 0.955364 [96000/345580]\n",
      "Train loss: 1.108158 [99200/345580]\n",
      "Train loss: 1.018486 [102400/345580]\n",
      "Train loss: 1.106165 [105600/345580]\n",
      "Train loss: 1.016490 [108800/345580]\n",
      "Train loss: 1.047759 [112000/345580]\n",
      "Train loss: 0.997324 [115200/345580]\n",
      "Train loss: 0.952293 [118400/345580]\n",
      "Train loss: 1.042678 [121600/345580]\n",
      "Train loss: 1.083777 [124800/345580]\n",
      "Train loss: 0.996904 [128000/345580]\n",
      "Train loss: 1.005676 [131200/345580]\n",
      "Train loss: 1.011861 [134400/345580]\n",
      "Train loss: 0.981868 [137600/345580]\n",
      "Train loss: 0.966082 [140800/345580]\n",
      "Train loss: 1.043058 [144000/345580]\n",
      "Train loss: 1.110653 [147200/345580]\n",
      "Train loss: 0.999317 [150400/345580]\n",
      "Train loss: 1.111351 [153600/345580]\n",
      "Train loss: 0.940762 [156800/345580]\n",
      "Train loss: 1.043423 [160000/345580]\n",
      "Train loss: 0.984763 [163200/345580]\n",
      "Train loss: 1.059278 [166400/345580]\n",
      "Train loss: 1.040748 [169600/345580]\n",
      "Train loss: 1.105576 [172800/345580]\n",
      "Train loss: 1.165432 [176000/345580]\n",
      "Train loss: 0.996778 [179200/345580]\n",
      "Train loss: 1.043481 [182400/345580]\n",
      "Train loss: 1.175682 [185600/345580]\n",
      "Train loss: 0.948210 [188800/345580]\n",
      "Train loss: 1.045241 [192000/345580]\n",
      "Train loss: 1.082439 [195200/345580]\n",
      "Train loss: 1.005509 [198400/345580]\n",
      "Train loss: 0.978166 [201600/345580]\n",
      "Train loss: 1.000632 [204800/345580]\n",
      "Train loss: 1.057716 [208000/345580]\n",
      "Train loss: 1.024060 [211200/345580]\n",
      "Train loss: 1.096107 [214400/345580]\n",
      "Train loss: 1.042657 [217600/345580]\n",
      "Train loss: 1.034810 [220800/345580]\n",
      "Train loss: 1.028439 [224000/345580]\n",
      "Train loss: 1.085300 [227200/345580]\n",
      "Train loss: 1.038714 [230400/345580]\n",
      "Train loss: 0.982729 [233600/345580]\n",
      "Train loss: 1.108785 [236800/345580]\n",
      "Train loss: 1.015023 [240000/345580]\n",
      "Train loss: 0.999151 [243200/345580]\n",
      "Train loss: 1.036290 [246400/345580]\n",
      "Train loss: 1.003173 [249600/345580]\n",
      "Train loss: 1.008387 [252800/345580]\n",
      "Train loss: 1.006995 [256000/345580]\n",
      "Train loss: 1.030358 [259200/345580]\n",
      "Train loss: 1.056128 [262400/345580]\n",
      "Train loss: 1.083702 [265600/345580]\n",
      "Train loss: 1.152313 [268800/345580]\n",
      "Train loss: 0.997597 [272000/345580]\n",
      "Train loss: 0.980354 [275200/345580]\n",
      "Train loss: 1.107397 [278400/345580]\n",
      "Train loss: 1.083530 [281600/345580]\n",
      "Train loss: 1.100271 [284800/345580]\n",
      "Train loss: 1.060902 [288000/345580]\n",
      "Train loss: 0.995653 [291200/345580]\n",
      "Train loss: 0.981533 [294400/345580]\n",
      "Train loss: 0.955334 [297600/345580]\n",
      "Train loss: 1.047061 [300800/345580]\n",
      "Train loss: 1.123872 [304000/345580]\n",
      "Train loss: 0.977391 [307200/345580]\n",
      "Train loss: 1.027167 [310400/345580]\n",
      "Train loss: 1.138901 [313600/345580]\n",
      "Train loss: 1.086020 [316800/345580]\n",
      "Train loss: 0.950287 [320000/345580]\n",
      "Train loss: 0.994824 [323200/345580]\n",
      "Train loss: 0.974520 [326400/345580]\n",
      "Train loss: 1.000466 [329600/345580]\n",
      "Train loss: 1.176512 [332800/345580]\n",
      "Train loss: 1.002668 [336000/345580]\n",
      "Train loss: 1.009461 [339200/345580]\n",
      "Train loss: 1.039289 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042352 \n",
      "\n",
      "Epoch 32\n",
      " -------------------\n",
      "Train loss: 1.054082 [    0/345580]\n",
      "Train loss: 1.042449 [ 3200/345580]\n",
      "Train loss: 0.963559 [ 6400/345580]\n",
      "Train loss: 0.925812 [ 9600/345580]\n",
      "Train loss: 1.069229 [12800/345580]\n",
      "Train loss: 1.031996 [16000/345580]\n",
      "Train loss: 1.015677 [19200/345580]\n",
      "Train loss: 1.169565 [22400/345580]\n",
      "Train loss: 1.056512 [25600/345580]\n",
      "Train loss: 1.042121 [28800/345580]\n",
      "Train loss: 1.012567 [32000/345580]\n",
      "Train loss: 1.074732 [35200/345580]\n",
      "Train loss: 0.988166 [38400/345580]\n",
      "Train loss: 1.110181 [41600/345580]\n",
      "Train loss: 0.999666 [44800/345580]\n",
      "Train loss: 1.033952 [48000/345580]\n",
      "Train loss: 1.107818 [51200/345580]\n",
      "Train loss: 1.040150 [54400/345580]\n",
      "Train loss: 1.031317 [57600/345580]\n",
      "Train loss: 1.026463 [60800/345580]\n",
      "Train loss: 1.099296 [64000/345580]\n",
      "Train loss: 0.996739 [67200/345580]\n",
      "Train loss: 1.139498 [70400/345580]\n",
      "Train loss: 0.986724 [73600/345580]\n",
      "Train loss: 1.052354 [76800/345580]\n",
      "Train loss: 1.100405 [80000/345580]\n",
      "Train loss: 0.922998 [83200/345580]\n",
      "Train loss: 1.113587 [86400/345580]\n",
      "Train loss: 0.999278 [89600/345580]\n",
      "Train loss: 1.048280 [92800/345580]\n",
      "Train loss: 1.016831 [96000/345580]\n",
      "Train loss: 1.164231 [99200/345580]\n",
      "Train loss: 1.034767 [102400/345580]\n",
      "Train loss: 1.013337 [105600/345580]\n",
      "Train loss: 1.016706 [108800/345580]\n",
      "Train loss: 1.090602 [112000/345580]\n",
      "Train loss: 1.005294 [115200/345580]\n",
      "Train loss: 1.132936 [118400/345580]\n",
      "Train loss: 1.064147 [121600/345580]\n",
      "Train loss: 1.067216 [124800/345580]\n",
      "Train loss: 0.962410 [128000/345580]\n",
      "Train loss: 0.999278 [131200/345580]\n",
      "Train loss: 1.066931 [134400/345580]\n",
      "Train loss: 1.072515 [137600/345580]\n",
      "Train loss: 1.077591 [140800/345580]\n",
      "Train loss: 1.016160 [144000/345580]\n",
      "Train loss: 1.020508 [147200/345580]\n",
      "Train loss: 1.024247 [150400/345580]\n",
      "Train loss: 0.932850 [153600/345580]\n",
      "Train loss: 1.182020 [156800/345580]\n",
      "Train loss: 1.112926 [160000/345580]\n",
      "Train loss: 1.018005 [163200/345580]\n",
      "Train loss: 1.088131 [166400/345580]\n",
      "Train loss: 1.042765 [169600/345580]\n",
      "Train loss: 1.060277 [172800/345580]\n",
      "Train loss: 1.078048 [176000/345580]\n",
      "Train loss: 0.992982 [179200/345580]\n",
      "Train loss: 0.995247 [182400/345580]\n",
      "Train loss: 0.990061 [185600/345580]\n",
      "Train loss: 0.981881 [188800/345580]\n",
      "Train loss: 1.031879 [192000/345580]\n",
      "Train loss: 1.088864 [195200/345580]\n",
      "Train loss: 1.056460 [198400/345580]\n",
      "Train loss: 1.050190 [201600/345580]\n",
      "Train loss: 1.001472 [204800/345580]\n",
      "Train loss: 1.047931 [208000/345580]\n",
      "Train loss: 0.939370 [211200/345580]\n",
      "Train loss: 0.995589 [214400/345580]\n",
      "Train loss: 1.086316 [217600/345580]\n",
      "Train loss: 1.066928 [220800/345580]\n",
      "Train loss: 1.032865 [224000/345580]\n",
      "Train loss: 1.077764 [227200/345580]\n",
      "Train loss: 0.982965 [230400/345580]\n",
      "Train loss: 1.003565 [233600/345580]\n",
      "Train loss: 0.999040 [236800/345580]\n",
      "Train loss: 1.005249 [240000/345580]\n",
      "Train loss: 1.045641 [243200/345580]\n",
      "Train loss: 1.027288 [246400/345580]\n",
      "Train loss: 1.143890 [249600/345580]\n",
      "Train loss: 1.065257 [252800/345580]\n",
      "Train loss: 1.078742 [256000/345580]\n",
      "Train loss: 1.030793 [259200/345580]\n",
      "Train loss: 1.019632 [262400/345580]\n",
      "Train loss: 1.094049 [265600/345580]\n",
      "Train loss: 1.154675 [268800/345580]\n",
      "Train loss: 1.093499 [272000/345580]\n",
      "Train loss: 1.215392 [275200/345580]\n",
      "Train loss: 1.005967 [278400/345580]\n",
      "Train loss: 1.097113 [281600/345580]\n",
      "Train loss: 0.940411 [284800/345580]\n",
      "Train loss: 1.097210 [288000/345580]\n",
      "Train loss: 1.036201 [291200/345580]\n",
      "Train loss: 1.056039 [294400/345580]\n",
      "Train loss: 1.101670 [297600/345580]\n",
      "Train loss: 1.000470 [300800/345580]\n",
      "Train loss: 1.180637 [304000/345580]\n",
      "Train loss: 1.065514 [307200/345580]\n",
      "Train loss: 0.972838 [310400/345580]\n",
      "Train loss: 1.049913 [313600/345580]\n",
      "Train loss: 1.112578 [316800/345580]\n",
      "Train loss: 1.102932 [320000/345580]\n",
      "Train loss: 1.093722 [323200/345580]\n",
      "Train loss: 1.093685 [326400/345580]\n",
      "Train loss: 0.975495 [329600/345580]\n",
      "Train loss: 1.072899 [332800/345580]\n",
      "Train loss: 1.051493 [336000/345580]\n",
      "Train loss: 1.074861 [339200/345580]\n",
      "Train loss: 1.020894 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042973 \n",
      "\n",
      "Epoch 33\n",
      " -------------------\n",
      "Train loss: 1.135476 [    0/345580]\n",
      "Train loss: 1.031891 [ 3200/345580]\n",
      "Train loss: 1.085528 [ 6400/345580]\n",
      "Train loss: 0.988058 [ 9600/345580]\n",
      "Train loss: 0.962452 [12800/345580]\n",
      "Train loss: 0.941791 [16000/345580]\n",
      "Train loss: 1.110185 [19200/345580]\n",
      "Train loss: 1.018155 [22400/345580]\n",
      "Train loss: 1.056159 [25600/345580]\n",
      "Train loss: 1.048944 [28800/345580]\n",
      "Train loss: 1.123588 [32000/345580]\n",
      "Train loss: 1.170377 [35200/345580]\n",
      "Train loss: 0.997731 [38400/345580]\n",
      "Train loss: 1.034317 [41600/345580]\n",
      "Train loss: 1.063255 [44800/345580]\n",
      "Train loss: 1.083851 [48000/345580]\n",
      "Train loss: 1.078921 [51200/345580]\n",
      "Train loss: 1.048586 [54400/345580]\n",
      "Train loss: 1.077357 [57600/345580]\n",
      "Train loss: 1.049730 [60800/345580]\n",
      "Train loss: 0.992460 [64000/345580]\n",
      "Train loss: 1.104896 [67200/345580]\n",
      "Train loss: 0.966863 [70400/345580]\n",
      "Train loss: 1.180123 [73600/345580]\n",
      "Train loss: 0.969698 [76800/345580]\n",
      "Train loss: 1.037502 [80000/345580]\n",
      "Train loss: 0.956009 [83200/345580]\n",
      "Train loss: 0.945091 [86400/345580]\n",
      "Train loss: 1.029765 [89600/345580]\n",
      "Train loss: 1.168849 [92800/345580]\n",
      "Train loss: 1.074145 [96000/345580]\n",
      "Train loss: 0.977052 [99200/345580]\n",
      "Train loss: 1.010075 [102400/345580]\n",
      "Train loss: 0.984099 [105600/345580]\n",
      "Train loss: 0.977912 [108800/345580]\n",
      "Train loss: 1.069765 [112000/345580]\n",
      "Train loss: 1.055195 [115200/345580]\n",
      "Train loss: 0.943036 [118400/345580]\n",
      "Train loss: 1.069958 [121600/345580]\n",
      "Train loss: 0.981623 [124800/345580]\n",
      "Train loss: 1.066327 [128000/345580]\n",
      "Train loss: 1.041803 [131200/345580]\n",
      "Train loss: 0.993892 [134400/345580]\n",
      "Train loss: 0.958023 [137600/345580]\n",
      "Train loss: 1.122749 [140800/345580]\n",
      "Train loss: 0.993136 [144000/345580]\n",
      "Train loss: 1.065831 [147200/345580]\n",
      "Train loss: 0.931926 [150400/345580]\n",
      "Train loss: 1.115242 [153600/345580]\n",
      "Train loss: 1.026645 [156800/345580]\n",
      "Train loss: 1.056126 [160000/345580]\n",
      "Train loss: 0.990070 [163200/345580]\n",
      "Train loss: 1.029019 [166400/345580]\n",
      "Train loss: 0.978365 [169600/345580]\n",
      "Train loss: 1.090500 [172800/345580]\n",
      "Train loss: 1.033662 [176000/345580]\n",
      "Train loss: 1.083447 [179200/345580]\n",
      "Train loss: 1.032610 [182400/345580]\n",
      "Train loss: 1.112725 [185600/345580]\n",
      "Train loss: 1.029144 [188800/345580]\n",
      "Train loss: 1.166003 [192000/345580]\n",
      "Train loss: 1.064606 [195200/345580]\n",
      "Train loss: 1.073739 [198400/345580]\n",
      "Train loss: 1.045762 [201600/345580]\n",
      "Train loss: 1.072829 [204800/345580]\n",
      "Train loss: 0.971259 [208000/345580]\n",
      "Train loss: 1.089760 [211200/345580]\n",
      "Train loss: 1.032059 [214400/345580]\n",
      "Train loss: 1.091030 [217600/345580]\n",
      "Train loss: 0.974022 [220800/345580]\n",
      "Train loss: 0.951431 [224000/345580]\n",
      "Train loss: 1.079257 [227200/345580]\n",
      "Train loss: 0.950102 [230400/345580]\n",
      "Train loss: 1.079834 [233600/345580]\n",
      "Train loss: 1.124043 [236800/345580]\n",
      "Train loss: 1.073720 [240000/345580]\n",
      "Train loss: 1.009622 [243200/345580]\n",
      "Train loss: 1.015236 [246400/345580]\n",
      "Train loss: 1.142255 [249600/345580]\n",
      "Train loss: 0.954965 [252800/345580]\n",
      "Train loss: 1.005495 [256000/345580]\n",
      "Train loss: 1.032085 [259200/345580]\n",
      "Train loss: 1.089590 [262400/345580]\n",
      "Train loss: 1.130618 [265600/345580]\n",
      "Train loss: 0.976647 [268800/345580]\n",
      "Train loss: 1.052851 [272000/345580]\n",
      "Train loss: 1.003314 [275200/345580]\n",
      "Train loss: 1.022137 [278400/345580]\n",
      "Train loss: 1.116911 [281600/345580]\n",
      "Train loss: 0.947496 [284800/345580]\n",
      "Train loss: 1.023310 [288000/345580]\n",
      "Train loss: 0.992598 [291200/345580]\n",
      "Train loss: 1.056958 [294400/345580]\n",
      "Train loss: 1.016940 [297600/345580]\n",
      "Train loss: 1.043538 [300800/345580]\n",
      "Train loss: 1.056118 [304000/345580]\n",
      "Train loss: 1.067854 [307200/345580]\n",
      "Train loss: 1.055394 [310400/345580]\n",
      "Train loss: 1.082116 [313600/345580]\n",
      "Train loss: 1.199286 [316800/345580]\n",
      "Train loss: 1.027502 [320000/345580]\n",
      "Train loss: 1.053908 [323200/345580]\n",
      "Train loss: 0.950899 [326400/345580]\n",
      "Train loss: 1.061076 [329600/345580]\n",
      "Train loss: 1.040741 [332800/345580]\n",
      "Train loss: 1.167313 [336000/345580]\n",
      "Train loss: 1.050027 [339200/345580]\n",
      "Train loss: 1.147808 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.3%, Avg loss: 1.043298 \n",
      "\n",
      "Epoch 34\n",
      " -------------------\n",
      "Train loss: 0.973721 [    0/345580]\n",
      "Train loss: 1.066394 [ 3200/345580]\n",
      "Train loss: 0.977038 [ 6400/345580]\n",
      "Train loss: 0.999052 [ 9600/345580]\n",
      "Train loss: 1.093120 [12800/345580]\n",
      "Train loss: 1.071994 [16000/345580]\n",
      "Train loss: 1.049224 [19200/345580]\n",
      "Train loss: 0.990252 [22400/345580]\n",
      "Train loss: 1.059158 [25600/345580]\n",
      "Train loss: 0.972730 [28800/345580]\n",
      "Train loss: 1.003116 [32000/345580]\n",
      "Train loss: 1.092540 [35200/345580]\n",
      "Train loss: 1.062659 [38400/345580]\n",
      "Train loss: 1.021177 [41600/345580]\n",
      "Train loss: 1.179586 [44800/345580]\n",
      "Train loss: 1.082289 [48000/345580]\n",
      "Train loss: 1.035733 [51200/345580]\n",
      "Train loss: 0.999296 [54400/345580]\n",
      "Train loss: 1.000177 [57600/345580]\n",
      "Train loss: 1.073664 [60800/345580]\n",
      "Train loss: 1.100331 [64000/345580]\n",
      "Train loss: 1.114498 [67200/345580]\n",
      "Train loss: 1.057345 [70400/345580]\n",
      "Train loss: 1.138306 [73600/345580]\n",
      "Train loss: 0.957765 [76800/345580]\n",
      "Train loss: 1.076717 [80000/345580]\n",
      "Train loss: 0.939085 [83200/345580]\n",
      "Train loss: 1.136182 [86400/345580]\n",
      "Train loss: 1.022028 [89600/345580]\n",
      "Train loss: 1.185342 [92800/345580]\n",
      "Train loss: 1.059404 [96000/345580]\n",
      "Train loss: 0.956615 [99200/345580]\n",
      "Train loss: 1.056507 [102400/345580]\n",
      "Train loss: 1.036741 [105600/345580]\n",
      "Train loss: 1.046429 [108800/345580]\n",
      "Train loss: 1.055459 [112000/345580]\n",
      "Train loss: 1.048728 [115200/345580]\n",
      "Train loss: 1.010378 [118400/345580]\n",
      "Train loss: 0.951136 [121600/345580]\n",
      "Train loss: 1.040273 [124800/345580]\n",
      "Train loss: 1.021348 [128000/345580]\n",
      "Train loss: 1.038940 [131200/345580]\n",
      "Train loss: 1.151984 [134400/345580]\n",
      "Train loss: 0.978637 [137600/345580]\n",
      "Train loss: 0.974929 [140800/345580]\n",
      "Train loss: 1.055455 [144000/345580]\n",
      "Train loss: 1.070047 [147200/345580]\n",
      "Train loss: 1.025836 [150400/345580]\n",
      "Train loss: 1.095764 [153600/345580]\n",
      "Train loss: 1.059345 [156800/345580]\n",
      "Train loss: 1.107711 [160000/345580]\n",
      "Train loss: 1.054654 [163200/345580]\n",
      "Train loss: 1.042083 [166400/345580]\n",
      "Train loss: 1.064032 [169600/345580]\n",
      "Train loss: 1.047802 [172800/345580]\n",
      "Train loss: 1.074130 [176000/345580]\n",
      "Train loss: 1.002943 [179200/345580]\n",
      "Train loss: 1.013670 [182400/345580]\n",
      "Train loss: 1.097006 [185600/345580]\n",
      "Train loss: 1.013830 [188800/345580]\n",
      "Train loss: 0.960878 [192000/345580]\n",
      "Train loss: 0.960771 [195200/345580]\n",
      "Train loss: 1.073901 [198400/345580]\n",
      "Train loss: 1.002866 [201600/345580]\n",
      "Train loss: 1.270454 [204800/345580]\n",
      "Train loss: 1.045668 [208000/345580]\n",
      "Train loss: 1.032346 [211200/345580]\n",
      "Train loss: 1.040263 [214400/345580]\n",
      "Train loss: 1.096256 [217600/345580]\n",
      "Train loss: 0.998468 [220800/345580]\n",
      "Train loss: 1.065670 [224000/345580]\n",
      "Train loss: 1.008454 [227200/345580]\n",
      "Train loss: 1.073711 [230400/345580]\n",
      "Train loss: 1.070944 [233600/345580]\n",
      "Train loss: 1.109716 [236800/345580]\n",
      "Train loss: 1.055178 [240000/345580]\n",
      "Train loss: 1.019750 [243200/345580]\n",
      "Train loss: 1.087838 [246400/345580]\n",
      "Train loss: 1.096706 [249600/345580]\n",
      "Train loss: 1.075885 [252800/345580]\n",
      "Train loss: 1.054976 [256000/345580]\n",
      "Train loss: 1.109601 [259200/345580]\n",
      "Train loss: 1.101478 [262400/345580]\n",
      "Train loss: 1.064868 [265600/345580]\n",
      "Train loss: 1.060789 [268800/345580]\n",
      "Train loss: 1.091991 [272000/345580]\n",
      "Train loss: 1.045533 [275200/345580]\n",
      "Train loss: 1.059804 [278400/345580]\n",
      "Train loss: 1.029637 [281600/345580]\n",
      "Train loss: 1.063493 [284800/345580]\n",
      "Train loss: 1.046489 [288000/345580]\n",
      "Train loss: 1.035676 [291200/345580]\n",
      "Train loss: 1.014143 [294400/345580]\n",
      "Train loss: 1.083581 [297600/345580]\n",
      "Train loss: 1.080532 [300800/345580]\n",
      "Train loss: 1.133417 [304000/345580]\n",
      "Train loss: 1.060153 [307200/345580]\n",
      "Train loss: 0.944909 [310400/345580]\n",
      "Train loss: 1.101529 [313600/345580]\n",
      "Train loss: 1.089493 [316800/345580]\n",
      "Train loss: 1.050066 [320000/345580]\n",
      "Train loss: 1.049053 [323200/345580]\n",
      "Train loss: 1.115192 [326400/345580]\n",
      "Train loss: 1.030175 [329600/345580]\n",
      "Train loss: 1.098372 [332800/345580]\n",
      "Train loss: 1.068890 [336000/345580]\n",
      "Train loss: 1.121494 [339200/345580]\n",
      "Train loss: 1.095528 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042850 \n",
      "\n",
      "Epoch 35\n",
      " -------------------\n",
      "Train loss: 1.089161 [    0/345580]\n",
      "Train loss: 1.029923 [ 3200/345580]\n",
      "Train loss: 1.157447 [ 6400/345580]\n",
      "Train loss: 1.099737 [ 9600/345580]\n",
      "Train loss: 1.071097 [12800/345580]\n",
      "Train loss: 1.143904 [16000/345580]\n",
      "Train loss: 1.079781 [19200/345580]\n",
      "Train loss: 1.068057 [22400/345580]\n",
      "Train loss: 1.077623 [25600/345580]\n",
      "Train loss: 1.129114 [28800/345580]\n",
      "Train loss: 1.078519 [32000/345580]\n",
      "Train loss: 1.016441 [35200/345580]\n",
      "Train loss: 1.161569 [38400/345580]\n",
      "Train loss: 0.984539 [41600/345580]\n",
      "Train loss: 1.069508 [44800/345580]\n",
      "Train loss: 1.034079 [48000/345580]\n",
      "Train loss: 1.055798 [51200/345580]\n",
      "Train loss: 1.049713 [54400/345580]\n",
      "Train loss: 1.083938 [57600/345580]\n",
      "Train loss: 1.066900 [60800/345580]\n",
      "Train loss: 1.188055 [64000/345580]\n",
      "Train loss: 1.085850 [67200/345580]\n",
      "Train loss: 1.035407 [70400/345580]\n",
      "Train loss: 1.057962 [73600/345580]\n",
      "Train loss: 1.061397 [76800/345580]\n",
      "Train loss: 0.945446 [80000/345580]\n",
      "Train loss: 0.990535 [83200/345580]\n",
      "Train loss: 1.102124 [86400/345580]\n",
      "Train loss: 1.028962 [89600/345580]\n",
      "Train loss: 1.038228 [92800/345580]\n",
      "Train loss: 1.086210 [96000/345580]\n",
      "Train loss: 1.116912 [99200/345580]\n",
      "Train loss: 1.011287 [102400/345580]\n",
      "Train loss: 1.042343 [105600/345580]\n",
      "Train loss: 1.109269 [108800/345580]\n",
      "Train loss: 1.117126 [112000/345580]\n",
      "Train loss: 0.912919 [115200/345580]\n",
      "Train loss: 1.044944 [118400/345580]\n",
      "Train loss: 1.088189 [121600/345580]\n",
      "Train loss: 1.013439 [124800/345580]\n",
      "Train loss: 1.100623 [128000/345580]\n",
      "Train loss: 1.072210 [131200/345580]\n",
      "Train loss: 1.039859 [134400/345580]\n",
      "Train loss: 1.133443 [137600/345580]\n",
      "Train loss: 0.980644 [140800/345580]\n",
      "Train loss: 1.082677 [144000/345580]\n",
      "Train loss: 0.950685 [147200/345580]\n",
      "Train loss: 0.991291 [150400/345580]\n",
      "Train loss: 1.063831 [153600/345580]\n",
      "Train loss: 1.048266 [156800/345580]\n",
      "Train loss: 1.078671 [160000/345580]\n",
      "Train loss: 1.046919 [163200/345580]\n",
      "Train loss: 1.110626 [166400/345580]\n",
      "Train loss: 1.039495 [169600/345580]\n",
      "Train loss: 1.075482 [172800/345580]\n",
      "Train loss: 1.124498 [176000/345580]\n",
      "Train loss: 0.952644 [179200/345580]\n",
      "Train loss: 0.970497 [182400/345580]\n",
      "Train loss: 1.096308 [185600/345580]\n",
      "Train loss: 0.974592 [188800/345580]\n",
      "Train loss: 1.115372 [192000/345580]\n",
      "Train loss: 0.950439 [195200/345580]\n",
      "Train loss: 0.974746 [198400/345580]\n",
      "Train loss: 1.112977 [201600/345580]\n",
      "Train loss: 1.048711 [204800/345580]\n",
      "Train loss: 1.088614 [208000/345580]\n",
      "Train loss: 0.965280 [211200/345580]\n",
      "Train loss: 1.009404 [214400/345580]\n",
      "Train loss: 1.044924 [217600/345580]\n",
      "Train loss: 1.161283 [220800/345580]\n",
      "Train loss: 1.074815 [224000/345580]\n",
      "Train loss: 1.051237 [227200/345580]\n",
      "Train loss: 1.090708 [230400/345580]\n",
      "Train loss: 1.057479 [233600/345580]\n",
      "Train loss: 1.082130 [236800/345580]\n",
      "Train loss: 1.065254 [240000/345580]\n",
      "Train loss: 1.050433 [243200/345580]\n",
      "Train loss: 1.000537 [246400/345580]\n",
      "Train loss: 1.169738 [249600/345580]\n",
      "Train loss: 1.027338 [252800/345580]\n",
      "Train loss: 1.043280 [256000/345580]\n",
      "Train loss: 1.101600 [259200/345580]\n",
      "Train loss: 1.055541 [262400/345580]\n",
      "Train loss: 1.023620 [265600/345580]\n",
      "Train loss: 1.006240 [268800/345580]\n",
      "Train loss: 1.026527 [272000/345580]\n",
      "Train loss: 1.041518 [275200/345580]\n",
      "Train loss: 1.092301 [278400/345580]\n",
      "Train loss: 1.203570 [281600/345580]\n",
      "Train loss: 1.029276 [284800/345580]\n",
      "Train loss: 1.045447 [288000/345580]\n",
      "Train loss: 1.094859 [291200/345580]\n",
      "Train loss: 0.993271 [294400/345580]\n",
      "Train loss: 1.089140 [297600/345580]\n",
      "Train loss: 1.012592 [300800/345580]\n",
      "Train loss: 1.039131 [304000/345580]\n",
      "Train loss: 1.058856 [307200/345580]\n",
      "Train loss: 1.039618 [310400/345580]\n",
      "Train loss: 1.043202 [313600/345580]\n",
      "Train loss: 1.106362 [316800/345580]\n",
      "Train loss: 1.057792 [320000/345580]\n",
      "Train loss: 1.134980 [323200/345580]\n",
      "Train loss: 1.056445 [326400/345580]\n",
      "Train loss: 1.029873 [329600/345580]\n",
      "Train loss: 1.019229 [332800/345580]\n",
      "Train loss: 0.945310 [336000/345580]\n",
      "Train loss: 1.113160 [339200/345580]\n",
      "Train loss: 1.051654 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042673 \n",
      "\n",
      "Epoch 36\n",
      " -------------------\n",
      "Train loss: 1.144990 [    0/345580]\n",
      "Train loss: 1.055262 [ 3200/345580]\n",
      "Train loss: 1.090160 [ 6400/345580]\n",
      "Train loss: 1.066269 [ 9600/345580]\n",
      "Train loss: 1.008899 [12800/345580]\n",
      "Train loss: 1.008614 [16000/345580]\n",
      "Train loss: 1.143500 [19200/345580]\n",
      "Train loss: 1.013246 [22400/345580]\n",
      "Train loss: 1.080543 [25600/345580]\n",
      "Train loss: 1.004800 [28800/345580]\n",
      "Train loss: 1.057897 [32000/345580]\n",
      "Train loss: 1.009584 [35200/345580]\n",
      "Train loss: 0.985092 [38400/345580]\n",
      "Train loss: 1.008215 [41600/345580]\n",
      "Train loss: 1.086809 [44800/345580]\n",
      "Train loss: 1.003037 [48000/345580]\n",
      "Train loss: 0.949080 [51200/345580]\n",
      "Train loss: 1.098374 [54400/345580]\n",
      "Train loss: 1.063778 [57600/345580]\n",
      "Train loss: 0.975410 [60800/345580]\n",
      "Train loss: 0.994292 [64000/345580]\n",
      "Train loss: 1.050329 [67200/345580]\n",
      "Train loss: 0.925311 [70400/345580]\n",
      "Train loss: 1.004289 [73600/345580]\n",
      "Train loss: 1.113876 [76800/345580]\n",
      "Train loss: 1.009660 [80000/345580]\n",
      "Train loss: 1.112018 [83200/345580]\n",
      "Train loss: 1.070176 [86400/345580]\n",
      "Train loss: 1.021978 [89600/345580]\n",
      "Train loss: 0.971032 [92800/345580]\n",
      "Train loss: 1.061020 [96000/345580]\n",
      "Train loss: 1.146075 [99200/345580]\n",
      "Train loss: 1.023596 [102400/345580]\n",
      "Train loss: 1.053132 [105600/345580]\n",
      "Train loss: 1.020785 [108800/345580]\n",
      "Train loss: 1.035911 [112000/345580]\n",
      "Train loss: 1.108295 [115200/345580]\n",
      "Train loss: 1.011313 [118400/345580]\n",
      "Train loss: 0.987665 [121600/345580]\n",
      "Train loss: 1.020297 [124800/345580]\n",
      "Train loss: 1.094742 [128000/345580]\n",
      "Train loss: 1.110618 [131200/345580]\n",
      "Train loss: 1.052579 [134400/345580]\n",
      "Train loss: 1.027299 [137600/345580]\n",
      "Train loss: 1.017502 [140800/345580]\n",
      "Train loss: 1.011461 [144000/345580]\n",
      "Train loss: 0.990762 [147200/345580]\n",
      "Train loss: 1.086249 [150400/345580]\n",
      "Train loss: 1.057391 [153600/345580]\n",
      "Train loss: 1.194775 [156800/345580]\n",
      "Train loss: 0.990605 [160000/345580]\n",
      "Train loss: 0.956500 [163200/345580]\n",
      "Train loss: 1.006179 [166400/345580]\n",
      "Train loss: 1.119632 [169600/345580]\n",
      "Train loss: 1.056251 [172800/345580]\n",
      "Train loss: 1.072819 [176000/345580]\n",
      "Train loss: 0.999172 [179200/345580]\n",
      "Train loss: 0.957627 [182400/345580]\n",
      "Train loss: 1.100965 [185600/345580]\n",
      "Train loss: 1.019163 [188800/345580]\n",
      "Train loss: 0.999780 [192000/345580]\n",
      "Train loss: 1.114001 [195200/345580]\n",
      "Train loss: 1.045655 [198400/345580]\n",
      "Train loss: 1.015023 [201600/345580]\n",
      "Train loss: 1.018216 [204800/345580]\n",
      "Train loss: 1.019238 [208000/345580]\n",
      "Train loss: 1.053927 [211200/345580]\n",
      "Train loss: 1.012335 [214400/345580]\n",
      "Train loss: 0.989749 [217600/345580]\n",
      "Train loss: 1.061597 [220800/345580]\n",
      "Train loss: 0.982360 [224000/345580]\n",
      "Train loss: 0.999039 [227200/345580]\n",
      "Train loss: 1.016645 [230400/345580]\n",
      "Train loss: 1.097219 [233600/345580]\n",
      "Train loss: 0.975761 [236800/345580]\n",
      "Train loss: 1.039639 [240000/345580]\n",
      "Train loss: 1.043939 [243200/345580]\n",
      "Train loss: 1.103770 [246400/345580]\n",
      "Train loss: 1.012694 [249600/345580]\n",
      "Train loss: 1.062846 [252800/345580]\n",
      "Train loss: 1.104656 [256000/345580]\n",
      "Train loss: 1.053061 [259200/345580]\n",
      "Train loss: 0.924018 [262400/345580]\n",
      "Train loss: 1.034028 [265600/345580]\n",
      "Train loss: 1.018808 [268800/345580]\n",
      "Train loss: 1.015666 [272000/345580]\n",
      "Train loss: 0.960273 [275200/345580]\n",
      "Train loss: 1.070288 [278400/345580]\n",
      "Train loss: 1.017605 [281600/345580]\n",
      "Train loss: 1.118446 [284800/345580]\n",
      "Train loss: 0.993665 [288000/345580]\n",
      "Train loss: 1.013397 [291200/345580]\n",
      "Train loss: 1.092541 [294400/345580]\n",
      "Train loss: 1.156818 [297600/345580]\n",
      "Train loss: 1.044513 [300800/345580]\n",
      "Train loss: 1.088347 [304000/345580]\n",
      "Train loss: 1.032367 [307200/345580]\n",
      "Train loss: 1.018945 [310400/345580]\n",
      "Train loss: 1.015940 [313600/345580]\n",
      "Train loss: 0.997889 [316800/345580]\n",
      "Train loss: 1.042854 [320000/345580]\n",
      "Train loss: 1.032820 [323200/345580]\n",
      "Train loss: 1.058646 [326400/345580]\n",
      "Train loss: 0.989258 [329600/345580]\n",
      "Train loss: 1.065363 [332800/345580]\n",
      "Train loss: 1.027595 [336000/345580]\n",
      "Train loss: 0.979102 [339200/345580]\n",
      "Train loss: 0.991370 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.3%, Avg loss: 1.041300 \n",
      "\n",
      "Epoch 37\n",
      " -------------------\n",
      "Train loss: 1.133687 [    0/345580]\n",
      "Train loss: 1.003354 [ 3200/345580]\n",
      "Train loss: 1.067950 [ 6400/345580]\n",
      "Train loss: 1.035023 [ 9600/345580]\n",
      "Train loss: 0.916732 [12800/345580]\n",
      "Train loss: 1.060967 [16000/345580]\n",
      "Train loss: 1.015145 [19200/345580]\n",
      "Train loss: 1.041302 [22400/345580]\n",
      "Train loss: 1.080811 [25600/345580]\n",
      "Train loss: 1.014350 [28800/345580]\n",
      "Train loss: 1.006964 [32000/345580]\n",
      "Train loss: 1.111669 [35200/345580]\n",
      "Train loss: 1.026110 [38400/345580]\n",
      "Train loss: 1.097990 [41600/345580]\n",
      "Train loss: 1.081509 [44800/345580]\n",
      "Train loss: 1.003525 [48000/345580]\n",
      "Train loss: 1.069784 [51200/345580]\n",
      "Train loss: 1.086957 [54400/345580]\n",
      "Train loss: 1.043258 [57600/345580]\n",
      "Train loss: 1.070606 [60800/345580]\n",
      "Train loss: 1.031695 [64000/345580]\n",
      "Train loss: 1.093341 [67200/345580]\n",
      "Train loss: 1.033369 [70400/345580]\n",
      "Train loss: 1.034143 [73600/345580]\n",
      "Train loss: 1.083947 [76800/345580]\n",
      "Train loss: 1.079182 [80000/345580]\n",
      "Train loss: 1.064667 [83200/345580]\n",
      "Train loss: 1.021356 [86400/345580]\n",
      "Train loss: 1.062966 [89600/345580]\n",
      "Train loss: 1.096839 [92800/345580]\n",
      "Train loss: 1.133637 [96000/345580]\n",
      "Train loss: 0.995399 [99200/345580]\n",
      "Train loss: 1.040992 [102400/345580]\n",
      "Train loss: 0.964781 [105600/345580]\n",
      "Train loss: 0.996578 [108800/345580]\n",
      "Train loss: 1.038861 [112000/345580]\n",
      "Train loss: 0.988263 [115200/345580]\n",
      "Train loss: 0.896933 [118400/345580]\n",
      "Train loss: 1.040821 [121600/345580]\n",
      "Train loss: 1.026951 [124800/345580]\n",
      "Train loss: 1.088978 [128000/345580]\n",
      "Train loss: 1.116948 [131200/345580]\n",
      "Train loss: 1.018616 [134400/345580]\n",
      "Train loss: 1.100683 [137600/345580]\n",
      "Train loss: 1.101156 [140800/345580]\n",
      "Train loss: 1.078645 [144000/345580]\n",
      "Train loss: 1.087612 [147200/345580]\n",
      "Train loss: 1.020639 [150400/345580]\n",
      "Train loss: 1.080678 [153600/345580]\n",
      "Train loss: 1.054955 [156800/345580]\n",
      "Train loss: 1.077404 [160000/345580]\n",
      "Train loss: 1.104480 [163200/345580]\n",
      "Train loss: 1.077338 [166400/345580]\n",
      "Train loss: 1.041221 [169600/345580]\n",
      "Train loss: 1.007050 [172800/345580]\n",
      "Train loss: 1.107989 [176000/345580]\n",
      "Train loss: 1.007663 [179200/345580]\n",
      "Train loss: 1.027322 [182400/345580]\n",
      "Train loss: 1.057037 [185600/345580]\n",
      "Train loss: 1.042025 [188800/345580]\n",
      "Train loss: 1.170999 [192000/345580]\n",
      "Train loss: 1.027677 [195200/345580]\n",
      "Train loss: 1.178505 [198400/345580]\n",
      "Train loss: 1.022436 [201600/345580]\n",
      "Train loss: 1.081068 [204800/345580]\n",
      "Train loss: 0.955844 [208000/345580]\n",
      "Train loss: 1.126091 [211200/345580]\n",
      "Train loss: 1.086387 [214400/345580]\n",
      "Train loss: 1.079735 [217600/345580]\n",
      "Train loss: 1.127819 [220800/345580]\n",
      "Train loss: 1.003854 [224000/345580]\n",
      "Train loss: 1.117371 [227200/345580]\n",
      "Train loss: 1.047324 [230400/345580]\n",
      "Train loss: 1.096002 [233600/345580]\n",
      "Train loss: 1.113214 [236800/345580]\n",
      "Train loss: 1.065518 [240000/345580]\n",
      "Train loss: 1.109981 [243200/345580]\n",
      "Train loss: 1.137719 [246400/345580]\n",
      "Train loss: 1.037577 [249600/345580]\n",
      "Train loss: 0.978459 [252800/345580]\n",
      "Train loss: 1.029405 [256000/345580]\n",
      "Train loss: 1.013251 [259200/345580]\n",
      "Train loss: 1.098561 [262400/345580]\n",
      "Train loss: 1.093890 [265600/345580]\n",
      "Train loss: 1.003801 [268800/345580]\n",
      "Train loss: 0.965429 [272000/345580]\n",
      "Train loss: 0.976332 [275200/345580]\n",
      "Train loss: 1.111242 [278400/345580]\n",
      "Train loss: 1.023729 [281600/345580]\n",
      "Train loss: 0.976791 [284800/345580]\n",
      "Train loss: 1.076837 [288000/345580]\n",
      "Train loss: 1.008936 [291200/345580]\n",
      "Train loss: 1.136591 [294400/345580]\n",
      "Train loss: 1.024367 [297600/345580]\n",
      "Train loss: 1.009718 [300800/345580]\n",
      "Train loss: 1.062405 [304000/345580]\n",
      "Train loss: 1.064362 [307200/345580]\n",
      "Train loss: 1.019621 [310400/345580]\n",
      "Train loss: 1.059779 [313600/345580]\n",
      "Train loss: 1.078263 [316800/345580]\n",
      "Train loss: 1.041927 [320000/345580]\n",
      "Train loss: 1.080592 [323200/345580]\n",
      "Train loss: 1.063525 [326400/345580]\n",
      "Train loss: 1.000199 [329600/345580]\n",
      "Train loss: 1.003818 [332800/345580]\n",
      "Train loss: 1.093704 [336000/345580]\n",
      "Train loss: 1.093057 [339200/345580]\n",
      "Train loss: 1.164282 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042176 \n",
      "\n",
      "Epoch 38\n",
      " -------------------\n",
      "Train loss: 1.018960 [    0/345580]\n",
      "Train loss: 1.056723 [ 3200/345580]\n",
      "Train loss: 1.142706 [ 6400/345580]\n",
      "Train loss: 1.089558 [ 9600/345580]\n",
      "Train loss: 1.003069 [12800/345580]\n",
      "Train loss: 1.101945 [16000/345580]\n",
      "Train loss: 1.010468 [19200/345580]\n",
      "Train loss: 1.033505 [22400/345580]\n",
      "Train loss: 0.992704 [25600/345580]\n",
      "Train loss: 1.045674 [28800/345580]\n",
      "Train loss: 1.019575 [32000/345580]\n",
      "Train loss: 1.057421 [35200/345580]\n",
      "Train loss: 1.009638 [38400/345580]\n",
      "Train loss: 1.038108 [41600/345580]\n",
      "Train loss: 1.005439 [44800/345580]\n",
      "Train loss: 1.079532 [48000/345580]\n",
      "Train loss: 0.941111 [51200/345580]\n",
      "Train loss: 1.068593 [54400/345580]\n",
      "Train loss: 1.123006 [57600/345580]\n",
      "Train loss: 0.973473 [60800/345580]\n",
      "Train loss: 0.979603 [64000/345580]\n",
      "Train loss: 0.982025 [67200/345580]\n",
      "Train loss: 1.074832 [70400/345580]\n",
      "Train loss: 1.111397 [73600/345580]\n",
      "Train loss: 1.098281 [76800/345580]\n",
      "Train loss: 1.040185 [80000/345580]\n",
      "Train loss: 1.084536 [83200/345580]\n",
      "Train loss: 1.006471 [86400/345580]\n",
      "Train loss: 1.110446 [89600/345580]\n",
      "Train loss: 1.032982 [92800/345580]\n",
      "Train loss: 1.098798 [96000/345580]\n",
      "Train loss: 0.998134 [99200/345580]\n",
      "Train loss: 0.995229 [102400/345580]\n",
      "Train loss: 1.099832 [105600/345580]\n",
      "Train loss: 1.109628 [108800/345580]\n",
      "Train loss: 1.081243 [112000/345580]\n",
      "Train loss: 0.988230 [115200/345580]\n",
      "Train loss: 1.021752 [118400/345580]\n",
      "Train loss: 1.039135 [121600/345580]\n",
      "Train loss: 1.014108 [124800/345580]\n",
      "Train loss: 1.079833 [128000/345580]\n",
      "Train loss: 1.042088 [131200/345580]\n",
      "Train loss: 0.963459 [134400/345580]\n",
      "Train loss: 1.084948 [137600/345580]\n",
      "Train loss: 1.116847 [140800/345580]\n",
      "Train loss: 0.985327 [144000/345580]\n",
      "Train loss: 1.098152 [147200/345580]\n",
      "Train loss: 1.091191 [150400/345580]\n",
      "Train loss: 1.097819 [153600/345580]\n",
      "Train loss: 1.129696 [156800/345580]\n",
      "Train loss: 0.950322 [160000/345580]\n",
      "Train loss: 1.106950 [163200/345580]\n",
      "Train loss: 1.102239 [166400/345580]\n",
      "Train loss: 1.120260 [169600/345580]\n",
      "Train loss: 1.124098 [172800/345580]\n",
      "Train loss: 1.108942 [176000/345580]\n",
      "Train loss: 1.064962 [179200/345580]\n",
      "Train loss: 1.073602 [182400/345580]\n",
      "Train loss: 0.967992 [185600/345580]\n",
      "Train loss: 1.011987 [188800/345580]\n",
      "Train loss: 1.073032 [192000/345580]\n",
      "Train loss: 1.170896 [195200/345580]\n",
      "Train loss: 1.060861 [198400/345580]\n",
      "Train loss: 1.034092 [201600/345580]\n",
      "Train loss: 1.090593 [204800/345580]\n",
      "Train loss: 1.115947 [208000/345580]\n",
      "Train loss: 1.072027 [211200/345580]\n",
      "Train loss: 1.069605 [214400/345580]\n",
      "Train loss: 1.075643 [217600/345580]\n",
      "Train loss: 1.015437 [220800/345580]\n",
      "Train loss: 1.103107 [224000/345580]\n",
      "Train loss: 1.074639 [227200/345580]\n",
      "Train loss: 1.007552 [230400/345580]\n",
      "Train loss: 1.128639 [233600/345580]\n",
      "Train loss: 0.980675 [236800/345580]\n",
      "Train loss: 1.012936 [240000/345580]\n",
      "Train loss: 1.022171 [243200/345580]\n",
      "Train loss: 1.000690 [246400/345580]\n",
      "Train loss: 1.063870 [249600/345580]\n",
      "Train loss: 1.123960 [252800/345580]\n",
      "Train loss: 0.935282 [256000/345580]\n",
      "Train loss: 1.079752 [259200/345580]\n",
      "Train loss: 0.990069 [262400/345580]\n",
      "Train loss: 1.069999 [265600/345580]\n",
      "Train loss: 1.082709 [268800/345580]\n",
      "Train loss: 1.023680 [272000/345580]\n",
      "Train loss: 1.004014 [275200/345580]\n",
      "Train loss: 1.094138 [278400/345580]\n",
      "Train loss: 1.161753 [281600/345580]\n",
      "Train loss: 1.099740 [284800/345580]\n",
      "Train loss: 1.108533 [288000/345580]\n",
      "Train loss: 1.065907 [291200/345580]\n",
      "Train loss: 1.167133 [294400/345580]\n",
      "Train loss: 1.067848 [297600/345580]\n",
      "Train loss: 1.071969 [300800/345580]\n",
      "Train loss: 0.997411 [304000/345580]\n",
      "Train loss: 0.995410 [307200/345580]\n",
      "Train loss: 1.004243 [310400/345580]\n",
      "Train loss: 1.066215 [313600/345580]\n",
      "Train loss: 0.956133 [316800/345580]\n",
      "Train loss: 1.080543 [320000/345580]\n",
      "Train loss: 1.066684 [323200/345580]\n",
      "Train loss: 0.989033 [326400/345580]\n",
      "Train loss: 1.054185 [329600/345580]\n",
      "Train loss: 1.049281 [332800/345580]\n",
      "Train loss: 1.148729 [336000/345580]\n",
      "Train loss: 1.008153 [339200/345580]\n",
      "Train loss: 1.001573 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.042824 \n",
      "\n",
      "Epoch 39\n",
      " -------------------\n",
      "Train loss: 1.085426 [    0/345580]\n",
      "Train loss: 1.034532 [ 3200/345580]\n",
      "Train loss: 1.077135 [ 6400/345580]\n",
      "Train loss: 1.051687 [ 9600/345580]\n",
      "Train loss: 1.042687 [12800/345580]\n",
      "Train loss: 1.068669 [16000/345580]\n",
      "Train loss: 1.018278 [19200/345580]\n",
      "Train loss: 0.888118 [22400/345580]\n",
      "Train loss: 1.045469 [25600/345580]\n",
      "Train loss: 1.079159 [28800/345580]\n",
      "Train loss: 0.953160 [32000/345580]\n",
      "Train loss: 1.052105 [35200/345580]\n",
      "Train loss: 1.056278 [38400/345580]\n",
      "Train loss: 1.122441 [41600/345580]\n",
      "Train loss: 0.966577 [44800/345580]\n",
      "Train loss: 1.058115 [48000/345580]\n",
      "Train loss: 1.017646 [51200/345580]\n",
      "Train loss: 1.073275 [54400/345580]\n",
      "Train loss: 1.033190 [57600/345580]\n",
      "Train loss: 1.070594 [60800/345580]\n",
      "Train loss: 0.946882 [64000/345580]\n",
      "Train loss: 1.051690 [67200/345580]\n",
      "Train loss: 1.015851 [70400/345580]\n",
      "Train loss: 1.082222 [73600/345580]\n",
      "Train loss: 0.998373 [76800/345580]\n",
      "Train loss: 1.089462 [80000/345580]\n",
      "Train loss: 0.991100 [83200/345580]\n",
      "Train loss: 0.974292 [86400/345580]\n",
      "Train loss: 1.052534 [89600/345580]\n",
      "Train loss: 1.016336 [92800/345580]\n",
      "Train loss: 1.059907 [96000/345580]\n",
      "Train loss: 1.037670 [99200/345580]\n",
      "Train loss: 1.046980 [102400/345580]\n",
      "Train loss: 0.996469 [105600/345580]\n",
      "Train loss: 1.109412 [108800/345580]\n",
      "Train loss: 1.011252 [112000/345580]\n",
      "Train loss: 1.250896 [115200/345580]\n",
      "Train loss: 1.009161 [118400/345580]\n",
      "Train loss: 1.033969 [121600/345580]\n",
      "Train loss: 1.071913 [124800/345580]\n",
      "Train loss: 1.057069 [128000/345580]\n",
      "Train loss: 1.061989 [131200/345580]\n",
      "Train loss: 1.108469 [134400/345580]\n",
      "Train loss: 1.032457 [137600/345580]\n",
      "Train loss: 1.162416 [140800/345580]\n",
      "Train loss: 1.035445 [144000/345580]\n",
      "Train loss: 1.055634 [147200/345580]\n",
      "Train loss: 1.045861 [150400/345580]\n",
      "Train loss: 1.025250 [153600/345580]\n",
      "Train loss: 0.972580 [156800/345580]\n",
      "Train loss: 1.014100 [160000/345580]\n",
      "Train loss: 1.083964 [163200/345580]\n",
      "Train loss: 1.042508 [166400/345580]\n",
      "Train loss: 0.954491 [169600/345580]\n",
      "Train loss: 0.981005 [172800/345580]\n",
      "Train loss: 1.091987 [176000/345580]\n",
      "Train loss: 0.985782 [179200/345580]\n",
      "Train loss: 1.047165 [182400/345580]\n",
      "Train loss: 0.996107 [185600/345580]\n",
      "Train loss: 1.082912 [188800/345580]\n",
      "Train loss: 1.071667 [192000/345580]\n",
      "Train loss: 1.139137 [195200/345580]\n",
      "Train loss: 1.015142 [198400/345580]\n",
      "Train loss: 0.947520 [201600/345580]\n",
      "Train loss: 0.956548 [204800/345580]\n",
      "Train loss: 1.125599 [208000/345580]\n",
      "Train loss: 0.988871 [211200/345580]\n",
      "Train loss: 1.011398 [214400/345580]\n",
      "Train loss: 0.986204 [217600/345580]\n",
      "Train loss: 1.029414 [220800/345580]\n",
      "Train loss: 0.925677 [224000/345580]\n",
      "Train loss: 0.998830 [227200/345580]\n",
      "Train loss: 1.049668 [230400/345580]\n",
      "Train loss: 0.990268 [233600/345580]\n",
      "Train loss: 1.038994 [236800/345580]\n",
      "Train loss: 1.083137 [240000/345580]\n",
      "Train loss: 0.991395 [243200/345580]\n",
      "Train loss: 1.137943 [246400/345580]\n",
      "Train loss: 0.932121 [249600/345580]\n",
      "Train loss: 1.086097 [252800/345580]\n",
      "Train loss: 1.097155 [256000/345580]\n",
      "Train loss: 1.049556 [259200/345580]\n",
      "Train loss: 1.045009 [262400/345580]\n",
      "Train loss: 0.996152 [265600/345580]\n",
      "Train loss: 1.145185 [268800/345580]\n",
      "Train loss: 1.009742 [272000/345580]\n",
      "Train loss: 1.039893 [275200/345580]\n",
      "Train loss: 1.052672 [278400/345580]\n",
      "Train loss: 0.985024 [281600/345580]\n",
      "Train loss: 1.002978 [284800/345580]\n",
      "Train loss: 0.997486 [288000/345580]\n",
      "Train loss: 1.074604 [291200/345580]\n",
      "Train loss: 1.092565 [294400/345580]\n",
      "Train loss: 1.054457 [297600/345580]\n",
      "Train loss: 0.953160 [300800/345580]\n",
      "Train loss: 1.014251 [304000/345580]\n",
      "Train loss: 1.145530 [307200/345580]\n",
      "Train loss: 1.049293 [310400/345580]\n",
      "Train loss: 0.995035 [313600/345580]\n",
      "Train loss: 1.019091 [316800/345580]\n",
      "Train loss: 1.056669 [320000/345580]\n",
      "Train loss: 1.058345 [323200/345580]\n",
      "Train loss: 1.050802 [326400/345580]\n",
      "Train loss: 1.001978 [329600/345580]\n",
      "Train loss: 0.954351 [332800/345580]\n",
      "Train loss: 0.950499 [336000/345580]\n",
      "Train loss: 0.885960 [339200/345580]\n",
      "Train loss: 1.057648 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 42.8%, Avg loss: 1.041980 \n",
      "\n",
      "Epoch 40\n",
      " -------------------\n",
      "Train loss: 1.094245 [    0/345580]\n",
      "Train loss: 0.978569 [ 3200/345580]\n",
      "Train loss: 1.050563 [ 6400/345580]\n",
      "Train loss: 1.137930 [ 9600/345580]\n",
      "Train loss: 1.089866 [12800/345580]\n",
      "Train loss: 1.019970 [16000/345580]\n",
      "Train loss: 0.984945 [19200/345580]\n",
      "Train loss: 1.078695 [22400/345580]\n",
      "Train loss: 1.064261 [25600/345580]\n",
      "Train loss: 1.103140 [28800/345580]\n",
      "Train loss: 1.066158 [32000/345580]\n",
      "Train loss: 1.026659 [35200/345580]\n",
      "Train loss: 1.148673 [38400/345580]\n",
      "Train loss: 1.029842 [41600/345580]\n",
      "Train loss: 1.000136 [44800/345580]\n",
      "Train loss: 1.039252 [48000/345580]\n",
      "Train loss: 1.045224 [51200/345580]\n",
      "Train loss: 1.028716 [54400/345580]\n",
      "Train loss: 1.026483 [57600/345580]\n",
      "Train loss: 1.030440 [60800/345580]\n",
      "Train loss: 1.072879 [64000/345580]\n",
      "Train loss: 1.132746 [67200/345580]\n",
      "Train loss: 1.013954 [70400/345580]\n",
      "Train loss: 1.081753 [73600/345580]\n",
      "Train loss: 1.046935 [76800/345580]\n",
      "Train loss: 1.027417 [80000/345580]\n",
      "Train loss: 0.976384 [83200/345580]\n",
      "Train loss: 1.032842 [86400/345580]\n",
      "Train loss: 1.005154 [89600/345580]\n",
      "Train loss: 1.048185 [92800/345580]\n",
      "Train loss: 1.037098 [96000/345580]\n",
      "Train loss: 1.110157 [99200/345580]\n",
      "Train loss: 1.035358 [102400/345580]\n",
      "Train loss: 1.074329 [105600/345580]\n",
      "Train loss: 0.999341 [108800/345580]\n",
      "Train loss: 1.082695 [112000/345580]\n",
      "Train loss: 0.985972 [115200/345580]\n",
      "Train loss: 1.053530 [118400/345580]\n",
      "Train loss: 1.030466 [121600/345580]\n",
      "Train loss: 1.023851 [124800/345580]\n",
      "Train loss: 1.095566 [128000/345580]\n",
      "Train loss: 1.006106 [131200/345580]\n",
      "Train loss: 1.067095 [134400/345580]\n",
      "Train loss: 1.030084 [137600/345580]\n",
      "Train loss: 1.091250 [140800/345580]\n",
      "Train loss: 1.017679 [144000/345580]\n",
      "Train loss: 1.053874 [147200/345580]\n",
      "Train loss: 0.931028 [150400/345580]\n",
      "Train loss: 1.148258 [153600/345580]\n",
      "Train loss: 0.987413 [156800/345580]\n",
      "Train loss: 1.074178 [160000/345580]\n",
      "Train loss: 1.058514 [163200/345580]\n",
      "Train loss: 1.086077 [166400/345580]\n",
      "Train loss: 1.059382 [169600/345580]\n",
      "Train loss: 1.011346 [172800/345580]\n",
      "Train loss: 1.036498 [176000/345580]\n",
      "Train loss: 1.054083 [179200/345580]\n",
      "Train loss: 0.968471 [182400/345580]\n",
      "Train loss: 0.981329 [185600/345580]\n",
      "Train loss: 1.004340 [188800/345580]\n",
      "Train loss: 1.079519 [192000/345580]\n",
      "Train loss: 1.033553 [195200/345580]\n",
      "Train loss: 0.985674 [198400/345580]\n",
      "Train loss: 1.082641 [201600/345580]\n",
      "Train loss: 1.100755 [204800/345580]\n",
      "Train loss: 1.061756 [208000/345580]\n",
      "Train loss: 1.081744 [211200/345580]\n",
      "Train loss: 1.065036 [214400/345580]\n",
      "Train loss: 1.001911 [217600/345580]\n",
      "Train loss: 1.005800 [220800/345580]\n",
      "Train loss: 1.161503 [224000/345580]\n",
      "Train loss: 1.001803 [227200/345580]\n",
      "Train loss: 1.102487 [230400/345580]\n",
      "Train loss: 1.067178 [233600/345580]\n",
      "Train loss: 0.976914 [236800/345580]\n",
      "Train loss: 1.031892 [240000/345580]\n",
      "Train loss: 1.020437 [243200/345580]\n",
      "Train loss: 1.032483 [246400/345580]\n",
      "Train loss: 0.918836 [249600/345580]\n",
      "Train loss: 1.083702 [252800/345580]\n",
      "Train loss: 1.137340 [256000/345580]\n",
      "Train loss: 1.115768 [259200/345580]\n",
      "Train loss: 1.052040 [262400/345580]\n",
      "Train loss: 1.008029 [265600/345580]\n",
      "Train loss: 1.043175 [268800/345580]\n",
      "Train loss: 1.067256 [272000/345580]\n",
      "Train loss: 1.064804 [275200/345580]\n",
      "Train loss: 0.992863 [278400/345580]\n",
      "Train loss: 1.023353 [281600/345580]\n",
      "Train loss: 1.081853 [284800/345580]\n",
      "Train loss: 1.088524 [288000/345580]\n",
      "Train loss: 1.093470 [291200/345580]\n",
      "Train loss: 1.077151 [294400/345580]\n",
      "Train loss: 0.936714 [297600/345580]\n",
      "Train loss: 1.091793 [300800/345580]\n",
      "Train loss: 1.036604 [304000/345580]\n",
      "Train loss: 1.015792 [307200/345580]\n",
      "Train loss: 1.110612 [310400/345580]\n",
      "Train loss: 1.139063 [313600/345580]\n",
      "Train loss: 1.039668 [316800/345580]\n",
      "Train loss: 1.121406 [320000/345580]\n",
      "Train loss: 0.933592 [323200/345580]\n",
      "Train loss: 1.104413 [326400/345580]\n",
      "Train loss: 0.958790 [329600/345580]\n",
      "Train loss: 1.045544 [332800/345580]\n",
      "Train loss: 1.038527 [336000/345580]\n",
      "Train loss: 1.046301 [339200/345580]\n",
      "Train loss: 0.988041 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.041625 \n",
      "\n",
      "Epoch 41\n",
      " -------------------\n",
      "Train loss: 1.019638 [    0/345580]\n",
      "Train loss: 0.957534 [ 3200/345580]\n",
      "Train loss: 1.010747 [ 6400/345580]\n",
      "Train loss: 0.971057 [ 9600/345580]\n",
      "Train loss: 0.981969 [12800/345580]\n",
      "Train loss: 1.015927 [16000/345580]\n",
      "Train loss: 1.021084 [19200/345580]\n",
      "Train loss: 1.018356 [22400/345580]\n",
      "Train loss: 1.112294 [25600/345580]\n",
      "Train loss: 1.114057 [28800/345580]\n",
      "Train loss: 0.950743 [32000/345580]\n",
      "Train loss: 1.019479 [35200/345580]\n",
      "Train loss: 1.020176 [38400/345580]\n",
      "Train loss: 0.947227 [41600/345580]\n",
      "Train loss: 1.052406 [44800/345580]\n",
      "Train loss: 1.079385 [48000/345580]\n",
      "Train loss: 1.022376 [51200/345580]\n",
      "Train loss: 1.128471 [54400/345580]\n",
      "Train loss: 0.974678 [57600/345580]\n",
      "Train loss: 1.033682 [60800/345580]\n",
      "Train loss: 0.995524 [64000/345580]\n",
      "Train loss: 1.062380 [67200/345580]\n",
      "Train loss: 1.048650 [70400/345580]\n",
      "Train loss: 1.004498 [73600/345580]\n",
      "Train loss: 1.103458 [76800/345580]\n",
      "Train loss: 1.104018 [80000/345580]\n",
      "Train loss: 1.024832 [83200/345580]\n",
      "Train loss: 1.118700 [86400/345580]\n",
      "Train loss: 1.017727 [89600/345580]\n",
      "Train loss: 1.025165 [92800/345580]\n",
      "Train loss: 1.047938 [96000/345580]\n",
      "Train loss: 1.014239 [99200/345580]\n",
      "Train loss: 1.150823 [102400/345580]\n",
      "Train loss: 1.079819 [105600/345580]\n",
      "Train loss: 1.019697 [108800/345580]\n",
      "Train loss: 1.112513 [112000/345580]\n",
      "Train loss: 0.936907 [115200/345580]\n",
      "Train loss: 1.105164 [118400/345580]\n",
      "Train loss: 1.018810 [121600/345580]\n",
      "Train loss: 1.060871 [124800/345580]\n",
      "Train loss: 1.073008 [128000/345580]\n",
      "Train loss: 1.073738 [131200/345580]\n",
      "Train loss: 1.035028 [134400/345580]\n",
      "Train loss: 0.927320 [137600/345580]\n",
      "Train loss: 1.094185 [140800/345580]\n",
      "Train loss: 1.063993 [144000/345580]\n",
      "Train loss: 0.986122 [147200/345580]\n",
      "Train loss: 0.953688 [150400/345580]\n",
      "Train loss: 1.032405 [153600/345580]\n",
      "Train loss: 1.089375 [156800/345580]\n",
      "Train loss: 1.116941 [160000/345580]\n",
      "Train loss: 1.071818 [163200/345580]\n",
      "Train loss: 0.910612 [166400/345580]\n",
      "Train loss: 1.063823 [169600/345580]\n",
      "Train loss: 1.105500 [172800/345580]\n",
      "Train loss: 1.013902 [176000/345580]\n",
      "Train loss: 0.985067 [179200/345580]\n",
      "Train loss: 1.062097 [182400/345580]\n",
      "Train loss: 0.986449 [185600/345580]\n",
      "Train loss: 1.170508 [188800/345580]\n",
      "Train loss: 1.055437 [192000/345580]\n",
      "Train loss: 1.046936 [195200/345580]\n",
      "Train loss: 0.996457 [198400/345580]\n",
      "Train loss: 1.051950 [201600/345580]\n",
      "Train loss: 1.121983 [204800/345580]\n",
      "Train loss: 1.098593 [208000/345580]\n",
      "Train loss: 0.970502 [211200/345580]\n",
      "Train loss: 1.149431 [214400/345580]\n",
      "Train loss: 1.070838 [217600/345580]\n",
      "Train loss: 1.064433 [220800/345580]\n",
      "Train loss: 0.976644 [224000/345580]\n",
      "Train loss: 1.056966 [227200/345580]\n",
      "Train loss: 1.048746 [230400/345580]\n",
      "Train loss: 1.064631 [233600/345580]\n",
      "Train loss: 1.113957 [236800/345580]\n",
      "Train loss: 1.063935 [240000/345580]\n",
      "Train loss: 1.080992 [243200/345580]\n",
      "Train loss: 0.986210 [246400/345580]\n",
      "Train loss: 1.020002 [249600/345580]\n",
      "Train loss: 1.054965 [252800/345580]\n",
      "Train loss: 1.007332 [256000/345580]\n",
      "Train loss: 1.018749 [259200/345580]\n",
      "Train loss: 1.055173 [262400/345580]\n",
      "Train loss: 1.026697 [265600/345580]\n",
      "Train loss: 1.001113 [268800/345580]\n",
      "Train loss: 1.180141 [272000/345580]\n",
      "Train loss: 1.203480 [275200/345580]\n",
      "Train loss: 1.067728 [278400/345580]\n",
      "Train loss: 0.954154 [281600/345580]\n",
      "Train loss: 1.017706 [284800/345580]\n",
      "Train loss: 1.135216 [288000/345580]\n",
      "Train loss: 1.023348 [291200/345580]\n",
      "Train loss: 1.077988 [294400/345580]\n",
      "Train loss: 1.162910 [297600/345580]\n",
      "Train loss: 0.945922 [300800/345580]\n",
      "Train loss: 1.056875 [304000/345580]\n",
      "Train loss: 1.040268 [307200/345580]\n",
      "Train loss: 1.179029 [310400/345580]\n",
      "Train loss: 0.975985 [313600/345580]\n",
      "Train loss: 1.046622 [316800/345580]\n",
      "Train loss: 1.043057 [320000/345580]\n",
      "Train loss: 0.981826 [323200/345580]\n",
      "Train loss: 1.022740 [326400/345580]\n",
      "Train loss: 1.105178 [329600/345580]\n",
      "Train loss: 1.103400 [332800/345580]\n",
      "Train loss: 1.094436 [336000/345580]\n",
      "Train loss: 0.998803 [339200/345580]\n",
      "Train loss: 1.046330 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.042277 \n",
      "\n",
      "Epoch 42\n",
      " -------------------\n",
      "Train loss: 1.027672 [    0/345580]\n",
      "Train loss: 1.140367 [ 3200/345580]\n",
      "Train loss: 0.890141 [ 6400/345580]\n",
      "Train loss: 0.950540 [ 9600/345580]\n",
      "Train loss: 1.011595 [12800/345580]\n",
      "Train loss: 1.043258 [16000/345580]\n",
      "Train loss: 1.102710 [19200/345580]\n",
      "Train loss: 1.101335 [22400/345580]\n",
      "Train loss: 1.107057 [25600/345580]\n",
      "Train loss: 1.104906 [28800/345580]\n",
      "Train loss: 1.079514 [32000/345580]\n",
      "Train loss: 1.058450 [35200/345580]\n",
      "Train loss: 1.094382 [38400/345580]\n",
      "Train loss: 1.014355 [41600/345580]\n",
      "Train loss: 1.072083 [44800/345580]\n",
      "Train loss: 1.097555 [48000/345580]\n",
      "Train loss: 1.030090 [51200/345580]\n",
      "Train loss: 0.980026 [54400/345580]\n",
      "Train loss: 1.073077 [57600/345580]\n",
      "Train loss: 0.996077 [60800/345580]\n",
      "Train loss: 1.095892 [64000/345580]\n",
      "Train loss: 1.119528 [67200/345580]\n",
      "Train loss: 1.062408 [70400/345580]\n",
      "Train loss: 1.038511 [73600/345580]\n",
      "Train loss: 1.025885 [76800/345580]\n",
      "Train loss: 1.013629 [80000/345580]\n",
      "Train loss: 1.049796 [83200/345580]\n",
      "Train loss: 1.057656 [86400/345580]\n",
      "Train loss: 0.959617 [89600/345580]\n",
      "Train loss: 0.999157 [92800/345580]\n",
      "Train loss: 0.962311 [96000/345580]\n",
      "Train loss: 0.957679 [99200/345580]\n",
      "Train loss: 1.043333 [102400/345580]\n",
      "Train loss: 1.064784 [105600/345580]\n",
      "Train loss: 1.074713 [108800/345580]\n",
      "Train loss: 1.062856 [112000/345580]\n",
      "Train loss: 1.048596 [115200/345580]\n",
      "Train loss: 1.121516 [118400/345580]\n",
      "Train loss: 1.060328 [121600/345580]\n",
      "Train loss: 1.077826 [124800/345580]\n",
      "Train loss: 1.069149 [128000/345580]\n",
      "Train loss: 0.987615 [131200/345580]\n",
      "Train loss: 1.088617 [134400/345580]\n",
      "Train loss: 1.061439 [137600/345580]\n",
      "Train loss: 1.029760 [140800/345580]\n",
      "Train loss: 1.201570 [144000/345580]\n",
      "Train loss: 1.131910 [147200/345580]\n",
      "Train loss: 1.009043 [150400/345580]\n",
      "Train loss: 0.939927 [153600/345580]\n",
      "Train loss: 1.064316 [156800/345580]\n",
      "Train loss: 1.060556 [160000/345580]\n",
      "Train loss: 1.017587 [163200/345580]\n",
      "Train loss: 1.005997 [166400/345580]\n",
      "Train loss: 1.013380 [169600/345580]\n",
      "Train loss: 0.959095 [172800/345580]\n",
      "Train loss: 1.034076 [176000/345580]\n",
      "Train loss: 1.000344 [179200/345580]\n",
      "Train loss: 1.013499 [182400/345580]\n",
      "Train loss: 0.988592 [185600/345580]\n",
      "Train loss: 1.118415 [188800/345580]\n",
      "Train loss: 1.173777 [192000/345580]\n",
      "Train loss: 0.941905 [195200/345580]\n",
      "Train loss: 1.118649 [198400/345580]\n",
      "Train loss: 0.956441 [201600/345580]\n",
      "Train loss: 0.949300 [204800/345580]\n",
      "Train loss: 1.032645 [208000/345580]\n",
      "Train loss: 1.185552 [211200/345580]\n",
      "Train loss: 1.117204 [214400/345580]\n",
      "Train loss: 0.977463 [217600/345580]\n",
      "Train loss: 0.943565 [220800/345580]\n",
      "Train loss: 1.109711 [224000/345580]\n",
      "Train loss: 1.045645 [227200/345580]\n",
      "Train loss: 0.984241 [230400/345580]\n",
      "Train loss: 1.030143 [233600/345580]\n",
      "Train loss: 1.144721 [236800/345580]\n",
      "Train loss: 1.059092 [240000/345580]\n",
      "Train loss: 1.044736 [243200/345580]\n",
      "Train loss: 1.004210 [246400/345580]\n",
      "Train loss: 1.016052 [249600/345580]\n",
      "Train loss: 1.091321 [252800/345580]\n",
      "Train loss: 1.157410 [256000/345580]\n",
      "Train loss: 0.998280 [259200/345580]\n",
      "Train loss: 1.088181 [262400/345580]\n",
      "Train loss: 1.049328 [265600/345580]\n",
      "Train loss: 1.038198 [268800/345580]\n",
      "Train loss: 1.069864 [272000/345580]\n",
      "Train loss: 1.064189 [275200/345580]\n",
      "Train loss: 1.063980 [278400/345580]\n",
      "Train loss: 1.020675 [281600/345580]\n",
      "Train loss: 1.073302 [284800/345580]\n",
      "Train loss: 1.060300 [288000/345580]\n",
      "Train loss: 1.035025 [291200/345580]\n",
      "Train loss: 1.072552 [294400/345580]\n",
      "Train loss: 1.066851 [297600/345580]\n",
      "Train loss: 1.075225 [300800/345580]\n",
      "Train loss: 1.028888 [304000/345580]\n",
      "Train loss: 1.045053 [307200/345580]\n",
      "Train loss: 0.975101 [310400/345580]\n",
      "Train loss: 0.965078 [313600/345580]\n",
      "Train loss: 1.038990 [316800/345580]\n",
      "Train loss: 1.070379 [320000/345580]\n",
      "Train loss: 1.035924 [323200/345580]\n",
      "Train loss: 0.955264 [326400/345580]\n",
      "Train loss: 1.038122 [329600/345580]\n",
      "Train loss: 1.042520 [332800/345580]\n",
      "Train loss: 0.961639 [336000/345580]\n",
      "Train loss: 0.997398 [339200/345580]\n",
      "Train loss: 1.047778 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.041299 \n",
      "\n",
      "Epoch 43\n",
      " -------------------\n",
      "Train loss: 1.069960 [    0/345580]\n",
      "Train loss: 1.111619 [ 3200/345580]\n",
      "Train loss: 1.056945 [ 6400/345580]\n",
      "Train loss: 1.049918 [ 9600/345580]\n",
      "Train loss: 1.074811 [12800/345580]\n",
      "Train loss: 1.036004 [16000/345580]\n",
      "Train loss: 1.072273 [19200/345580]\n",
      "Train loss: 1.014430 [22400/345580]\n",
      "Train loss: 0.936609 [25600/345580]\n",
      "Train loss: 1.045796 [28800/345580]\n",
      "Train loss: 1.135438 [32000/345580]\n",
      "Train loss: 1.080297 [35200/345580]\n",
      "Train loss: 1.051442 [38400/345580]\n",
      "Train loss: 1.099804 [41600/345580]\n",
      "Train loss: 1.047047 [44800/345580]\n",
      "Train loss: 1.015979 [48000/345580]\n",
      "Train loss: 1.095600 [51200/345580]\n",
      "Train loss: 1.071408 [54400/345580]\n",
      "Train loss: 0.973156 [57600/345580]\n",
      "Train loss: 0.919939 [60800/345580]\n",
      "Train loss: 1.011179 [64000/345580]\n",
      "Train loss: 1.065723 [67200/345580]\n",
      "Train loss: 1.042158 [70400/345580]\n",
      "Train loss: 1.132589 [73600/345580]\n",
      "Train loss: 1.051646 [76800/345580]\n",
      "Train loss: 1.046597 [80000/345580]\n",
      "Train loss: 1.139807 [83200/345580]\n",
      "Train loss: 1.026634 [86400/345580]\n",
      "Train loss: 0.980099 [89600/345580]\n",
      "Train loss: 1.060369 [92800/345580]\n",
      "Train loss: 1.080304 [96000/345580]\n",
      "Train loss: 1.112156 [99200/345580]\n",
      "Train loss: 1.067480 [102400/345580]\n",
      "Train loss: 1.074751 [105600/345580]\n",
      "Train loss: 0.977686 [108800/345580]\n",
      "Train loss: 0.984449 [112000/345580]\n",
      "Train loss: 0.935355 [115200/345580]\n",
      "Train loss: 0.994589 [118400/345580]\n",
      "Train loss: 1.061470 [121600/345580]\n",
      "Train loss: 0.953823 [124800/345580]\n",
      "Train loss: 1.074335 [128000/345580]\n",
      "Train loss: 1.046643 [131200/345580]\n",
      "Train loss: 0.969332 [134400/345580]\n",
      "Train loss: 0.941991 [137600/345580]\n",
      "Train loss: 1.227628 [140800/345580]\n",
      "Train loss: 1.006886 [144000/345580]\n",
      "Train loss: 1.032540 [147200/345580]\n",
      "Train loss: 1.005220 [150400/345580]\n",
      "Train loss: 1.162124 [153600/345580]\n",
      "Train loss: 1.083760 [156800/345580]\n",
      "Train loss: 1.092400 [160000/345580]\n",
      "Train loss: 1.059164 [163200/345580]\n",
      "Train loss: 1.039080 [166400/345580]\n",
      "Train loss: 1.046366 [169600/345580]\n",
      "Train loss: 1.045009 [172800/345580]\n",
      "Train loss: 1.136323 [176000/345580]\n",
      "Train loss: 0.974273 [179200/345580]\n",
      "Train loss: 0.974388 [182400/345580]\n",
      "Train loss: 1.214228 [185600/345580]\n",
      "Train loss: 1.048034 [188800/345580]\n",
      "Train loss: 1.129263 [192000/345580]\n",
      "Train loss: 1.041245 [195200/345580]\n",
      "Train loss: 1.034093 [198400/345580]\n",
      "Train loss: 1.109542 [201600/345580]\n",
      "Train loss: 1.060422 [204800/345580]\n",
      "Train loss: 0.974064 [208000/345580]\n",
      "Train loss: 1.001033 [211200/345580]\n",
      "Train loss: 1.087788 [214400/345580]\n",
      "Train loss: 1.086849 [217600/345580]\n",
      "Train loss: 1.080436 [220800/345580]\n",
      "Train loss: 1.027079 [224000/345580]\n",
      "Train loss: 1.039648 [227200/345580]\n",
      "Train loss: 1.197750 [230400/345580]\n",
      "Train loss: 1.053786 [233600/345580]\n",
      "Train loss: 1.019207 [236800/345580]\n",
      "Train loss: 1.044662 [240000/345580]\n",
      "Train loss: 1.106685 [243200/345580]\n",
      "Train loss: 1.041095 [246400/345580]\n",
      "Train loss: 1.032780 [249600/345580]\n",
      "Train loss: 1.013479 [252800/345580]\n",
      "Train loss: 1.002642 [256000/345580]\n",
      "Train loss: 1.107625 [259200/345580]\n",
      "Train loss: 1.080146 [262400/345580]\n",
      "Train loss: 0.997715 [265600/345580]\n",
      "Train loss: 1.069965 [268800/345580]\n",
      "Train loss: 1.037344 [272000/345580]\n",
      "Train loss: 1.104852 [275200/345580]\n",
      "Train loss: 0.980356 [278400/345580]\n",
      "Train loss: 1.041910 [281600/345580]\n",
      "Train loss: 1.092200 [284800/345580]\n",
      "Train loss: 1.035306 [288000/345580]\n",
      "Train loss: 1.063077 [291200/345580]\n",
      "Train loss: 1.071569 [294400/345580]\n",
      "Train loss: 1.080069 [297600/345580]\n",
      "Train loss: 1.061591 [300800/345580]\n",
      "Train loss: 1.052522 [304000/345580]\n",
      "Train loss: 1.119322 [307200/345580]\n",
      "Train loss: 0.981168 [310400/345580]\n",
      "Train loss: 1.014962 [313600/345580]\n",
      "Train loss: 0.987929 [316800/345580]\n",
      "Train loss: 1.096459 [320000/345580]\n",
      "Train loss: 1.040701 [323200/345580]\n",
      "Train loss: 1.009825 [326400/345580]\n",
      "Train loss: 1.048815 [329600/345580]\n",
      "Train loss: 0.909364 [332800/345580]\n",
      "Train loss: 1.143190 [336000/345580]\n",
      "Train loss: 1.081640 [339200/345580]\n",
      "Train loss: 1.089628 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.042258 \n",
      "\n",
      "Epoch 44\n",
      " -------------------\n",
      "Train loss: 1.106286 [    0/345580]\n",
      "Train loss: 0.957715 [ 3200/345580]\n",
      "Train loss: 1.041986 [ 6400/345580]\n",
      "Train loss: 0.936214 [ 9600/345580]\n",
      "Train loss: 1.071538 [12800/345580]\n",
      "Train loss: 0.997949 [16000/345580]\n",
      "Train loss: 1.073339 [19200/345580]\n",
      "Train loss: 1.026463 [22400/345580]\n",
      "Train loss: 1.004373 [25600/345580]\n",
      "Train loss: 1.082999 [28800/345580]\n",
      "Train loss: 1.087191 [32000/345580]\n",
      "Train loss: 0.984046 [35200/345580]\n",
      "Train loss: 1.076249 [38400/345580]\n",
      "Train loss: 1.058656 [41600/345580]\n",
      "Train loss: 1.051079 [44800/345580]\n",
      "Train loss: 1.036693 [48000/345580]\n",
      "Train loss: 1.101897 [51200/345580]\n",
      "Train loss: 1.130492 [54400/345580]\n",
      "Train loss: 0.991658 [57600/345580]\n",
      "Train loss: 1.114558 [60800/345580]\n",
      "Train loss: 1.084577 [64000/345580]\n",
      "Train loss: 0.917391 [67200/345580]\n",
      "Train loss: 1.181644 [70400/345580]\n",
      "Train loss: 0.998061 [73600/345580]\n",
      "Train loss: 1.041141 [76800/345580]\n",
      "Train loss: 0.996248 [80000/345580]\n",
      "Train loss: 1.083715 [83200/345580]\n",
      "Train loss: 1.154709 [86400/345580]\n",
      "Train loss: 1.081053 [89600/345580]\n",
      "Train loss: 1.008383 [92800/345580]\n",
      "Train loss: 0.985582 [96000/345580]\n",
      "Train loss: 1.088195 [99200/345580]\n",
      "Train loss: 1.007093 [102400/345580]\n",
      "Train loss: 0.985503 [105600/345580]\n",
      "Train loss: 1.061208 [108800/345580]\n",
      "Train loss: 1.076667 [112000/345580]\n",
      "Train loss: 1.090796 [115200/345580]\n",
      "Train loss: 1.109809 [118400/345580]\n",
      "Train loss: 1.021457 [121600/345580]\n",
      "Train loss: 1.052511 [124800/345580]\n",
      "Train loss: 1.035280 [128000/345580]\n",
      "Train loss: 1.020415 [131200/345580]\n",
      "Train loss: 0.961130 [134400/345580]\n",
      "Train loss: 1.024969 [137600/345580]\n",
      "Train loss: 1.073021 [140800/345580]\n",
      "Train loss: 0.972476 [144000/345580]\n",
      "Train loss: 1.045605 [147200/345580]\n",
      "Train loss: 1.087017 [150400/345580]\n",
      "Train loss: 1.055692 [153600/345580]\n",
      "Train loss: 0.975598 [156800/345580]\n",
      "Train loss: 0.981490 [160000/345580]\n",
      "Train loss: 1.133435 [163200/345580]\n",
      "Train loss: 1.127415 [166400/345580]\n",
      "Train loss: 1.022819 [169600/345580]\n",
      "Train loss: 1.078341 [172800/345580]\n",
      "Train loss: 1.067210 [176000/345580]\n",
      "Train loss: 1.136154 [179200/345580]\n",
      "Train loss: 1.016930 [182400/345580]\n",
      "Train loss: 0.995961 [185600/345580]\n",
      "Train loss: 1.027563 [188800/345580]\n",
      "Train loss: 0.985067 [192000/345580]\n",
      "Train loss: 1.139423 [195200/345580]\n",
      "Train loss: 0.982666 [198400/345580]\n",
      "Train loss: 1.130762 [201600/345580]\n",
      "Train loss: 1.057958 [204800/345580]\n",
      "Train loss: 1.002625 [208000/345580]\n",
      "Train loss: 1.100894 [211200/345580]\n",
      "Train loss: 1.026729 [214400/345580]\n",
      "Train loss: 1.063387 [217600/345580]\n",
      "Train loss: 1.106198 [220800/345580]\n",
      "Train loss: 0.928746 [224000/345580]\n",
      "Train loss: 1.076959 [227200/345580]\n",
      "Train loss: 1.048211 [230400/345580]\n",
      "Train loss: 1.045368 [233600/345580]\n",
      "Train loss: 1.090678 [236800/345580]\n",
      "Train loss: 1.070642 [240000/345580]\n",
      "Train loss: 1.015011 [243200/345580]\n",
      "Train loss: 1.115889 [246400/345580]\n",
      "Train loss: 1.041010 [249600/345580]\n",
      "Train loss: 1.057224 [252800/345580]\n",
      "Train loss: 1.105802 [256000/345580]\n",
      "Train loss: 0.978496 [259200/345580]\n",
      "Train loss: 1.137294 [262400/345580]\n",
      "Train loss: 1.076513 [265600/345580]\n",
      "Train loss: 1.127108 [268800/345580]\n",
      "Train loss: 1.023623 [272000/345580]\n",
      "Train loss: 1.090350 [275200/345580]\n",
      "Train loss: 1.108796 [278400/345580]\n",
      "Train loss: 0.996625 [281600/345580]\n",
      "Train loss: 1.063873 [284800/345580]\n",
      "Train loss: 1.026118 [288000/345580]\n",
      "Train loss: 0.977025 [291200/345580]\n",
      "Train loss: 1.045562 [294400/345580]\n",
      "Train loss: 1.055741 [297600/345580]\n",
      "Train loss: 1.134650 [300800/345580]\n",
      "Train loss: 1.127366 [304000/345580]\n",
      "Train loss: 0.996841 [307200/345580]\n",
      "Train loss: 1.109992 [310400/345580]\n",
      "Train loss: 1.067820 [313600/345580]\n",
      "Train loss: 0.995409 [316800/345580]\n",
      "Train loss: 1.151762 [320000/345580]\n",
      "Train loss: 1.023709 [323200/345580]\n",
      "Train loss: 0.934801 [326400/345580]\n",
      "Train loss: 1.039394 [329600/345580]\n",
      "Train loss: 1.038125 [332800/345580]\n",
      "Train loss: 1.072360 [336000/345580]\n",
      "Train loss: 1.046735 [339200/345580]\n",
      "Train loss: 0.987423 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.041458 \n",
      "\n",
      "Epoch 45\n",
      " -------------------\n",
      "Train loss: 0.954210 [    0/345580]\n",
      "Train loss: 1.027176 [ 3200/345580]\n",
      "Train loss: 1.030807 [ 6400/345580]\n",
      "Train loss: 1.120386 [ 9600/345580]\n",
      "Train loss: 1.109245 [12800/345580]\n",
      "Train loss: 0.945989 [16000/345580]\n",
      "Train loss: 1.119370 [19200/345580]\n",
      "Train loss: 0.993588 [22400/345580]\n",
      "Train loss: 1.072085 [25600/345580]\n",
      "Train loss: 1.239866 [28800/345580]\n",
      "Train loss: 1.043013 [32000/345580]\n",
      "Train loss: 0.987954 [35200/345580]\n",
      "Train loss: 1.032030 [38400/345580]\n",
      "Train loss: 1.031497 [41600/345580]\n",
      "Train loss: 1.016221 [44800/345580]\n",
      "Train loss: 1.109461 [48000/345580]\n",
      "Train loss: 1.003908 [51200/345580]\n",
      "Train loss: 0.982918 [54400/345580]\n",
      "Train loss: 1.004149 [57600/345580]\n",
      "Train loss: 1.008452 [60800/345580]\n",
      "Train loss: 1.072664 [64000/345580]\n",
      "Train loss: 1.029759 [67200/345580]\n",
      "Train loss: 1.098483 [70400/345580]\n",
      "Train loss: 1.058974 [73600/345580]\n",
      "Train loss: 1.132246 [76800/345580]\n",
      "Train loss: 1.124930 [80000/345580]\n",
      "Train loss: 1.091194 [83200/345580]\n",
      "Train loss: 1.013150 [86400/345580]\n",
      "Train loss: 0.984429 [89600/345580]\n",
      "Train loss: 1.124558 [92800/345580]\n",
      "Train loss: 1.042566 [96000/345580]\n",
      "Train loss: 1.087257 [99200/345580]\n",
      "Train loss: 1.053156 [102400/345580]\n",
      "Train loss: 1.023836 [105600/345580]\n",
      "Train loss: 1.039575 [108800/345580]\n",
      "Train loss: 0.910255 [112000/345580]\n",
      "Train loss: 1.018832 [115200/345580]\n",
      "Train loss: 1.009920 [118400/345580]\n",
      "Train loss: 1.003156 [121600/345580]\n",
      "Train loss: 1.101618 [124800/345580]\n",
      "Train loss: 0.923495 [128000/345580]\n",
      "Train loss: 1.108911 [131200/345580]\n",
      "Train loss: 1.092784 [134400/345580]\n",
      "Train loss: 1.061557 [137600/345580]\n",
      "Train loss: 1.113525 [140800/345580]\n",
      "Train loss: 1.004044 [144000/345580]\n",
      "Train loss: 0.959502 [147200/345580]\n",
      "Train loss: 1.044562 [150400/345580]\n",
      "Train loss: 0.987219 [153600/345580]\n",
      "Train loss: 1.043373 [156800/345580]\n",
      "Train loss: 1.051410 [160000/345580]\n",
      "Train loss: 1.047240 [163200/345580]\n",
      "Train loss: 1.016523 [166400/345580]\n",
      "Train loss: 1.124194 [169600/345580]\n",
      "Train loss: 1.052794 [172800/345580]\n",
      "Train loss: 1.120463 [176000/345580]\n",
      "Train loss: 1.013863 [179200/345580]\n",
      "Train loss: 1.083532 [182400/345580]\n",
      "Train loss: 0.951610 [185600/345580]\n",
      "Train loss: 0.966533 [188800/345580]\n",
      "Train loss: 1.021835 [192000/345580]\n",
      "Train loss: 1.050807 [195200/345580]\n",
      "Train loss: 0.995391 [198400/345580]\n",
      "Train loss: 0.954237 [201600/345580]\n",
      "Train loss: 1.024939 [204800/345580]\n",
      "Train loss: 0.974233 [208000/345580]\n",
      "Train loss: 1.051980 [211200/345580]\n",
      "Train loss: 1.134849 [214400/345580]\n",
      "Train loss: 1.064862 [217600/345580]\n",
      "Train loss: 1.163343 [220800/345580]\n",
      "Train loss: 1.015477 [224000/345580]\n",
      "Train loss: 1.195824 [227200/345580]\n",
      "Train loss: 0.961303 [230400/345580]\n",
      "Train loss: 1.040139 [233600/345580]\n",
      "Train loss: 1.106208 [236800/345580]\n",
      "Train loss: 0.999435 [240000/345580]\n",
      "Train loss: 1.016271 [243200/345580]\n",
      "Train loss: 0.967248 [246400/345580]\n",
      "Train loss: 0.978137 [249600/345580]\n",
      "Train loss: 1.043655 [252800/345580]\n",
      "Train loss: 1.014943 [256000/345580]\n",
      "Train loss: 1.051836 [259200/345580]\n",
      "Train loss: 0.964570 [262400/345580]\n",
      "Train loss: 0.984746 [265600/345580]\n",
      "Train loss: 1.019763 [268800/345580]\n",
      "Train loss: 1.089374 [272000/345580]\n",
      "Train loss: 0.981467 [275200/345580]\n",
      "Train loss: 1.053741 [278400/345580]\n",
      "Train loss: 1.006396 [281600/345580]\n",
      "Train loss: 1.090739 [284800/345580]\n",
      "Train loss: 1.155997 [288000/345580]\n",
      "Train loss: 0.999888 [291200/345580]\n",
      "Train loss: 1.015196 [294400/345580]\n",
      "Train loss: 1.088382 [297600/345580]\n",
      "Train loss: 1.037958 [300800/345580]\n",
      "Train loss: 1.036130 [304000/345580]\n",
      "Train loss: 1.062093 [307200/345580]\n",
      "Train loss: 1.046431 [310400/345580]\n",
      "Train loss: 1.112014 [313600/345580]\n",
      "Train loss: 1.063039 [316800/345580]\n",
      "Train loss: 0.929950 [320000/345580]\n",
      "Train loss: 0.876268 [323200/345580]\n",
      "Train loss: 0.994411 [326400/345580]\n",
      "Train loss: 0.987449 [329600/345580]\n",
      "Train loss: 1.024623 [332800/345580]\n",
      "Train loss: 1.068509 [336000/345580]\n",
      "Train loss: 1.048960 [339200/345580]\n",
      "Train loss: 1.033126 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 42.7%, Avg loss: 1.042764 \n",
      "\n",
      "Epoch 46\n",
      " -------------------\n",
      "Train loss: 1.062048 [    0/345580]\n",
      "Train loss: 1.108778 [ 3200/345580]\n",
      "Train loss: 1.035096 [ 6400/345580]\n",
      "Train loss: 1.015403 [ 9600/345580]\n",
      "Train loss: 0.996754 [12800/345580]\n",
      "Train loss: 1.062019 [16000/345580]\n",
      "Train loss: 0.940053 [19200/345580]\n",
      "Train loss: 1.073053 [22400/345580]\n",
      "Train loss: 1.042103 [25600/345580]\n",
      "Train loss: 1.013007 [28800/345580]\n",
      "Train loss: 1.083409 [32000/345580]\n",
      "Train loss: 1.060185 [35200/345580]\n",
      "Train loss: 1.034655 [38400/345580]\n",
      "Train loss: 1.039993 [41600/345580]\n",
      "Train loss: 0.992457 [44800/345580]\n",
      "Train loss: 1.034724 [48000/345580]\n",
      "Train loss: 1.080723 [51200/345580]\n",
      "Train loss: 1.013714 [54400/345580]\n",
      "Train loss: 0.941754 [57600/345580]\n",
      "Train loss: 1.044956 [60800/345580]\n",
      "Train loss: 0.967864 [64000/345580]\n",
      "Train loss: 1.088416 [67200/345580]\n",
      "Train loss: 1.057936 [70400/345580]\n",
      "Train loss: 1.059692 [73600/345580]\n",
      "Train loss: 1.010018 [76800/345580]\n",
      "Train loss: 0.947242 [80000/345580]\n",
      "Train loss: 1.107727 [83200/345580]\n",
      "Train loss: 1.096240 [86400/345580]\n",
      "Train loss: 1.052871 [89600/345580]\n",
      "Train loss: 0.991196 [92800/345580]\n",
      "Train loss: 0.931016 [96000/345580]\n",
      "Train loss: 1.112447 [99200/345580]\n",
      "Train loss: 1.055169 [102400/345580]\n",
      "Train loss: 1.026818 [105600/345580]\n",
      "Train loss: 1.066640 [108800/345580]\n",
      "Train loss: 1.059507 [112000/345580]\n",
      "Train loss: 1.006299 [115200/345580]\n",
      "Train loss: 1.134090 [118400/345580]\n",
      "Train loss: 1.094660 [121600/345580]\n",
      "Train loss: 1.046595 [124800/345580]\n",
      "Train loss: 1.026301 [128000/345580]\n",
      "Train loss: 1.034632 [131200/345580]\n",
      "Train loss: 1.024325 [134400/345580]\n",
      "Train loss: 0.994794 [137600/345580]\n",
      "Train loss: 1.055910 [140800/345580]\n",
      "Train loss: 1.048936 [144000/345580]\n",
      "Train loss: 0.994798 [147200/345580]\n",
      "Train loss: 1.017717 [150400/345580]\n",
      "Train loss: 1.008166 [153600/345580]\n",
      "Train loss: 1.113254 [156800/345580]\n",
      "Train loss: 1.072607 [160000/345580]\n",
      "Train loss: 1.127464 [163200/345580]\n",
      "Train loss: 1.031124 [166400/345580]\n",
      "Train loss: 1.054289 [169600/345580]\n",
      "Train loss: 0.992547 [172800/345580]\n",
      "Train loss: 0.997306 [176000/345580]\n",
      "Train loss: 1.261456 [179200/345580]\n",
      "Train loss: 1.042534 [182400/345580]\n",
      "Train loss: 1.028737 [185600/345580]\n",
      "Train loss: 1.048857 [188800/345580]\n",
      "Train loss: 1.069584 [192000/345580]\n",
      "Train loss: 1.093789 [195200/345580]\n",
      "Train loss: 1.074473 [198400/345580]\n",
      "Train loss: 1.088418 [201600/345580]\n",
      "Train loss: 1.088228 [204800/345580]\n",
      "Train loss: 1.072357 [208000/345580]\n",
      "Train loss: 0.940064 [211200/345580]\n",
      "Train loss: 1.079318 [214400/345580]\n",
      "Train loss: 1.018266 [217600/345580]\n",
      "Train loss: 1.016711 [220800/345580]\n",
      "Train loss: 1.051223 [224000/345580]\n",
      "Train loss: 1.074264 [227200/345580]\n",
      "Train loss: 1.034179 [230400/345580]\n",
      "Train loss: 1.025738 [233600/345580]\n",
      "Train loss: 1.083096 [236800/345580]\n",
      "Train loss: 1.027570 [240000/345580]\n",
      "Train loss: 1.015736 [243200/345580]\n",
      "Train loss: 0.984984 [246400/345580]\n",
      "Train loss: 0.978898 [249600/345580]\n",
      "Train loss: 1.006261 [252800/345580]\n",
      "Train loss: 1.041985 [256000/345580]\n",
      "Train loss: 1.038495 [259200/345580]\n",
      "Train loss: 1.022158 [262400/345580]\n",
      "Train loss: 0.979439 [265600/345580]\n",
      "Train loss: 1.096806 [268800/345580]\n",
      "Train loss: 1.116392 [272000/345580]\n",
      "Train loss: 1.124519 [275200/345580]\n",
      "Train loss: 1.054300 [278400/345580]\n",
      "Train loss: 1.067766 [281600/345580]\n",
      "Train loss: 1.044703 [284800/345580]\n",
      "Train loss: 0.966406 [288000/345580]\n",
      "Train loss: 1.036833 [291200/345580]\n",
      "Train loss: 0.907223 [294400/345580]\n",
      "Train loss: 1.028600 [297600/345580]\n",
      "Train loss: 1.020250 [300800/345580]\n",
      "Train loss: 0.991304 [304000/345580]\n",
      "Train loss: 1.031301 [307200/345580]\n",
      "Train loss: 1.007959 [310400/345580]\n",
      "Train loss: 1.085424 [313600/345580]\n",
      "Train loss: 1.061543 [316800/345580]\n",
      "Train loss: 1.071375 [320000/345580]\n",
      "Train loss: 1.158472 [323200/345580]\n",
      "Train loss: 1.101270 [326400/345580]\n",
      "Train loss: 1.036924 [329600/345580]\n",
      "Train loss: 1.110208 [332800/345580]\n",
      "Train loss: 1.014947 [336000/345580]\n",
      "Train loss: 1.104318 [339200/345580]\n",
      "Train loss: 1.139705 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042194 \n",
      "\n",
      "Epoch 47\n",
      " -------------------\n",
      "Train loss: 1.057196 [    0/345580]\n",
      "Train loss: 1.079473 [ 3200/345580]\n",
      "Train loss: 1.101068 [ 6400/345580]\n",
      "Train loss: 1.111913 [ 9600/345580]\n",
      "Train loss: 1.025814 [12800/345580]\n",
      "Train loss: 0.895162 [16000/345580]\n",
      "Train loss: 1.038108 [19200/345580]\n",
      "Train loss: 1.058409 [22400/345580]\n",
      "Train loss: 1.110826 [25600/345580]\n",
      "Train loss: 1.060884 [28800/345580]\n",
      "Train loss: 1.093193 [32000/345580]\n",
      "Train loss: 1.083059 [35200/345580]\n",
      "Train loss: 1.051650 [38400/345580]\n",
      "Train loss: 0.978880 [41600/345580]\n",
      "Train loss: 1.021940 [44800/345580]\n",
      "Train loss: 1.022872 [48000/345580]\n",
      "Train loss: 1.050982 [51200/345580]\n",
      "Train loss: 1.013935 [54400/345580]\n",
      "Train loss: 1.059813 [57600/345580]\n",
      "Train loss: 1.027642 [60800/345580]\n",
      "Train loss: 1.062748 [64000/345580]\n",
      "Train loss: 1.040945 [67200/345580]\n",
      "Train loss: 1.087868 [70400/345580]\n",
      "Train loss: 1.092950 [73600/345580]\n",
      "Train loss: 0.971289 [76800/345580]\n",
      "Train loss: 1.124097 [80000/345580]\n",
      "Train loss: 1.074094 [83200/345580]\n",
      "Train loss: 1.029026 [86400/345580]\n",
      "Train loss: 1.093935 [89600/345580]\n",
      "Train loss: 0.929721 [92800/345580]\n",
      "Train loss: 1.046122 [96000/345580]\n",
      "Train loss: 0.897464 [99200/345580]\n",
      "Train loss: 1.081519 [102400/345580]\n",
      "Train loss: 1.033477 [105600/345580]\n",
      "Train loss: 1.046148 [108800/345580]\n",
      "Train loss: 0.942256 [112000/345580]\n",
      "Train loss: 1.038134 [115200/345580]\n",
      "Train loss: 1.163425 [118400/345580]\n",
      "Train loss: 0.972497 [121600/345580]\n",
      "Train loss: 1.063234 [124800/345580]\n",
      "Train loss: 0.983591 [128000/345580]\n",
      "Train loss: 0.983652 [131200/345580]\n",
      "Train loss: 1.053279 [134400/345580]\n",
      "Train loss: 0.966525 [137600/345580]\n",
      "Train loss: 1.133017 [140800/345580]\n",
      "Train loss: 0.992550 [144000/345580]\n",
      "Train loss: 1.007142 [147200/345580]\n",
      "Train loss: 1.038849 [150400/345580]\n",
      "Train loss: 0.971045 [153600/345580]\n",
      "Train loss: 1.029979 [156800/345580]\n",
      "Train loss: 1.009505 [160000/345580]\n",
      "Train loss: 1.132147 [163200/345580]\n",
      "Train loss: 1.011934 [166400/345580]\n",
      "Train loss: 0.928000 [169600/345580]\n",
      "Train loss: 1.008096 [172800/345580]\n",
      "Train loss: 1.094088 [176000/345580]\n",
      "Train loss: 1.030475 [179200/345580]\n",
      "Train loss: 1.028426 [182400/345580]\n",
      "Train loss: 0.968101 [185600/345580]\n",
      "Train loss: 1.128976 [188800/345580]\n",
      "Train loss: 1.075187 [192000/345580]\n",
      "Train loss: 1.070918 [195200/345580]\n",
      "Train loss: 1.135162 [198400/345580]\n",
      "Train loss: 1.034051 [201600/345580]\n",
      "Train loss: 0.946292 [204800/345580]\n",
      "Train loss: 1.094938 [208000/345580]\n",
      "Train loss: 1.037246 [211200/345580]\n",
      "Train loss: 1.040364 [214400/345580]\n",
      "Train loss: 1.025239 [217600/345580]\n",
      "Train loss: 0.985003 [220800/345580]\n",
      "Train loss: 1.047611 [224000/345580]\n",
      "Train loss: 1.038559 [227200/345580]\n",
      "Train loss: 1.054734 [230400/345580]\n",
      "Train loss: 1.029564 [233600/345580]\n",
      "Train loss: 1.078855 [236800/345580]\n",
      "Train loss: 1.032136 [240000/345580]\n",
      "Train loss: 0.951289 [243200/345580]\n",
      "Train loss: 1.024885 [246400/345580]\n",
      "Train loss: 0.939057 [249600/345580]\n",
      "Train loss: 1.084175 [252800/345580]\n",
      "Train loss: 1.066644 [256000/345580]\n",
      "Train loss: 1.007208 [259200/345580]\n",
      "Train loss: 1.054561 [262400/345580]\n",
      "Train loss: 0.969229 [265600/345580]\n",
      "Train loss: 1.034873 [268800/345580]\n",
      "Train loss: 1.027995 [272000/345580]\n",
      "Train loss: 1.007682 [275200/345580]\n",
      "Train loss: 1.169450 [278400/345580]\n",
      "Train loss: 0.941953 [281600/345580]\n",
      "Train loss: 0.956631 [284800/345580]\n",
      "Train loss: 1.062489 [288000/345580]\n",
      "Train loss: 1.000640 [291200/345580]\n",
      "Train loss: 1.081440 [294400/345580]\n",
      "Train loss: 0.977866 [297600/345580]\n",
      "Train loss: 1.034769 [300800/345580]\n",
      "Train loss: 1.137847 [304000/345580]\n",
      "Train loss: 0.985883 [307200/345580]\n",
      "Train loss: 0.974249 [310400/345580]\n",
      "Train loss: 1.050788 [313600/345580]\n",
      "Train loss: 1.095984 [316800/345580]\n",
      "Train loss: 1.028007 [320000/345580]\n",
      "Train loss: 1.027737 [323200/345580]\n",
      "Train loss: 0.974435 [326400/345580]\n",
      "Train loss: 1.108595 [329600/345580]\n",
      "Train loss: 1.013779 [332800/345580]\n",
      "Train loss: 1.062571 [336000/345580]\n",
      "Train loss: 1.044632 [339200/345580]\n",
      "Train loss: 1.101951 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.043050 \n",
      "\n",
      "Epoch 48\n",
      " -------------------\n",
      "Train loss: 1.031680 [    0/345580]\n",
      "Train loss: 0.971209 [ 3200/345580]\n",
      "Train loss: 1.019006 [ 6400/345580]\n",
      "Train loss: 1.077477 [ 9600/345580]\n",
      "Train loss: 1.003518 [12800/345580]\n",
      "Train loss: 1.087584 [16000/345580]\n",
      "Train loss: 0.981724 [19200/345580]\n",
      "Train loss: 1.029189 [22400/345580]\n",
      "Train loss: 0.993032 [25600/345580]\n",
      "Train loss: 1.062795 [28800/345580]\n",
      "Train loss: 1.185047 [32000/345580]\n",
      "Train loss: 1.001985 [35200/345580]\n",
      "Train loss: 0.997321 [38400/345580]\n",
      "Train loss: 1.119315 [41600/345580]\n",
      "Train loss: 1.023532 [44800/345580]\n",
      "Train loss: 1.100116 [48000/345580]\n",
      "Train loss: 1.144224 [51200/345580]\n",
      "Train loss: 1.047117 [54400/345580]\n",
      "Train loss: 0.913986 [57600/345580]\n",
      "Train loss: 1.103179 [60800/345580]\n",
      "Train loss: 0.972222 [64000/345580]\n",
      "Train loss: 0.976363 [67200/345580]\n",
      "Train loss: 0.980713 [70400/345580]\n",
      "Train loss: 1.002288 [73600/345580]\n",
      "Train loss: 0.990307 [76800/345580]\n",
      "Train loss: 0.985479 [80000/345580]\n",
      "Train loss: 0.924408 [83200/345580]\n",
      "Train loss: 0.986719 [86400/345580]\n",
      "Train loss: 1.101257 [89600/345580]\n",
      "Train loss: 1.086109 [92800/345580]\n",
      "Train loss: 0.980630 [96000/345580]\n",
      "Train loss: 1.183303 [99200/345580]\n",
      "Train loss: 1.056201 [102400/345580]\n",
      "Train loss: 1.043659 [105600/345580]\n",
      "Train loss: 0.900343 [108800/345580]\n",
      "Train loss: 1.077868 [112000/345580]\n",
      "Train loss: 0.977709 [115200/345580]\n",
      "Train loss: 1.155004 [118400/345580]\n",
      "Train loss: 0.962464 [121600/345580]\n",
      "Train loss: 1.133068 [124800/345580]\n",
      "Train loss: 1.029668 [128000/345580]\n",
      "Train loss: 1.035716 [131200/345580]\n",
      "Train loss: 1.008649 [134400/345580]\n",
      "Train loss: 1.122122 [137600/345580]\n",
      "Train loss: 1.037379 [140800/345580]\n",
      "Train loss: 1.122456 [144000/345580]\n",
      "Train loss: 0.963614 [147200/345580]\n",
      "Train loss: 1.051193 [150400/345580]\n",
      "Train loss: 1.105338 [153600/345580]\n",
      "Train loss: 1.039560 [156800/345580]\n",
      "Train loss: 0.937522 [160000/345580]\n",
      "Train loss: 0.979411 [163200/345580]\n",
      "Train loss: 1.012704 [166400/345580]\n",
      "Train loss: 1.068135 [169600/345580]\n",
      "Train loss: 1.002107 [172800/345580]\n",
      "Train loss: 1.091129 [176000/345580]\n",
      "Train loss: 1.021279 [179200/345580]\n",
      "Train loss: 1.045918 [182400/345580]\n",
      "Train loss: 1.121600 [185600/345580]\n",
      "Train loss: 1.080750 [188800/345580]\n",
      "Train loss: 0.991456 [192000/345580]\n",
      "Train loss: 1.032428 [195200/345580]\n",
      "Train loss: 1.068852 [198400/345580]\n",
      "Train loss: 1.108635 [201600/345580]\n",
      "Train loss: 1.012408 [204800/345580]\n",
      "Train loss: 1.055062 [208000/345580]\n",
      "Train loss: 1.068982 [211200/345580]\n",
      "Train loss: 0.993466 [214400/345580]\n",
      "Train loss: 1.097924 [217600/345580]\n",
      "Train loss: 1.092111 [220800/345580]\n",
      "Train loss: 1.000678 [224000/345580]\n",
      "Train loss: 0.999570 [227200/345580]\n",
      "Train loss: 1.059698 [230400/345580]\n",
      "Train loss: 1.061315 [233600/345580]\n",
      "Train loss: 1.128287 [236800/345580]\n",
      "Train loss: 1.046901 [240000/345580]\n",
      "Train loss: 1.129918 [243200/345580]\n",
      "Train loss: 1.083151 [246400/345580]\n",
      "Train loss: 1.030216 [249600/345580]\n",
      "Train loss: 0.979390 [252800/345580]\n",
      "Train loss: 1.002978 [256000/345580]\n",
      "Train loss: 1.006438 [259200/345580]\n",
      "Train loss: 1.062975 [262400/345580]\n",
      "Train loss: 1.061937 [265600/345580]\n",
      "Train loss: 1.007739 [268800/345580]\n",
      "Train loss: 1.080579 [272000/345580]\n",
      "Train loss: 0.956549 [275200/345580]\n",
      "Train loss: 1.020836 [278400/345580]\n",
      "Train loss: 1.112488 [281600/345580]\n",
      "Train loss: 1.139399 [284800/345580]\n",
      "Train loss: 1.189990 [288000/345580]\n",
      "Train loss: 1.057096 [291200/345580]\n",
      "Train loss: 0.956442 [294400/345580]\n",
      "Train loss: 1.096703 [297600/345580]\n",
      "Train loss: 1.020688 [300800/345580]\n",
      "Train loss: 0.920900 [304000/345580]\n",
      "Train loss: 1.048015 [307200/345580]\n",
      "Train loss: 1.061434 [310400/345580]\n",
      "Train loss: 1.085631 [313600/345580]\n",
      "Train loss: 1.150015 [316800/345580]\n",
      "Train loss: 1.067875 [320000/345580]\n",
      "Train loss: 1.040897 [323200/345580]\n",
      "Train loss: 1.136030 [326400/345580]\n",
      "Train loss: 1.026348 [329600/345580]\n",
      "Train loss: 0.991945 [332800/345580]\n",
      "Train loss: 1.086768 [336000/345580]\n",
      "Train loss: 1.070084 [339200/345580]\n",
      "Train loss: 0.945790 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.041862 \n",
      "\n",
      "Epoch 49\n",
      " -------------------\n",
      "Train loss: 1.019694 [    0/345580]\n",
      "Train loss: 1.020698 [ 3200/345580]\n",
      "Train loss: 0.968230 [ 6400/345580]\n",
      "Train loss: 1.055997 [ 9600/345580]\n",
      "Train loss: 1.056817 [12800/345580]\n",
      "Train loss: 1.095557 [16000/345580]\n",
      "Train loss: 1.025200 [19200/345580]\n",
      "Train loss: 1.100565 [22400/345580]\n",
      "Train loss: 1.046922 [25600/345580]\n",
      "Train loss: 0.998506 [28800/345580]\n",
      "Train loss: 1.061109 [32000/345580]\n",
      "Train loss: 1.031218 [35200/345580]\n",
      "Train loss: 1.189566 [38400/345580]\n",
      "Train loss: 1.039407 [41600/345580]\n",
      "Train loss: 1.107006 [44800/345580]\n",
      "Train loss: 1.038221 [48000/345580]\n",
      "Train loss: 1.079552 [51200/345580]\n",
      "Train loss: 1.129247 [54400/345580]\n",
      "Train loss: 1.121919 [57600/345580]\n",
      "Train loss: 1.030355 [60800/345580]\n",
      "Train loss: 0.929295 [64000/345580]\n",
      "Train loss: 1.143768 [67200/345580]\n",
      "Train loss: 1.098590 [70400/345580]\n",
      "Train loss: 1.074382 [73600/345580]\n",
      "Train loss: 1.039861 [76800/345580]\n",
      "Train loss: 1.075040 [80000/345580]\n",
      "Train loss: 0.894212 [83200/345580]\n",
      "Train loss: 1.156678 [86400/345580]\n",
      "Train loss: 0.986773 [89600/345580]\n",
      "Train loss: 0.938730 [92800/345580]\n",
      "Train loss: 1.081233 [96000/345580]\n",
      "Train loss: 1.066260 [99200/345580]\n",
      "Train loss: 1.004528 [102400/345580]\n",
      "Train loss: 1.035559 [105600/345580]\n",
      "Train loss: 1.017762 [108800/345580]\n",
      "Train loss: 1.020533 [112000/345580]\n",
      "Train loss: 1.053872 [115200/345580]\n",
      "Train loss: 1.007564 [118400/345580]\n",
      "Train loss: 1.034896 [121600/345580]\n",
      "Train loss: 1.029461 [124800/345580]\n",
      "Train loss: 1.078355 [128000/345580]\n",
      "Train loss: 1.090068 [131200/345580]\n",
      "Train loss: 1.144830 [134400/345580]\n",
      "Train loss: 1.163847 [137600/345580]\n",
      "Train loss: 1.137520 [140800/345580]\n",
      "Train loss: 1.075551 [144000/345580]\n",
      "Train loss: 1.135741 [147200/345580]\n",
      "Train loss: 1.050774 [150400/345580]\n",
      "Train loss: 1.156836 [153600/345580]\n",
      "Train loss: 1.087608 [156800/345580]\n",
      "Train loss: 1.040297 [160000/345580]\n",
      "Train loss: 1.059346 [163200/345580]\n",
      "Train loss: 1.058625 [166400/345580]\n",
      "Train loss: 1.066210 [169600/345580]\n",
      "Train loss: 1.060243 [172800/345580]\n",
      "Train loss: 1.123580 [176000/345580]\n",
      "Train loss: 1.172270 [179200/345580]\n",
      "Train loss: 1.070078 [182400/345580]\n",
      "Train loss: 0.992957 [185600/345580]\n",
      "Train loss: 1.070860 [188800/345580]\n",
      "Train loss: 0.963283 [192000/345580]\n",
      "Train loss: 1.076784 [195200/345580]\n",
      "Train loss: 1.039632 [198400/345580]\n",
      "Train loss: 0.993670 [201600/345580]\n",
      "Train loss: 1.061139 [204800/345580]\n",
      "Train loss: 1.107774 [208000/345580]\n",
      "Train loss: 1.027647 [211200/345580]\n",
      "Train loss: 1.071249 [214400/345580]\n",
      "Train loss: 1.017733 [217600/345580]\n",
      "Train loss: 0.935486 [220800/345580]\n",
      "Train loss: 1.017460 [224000/345580]\n",
      "Train loss: 1.096311 [227200/345580]\n",
      "Train loss: 1.087582 [230400/345580]\n",
      "Train loss: 1.027580 [233600/345580]\n",
      "Train loss: 1.097987 [236800/345580]\n",
      "Train loss: 0.998571 [240000/345580]\n",
      "Train loss: 1.053954 [243200/345580]\n",
      "Train loss: 1.080197 [246400/345580]\n",
      "Train loss: 0.936435 [249600/345580]\n",
      "Train loss: 0.978052 [252800/345580]\n",
      "Train loss: 1.019639 [256000/345580]\n",
      "Train loss: 1.079772 [259200/345580]\n",
      "Train loss: 1.068900 [262400/345580]\n",
      "Train loss: 1.033739 [265600/345580]\n",
      "Train loss: 1.018353 [268800/345580]\n",
      "Train loss: 1.050308 [272000/345580]\n",
      "Train loss: 0.995061 [275200/345580]\n",
      "Train loss: 0.975250 [278400/345580]\n",
      "Train loss: 0.957667 [281600/345580]\n",
      "Train loss: 0.998180 [284800/345580]\n",
      "Train loss: 1.056201 [288000/345580]\n",
      "Train loss: 1.049771 [291200/345580]\n",
      "Train loss: 1.017999 [294400/345580]\n",
      "Train loss: 1.075716 [297600/345580]\n",
      "Train loss: 1.107615 [300800/345580]\n",
      "Train loss: 1.007226 [304000/345580]\n",
      "Train loss: 1.064648 [307200/345580]\n",
      "Train loss: 1.141614 [310400/345580]\n",
      "Train loss: 1.032570 [313600/345580]\n",
      "Train loss: 1.161940 [316800/345580]\n",
      "Train loss: 1.044403 [320000/345580]\n",
      "Train loss: 1.109665 [323200/345580]\n",
      "Train loss: 0.950114 [326400/345580]\n",
      "Train loss: 1.072203 [329600/345580]\n",
      "Train loss: 1.194830 [332800/345580]\n",
      "Train loss: 1.156471 [336000/345580]\n",
      "Train loss: 0.946356 [339200/345580]\n",
      "Train loss: 1.062245 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042008 \n",
      "\n",
      "Epoch 50\n",
      " -------------------\n",
      "Train loss: 1.174130 [    0/345580]\n",
      "Train loss: 1.084731 [ 3200/345580]\n",
      "Train loss: 1.008623 [ 6400/345580]\n",
      "Train loss: 1.100928 [ 9600/345580]\n",
      "Train loss: 1.077336 [12800/345580]\n",
      "Train loss: 1.033866 [16000/345580]\n",
      "Train loss: 1.122929 [19200/345580]\n",
      "Train loss: 1.042821 [22400/345580]\n",
      "Train loss: 1.091311 [25600/345580]\n",
      "Train loss: 0.971069 [28800/345580]\n",
      "Train loss: 1.119215 [32000/345580]\n",
      "Train loss: 0.963508 [35200/345580]\n",
      "Train loss: 1.090085 [38400/345580]\n",
      "Train loss: 1.021847 [41600/345580]\n",
      "Train loss: 1.154124 [44800/345580]\n",
      "Train loss: 1.063177 [48000/345580]\n",
      "Train loss: 0.936060 [51200/345580]\n",
      "Train loss: 1.154091 [54400/345580]\n",
      "Train loss: 1.110538 [57600/345580]\n",
      "Train loss: 1.028042 [60800/345580]\n",
      "Train loss: 1.129438 [64000/345580]\n",
      "Train loss: 1.106205 [67200/345580]\n",
      "Train loss: 0.923885 [70400/345580]\n",
      "Train loss: 1.038044 [73600/345580]\n",
      "Train loss: 0.994393 [76800/345580]\n",
      "Train loss: 1.169405 [80000/345580]\n",
      "Train loss: 1.093318 [83200/345580]\n",
      "Train loss: 0.992283 [86400/345580]\n",
      "Train loss: 1.039197 [89600/345580]\n",
      "Train loss: 1.018255 [92800/345580]\n",
      "Train loss: 1.027478 [96000/345580]\n",
      "Train loss: 0.961814 [99200/345580]\n",
      "Train loss: 1.063188 [102400/345580]\n",
      "Train loss: 1.066963 [105600/345580]\n",
      "Train loss: 1.091499 [108800/345580]\n",
      "Train loss: 1.017282 [112000/345580]\n",
      "Train loss: 1.076589 [115200/345580]\n",
      "Train loss: 0.919214 [118400/345580]\n",
      "Train loss: 0.963513 [121600/345580]\n",
      "Train loss: 1.020679 [124800/345580]\n",
      "Train loss: 0.995719 [128000/345580]\n",
      "Train loss: 1.097655 [131200/345580]\n",
      "Train loss: 1.108758 [134400/345580]\n",
      "Train loss: 1.082473 [137600/345580]\n",
      "Train loss: 1.077235 [140800/345580]\n",
      "Train loss: 1.057672 [144000/345580]\n",
      "Train loss: 1.075037 [147200/345580]\n",
      "Train loss: 1.100366 [150400/345580]\n",
      "Train loss: 1.095283 [153600/345580]\n",
      "Train loss: 1.093422 [156800/345580]\n",
      "Train loss: 1.115873 [160000/345580]\n",
      "Train loss: 1.118207 [163200/345580]\n",
      "Train loss: 1.058575 [166400/345580]\n",
      "Train loss: 1.055895 [169600/345580]\n",
      "Train loss: 1.089396 [172800/345580]\n",
      "Train loss: 1.178629 [176000/345580]\n",
      "Train loss: 1.030430 [179200/345580]\n",
      "Train loss: 1.050807 [182400/345580]\n",
      "Train loss: 1.013990 [185600/345580]\n",
      "Train loss: 1.003699 [188800/345580]\n",
      "Train loss: 1.080116 [192000/345580]\n",
      "Train loss: 1.081729 [195200/345580]\n",
      "Train loss: 1.038147 [198400/345580]\n",
      "Train loss: 0.999333 [201600/345580]\n",
      "Train loss: 1.070942 [204800/345580]\n",
      "Train loss: 1.028989 [208000/345580]\n",
      "Train loss: 1.103323 [211200/345580]\n",
      "Train loss: 1.038784 [214400/345580]\n",
      "Train loss: 1.057000 [217600/345580]\n",
      "Train loss: 1.068619 [220800/345580]\n",
      "Train loss: 1.052419 [224000/345580]\n",
      "Train loss: 1.028790 [227200/345580]\n",
      "Train loss: 0.955665 [230400/345580]\n",
      "Train loss: 0.992468 [233600/345580]\n",
      "Train loss: 1.102545 [236800/345580]\n",
      "Train loss: 1.036049 [240000/345580]\n",
      "Train loss: 1.032279 [243200/345580]\n",
      "Train loss: 1.080843 [246400/345580]\n",
      "Train loss: 0.963911 [249600/345580]\n",
      "Train loss: 0.956648 [252800/345580]\n",
      "Train loss: 1.040713 [256000/345580]\n",
      "Train loss: 1.044868 [259200/345580]\n",
      "Train loss: 0.987544 [262400/345580]\n",
      "Train loss: 1.021387 [265600/345580]\n",
      "Train loss: 1.100431 [268800/345580]\n",
      "Train loss: 1.046548 [272000/345580]\n",
      "Train loss: 1.044476 [275200/345580]\n",
      "Train loss: 1.158276 [278400/345580]\n",
      "Train loss: 1.069323 [281600/345580]\n",
      "Train loss: 1.006388 [284800/345580]\n",
      "Train loss: 1.126615 [288000/345580]\n",
      "Train loss: 1.036532 [291200/345580]\n",
      "Train loss: 0.947665 [294400/345580]\n",
      "Train loss: 1.044985 [297600/345580]\n",
      "Train loss: 1.110660 [300800/345580]\n",
      "Train loss: 1.043909 [304000/345580]\n",
      "Train loss: 1.110577 [307200/345580]\n",
      "Train loss: 1.021171 [310400/345580]\n",
      "Train loss: 1.022430 [313600/345580]\n",
      "Train loss: 1.123365 [316800/345580]\n",
      "Train loss: 0.990631 [320000/345580]\n",
      "Train loss: 1.103227 [323200/345580]\n",
      "Train loss: 1.026643 [326400/345580]\n",
      "Train loss: 1.117961 [329600/345580]\n",
      "Train loss: 1.097273 [332800/345580]\n",
      "Train loss: 1.093046 [336000/345580]\n",
      "Train loss: 1.030519 [339200/345580]\n",
      "Train loss: 1.115746 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.042084 \n",
      "\n",
      "Epoch 51\n",
      " -------------------\n",
      "Train loss: 0.993619 [    0/345580]\n",
      "Train loss: 1.075032 [ 3200/345580]\n",
      "Train loss: 1.049721 [ 6400/345580]\n",
      "Train loss: 1.030765 [ 9600/345580]\n",
      "Train loss: 1.023231 [12800/345580]\n",
      "Train loss: 1.098204 [16000/345580]\n",
      "Train loss: 1.044834 [19200/345580]\n",
      "Train loss: 1.091018 [22400/345580]\n",
      "Train loss: 1.019917 [25600/345580]\n",
      "Train loss: 1.116107 [28800/345580]\n",
      "Train loss: 1.106409 [32000/345580]\n",
      "Train loss: 1.068100 [35200/345580]\n",
      "Train loss: 1.081457 [38400/345580]\n",
      "Train loss: 1.144642 [41600/345580]\n",
      "Train loss: 1.010103 [44800/345580]\n",
      "Train loss: 1.079871 [48000/345580]\n",
      "Train loss: 0.960728 [51200/345580]\n",
      "Train loss: 1.186263 [54400/345580]\n",
      "Train loss: 1.019275 [57600/345580]\n",
      "Train loss: 0.893807 [60800/345580]\n",
      "Train loss: 1.067842 [64000/345580]\n",
      "Train loss: 1.190663 [67200/345580]\n",
      "Train loss: 0.982434 [70400/345580]\n",
      "Train loss: 1.041252 [73600/345580]\n",
      "Train loss: 0.999577 [76800/345580]\n",
      "Train loss: 1.147412 [80000/345580]\n",
      "Train loss: 1.233193 [83200/345580]\n",
      "Train loss: 1.047009 [86400/345580]\n",
      "Train loss: 1.080674 [89600/345580]\n",
      "Train loss: 1.109655 [92800/345580]\n",
      "Train loss: 1.031145 [96000/345580]\n",
      "Train loss: 0.991237 [99200/345580]\n",
      "Train loss: 1.032448 [102400/345580]\n",
      "Train loss: 0.957254 [105600/345580]\n",
      "Train loss: 1.007842 [108800/345580]\n",
      "Train loss: 1.071290 [112000/345580]\n",
      "Train loss: 0.982424 [115200/345580]\n",
      "Train loss: 1.015260 [118400/345580]\n",
      "Train loss: 1.021346 [121600/345580]\n",
      "Train loss: 1.038806 [124800/345580]\n",
      "Train loss: 1.108990 [128000/345580]\n",
      "Train loss: 1.095954 [131200/345580]\n",
      "Train loss: 0.995826 [134400/345580]\n",
      "Train loss: 1.099956 [137600/345580]\n",
      "Train loss: 1.109377 [140800/345580]\n",
      "Train loss: 1.045946 [144000/345580]\n",
      "Train loss: 1.055393 [147200/345580]\n",
      "Train loss: 1.024291 [150400/345580]\n",
      "Train loss: 0.994732 [153600/345580]\n",
      "Train loss: 1.051413 [156800/345580]\n",
      "Train loss: 1.056534 [160000/345580]\n",
      "Train loss: 0.986747 [163200/345580]\n",
      "Train loss: 0.969532 [166400/345580]\n",
      "Train loss: 1.060831 [169600/345580]\n",
      "Train loss: 0.935958 [172800/345580]\n",
      "Train loss: 1.104554 [176000/345580]\n",
      "Train loss: 1.004716 [179200/345580]\n",
      "Train loss: 0.980439 [182400/345580]\n",
      "Train loss: 1.069538 [185600/345580]\n",
      "Train loss: 1.015351 [188800/345580]\n",
      "Train loss: 1.035483 [192000/345580]\n",
      "Train loss: 1.054299 [195200/345580]\n",
      "Train loss: 1.033825 [198400/345580]\n",
      "Train loss: 1.126601 [201600/345580]\n",
      "Train loss: 1.056664 [204800/345580]\n",
      "Train loss: 1.024847 [208000/345580]\n",
      "Train loss: 1.019116 [211200/345580]\n",
      "Train loss: 1.049441 [214400/345580]\n",
      "Train loss: 1.000712 [217600/345580]\n",
      "Train loss: 1.028734 [220800/345580]\n",
      "Train loss: 1.073321 [224000/345580]\n",
      "Train loss: 0.999592 [227200/345580]\n",
      "Train loss: 1.081306 [230400/345580]\n",
      "Train loss: 1.062219 [233600/345580]\n",
      "Train loss: 1.100179 [236800/345580]\n",
      "Train loss: 1.089256 [240000/345580]\n",
      "Train loss: 0.943443 [243200/345580]\n",
      "Train loss: 0.974171 [246400/345580]\n",
      "Train loss: 1.024463 [249600/345580]\n",
      "Train loss: 1.077511 [252800/345580]\n",
      "Train loss: 1.115499 [256000/345580]\n",
      "Train loss: 0.928769 [259200/345580]\n",
      "Train loss: 0.986300 [262400/345580]\n",
      "Train loss: 0.969190 [265600/345580]\n",
      "Train loss: 0.977103 [268800/345580]\n",
      "Train loss: 1.032008 [272000/345580]\n",
      "Train loss: 1.055029 [275200/345580]\n",
      "Train loss: 1.060817 [278400/345580]\n",
      "Train loss: 1.112045 [281600/345580]\n",
      "Train loss: 0.974001 [284800/345580]\n",
      "Train loss: 0.997735 [288000/345580]\n",
      "Train loss: 0.983547 [291200/345580]\n",
      "Train loss: 1.058958 [294400/345580]\n",
      "Train loss: 1.121051 [297600/345580]\n",
      "Train loss: 0.985452 [300800/345580]\n",
      "Train loss: 0.968134 [304000/345580]\n",
      "Train loss: 1.097188 [307200/345580]\n",
      "Train loss: 1.048382 [310400/345580]\n",
      "Train loss: 0.960051 [313600/345580]\n",
      "Train loss: 1.066911 [316800/345580]\n",
      "Train loss: 1.057857 [320000/345580]\n",
      "Train loss: 1.042362 [323200/345580]\n",
      "Train loss: 1.118685 [326400/345580]\n",
      "Train loss: 0.973209 [329600/345580]\n",
      "Train loss: 1.042091 [332800/345580]\n",
      "Train loss: 1.044459 [336000/345580]\n",
      "Train loss: 1.101193 [339200/345580]\n",
      "Train loss: 1.056973 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 42.8%, Avg loss: 1.041424 \n",
      "\n",
      "Epoch 52\n",
      " -------------------\n",
      "Train loss: 1.023642 [    0/345580]\n",
      "Train loss: 1.040774 [ 3200/345580]\n",
      "Train loss: 0.984079 [ 6400/345580]\n",
      "Train loss: 0.991606 [ 9600/345580]\n",
      "Train loss: 1.045364 [12800/345580]\n",
      "Train loss: 0.980505 [16000/345580]\n",
      "Train loss: 1.018817 [19200/345580]\n",
      "Train loss: 1.042001 [22400/345580]\n",
      "Train loss: 1.092779 [25600/345580]\n",
      "Train loss: 1.012873 [28800/345580]\n",
      "Train loss: 1.041413 [32000/345580]\n",
      "Train loss: 1.015133 [35200/345580]\n",
      "Train loss: 1.173055 [38400/345580]\n",
      "Train loss: 1.050022 [41600/345580]\n",
      "Train loss: 1.051334 [44800/345580]\n",
      "Train loss: 1.085124 [48000/345580]\n",
      "Train loss: 1.089170 [51200/345580]\n",
      "Train loss: 1.186045 [54400/345580]\n",
      "Train loss: 0.991729 [57600/345580]\n",
      "Train loss: 1.058392 [60800/345580]\n",
      "Train loss: 1.000471 [64000/345580]\n",
      "Train loss: 1.028068 [67200/345580]\n",
      "Train loss: 1.113426 [70400/345580]\n",
      "Train loss: 1.068066 [73600/345580]\n",
      "Train loss: 1.037545 [76800/345580]\n",
      "Train loss: 1.119612 [80000/345580]\n",
      "Train loss: 1.113683 [83200/345580]\n",
      "Train loss: 0.980993 [86400/345580]\n",
      "Train loss: 1.059440 [89600/345580]\n",
      "Train loss: 1.099897 [92800/345580]\n",
      "Train loss: 1.061512 [96000/345580]\n",
      "Train loss: 1.057092 [99200/345580]\n",
      "Train loss: 0.919960 [102400/345580]\n",
      "Train loss: 1.129983 [105600/345580]\n",
      "Train loss: 1.053842 [108800/345580]\n",
      "Train loss: 1.059061 [112000/345580]\n",
      "Train loss: 1.020122 [115200/345580]\n",
      "Train loss: 1.127184 [118400/345580]\n",
      "Train loss: 0.989552 [121600/345580]\n",
      "Train loss: 1.059289 [124800/345580]\n",
      "Train loss: 1.010857 [128000/345580]\n",
      "Train loss: 1.035209 [131200/345580]\n",
      "Train loss: 1.060463 [134400/345580]\n",
      "Train loss: 1.046769 [137600/345580]\n",
      "Train loss: 1.018756 [140800/345580]\n",
      "Train loss: 1.018892 [144000/345580]\n",
      "Train loss: 0.988218 [147200/345580]\n",
      "Train loss: 1.046439 [150400/345580]\n",
      "Train loss: 1.027957 [153600/345580]\n",
      "Train loss: 1.095823 [156800/345580]\n",
      "Train loss: 1.116149 [160000/345580]\n",
      "Train loss: 1.069928 [163200/345580]\n",
      "Train loss: 0.994997 [166400/345580]\n",
      "Train loss: 0.973644 [169600/345580]\n",
      "Train loss: 1.019795 [172800/345580]\n",
      "Train loss: 1.041209 [176000/345580]\n",
      "Train loss: 1.121066 [179200/345580]\n",
      "Train loss: 1.034693 [182400/345580]\n",
      "Train loss: 1.052710 [185600/345580]\n",
      "Train loss: 1.084614 [188800/345580]\n",
      "Train loss: 0.972611 [192000/345580]\n",
      "Train loss: 1.043604 [195200/345580]\n",
      "Train loss: 1.141041 [198400/345580]\n",
      "Train loss: 1.006739 [201600/345580]\n",
      "Train loss: 1.042958 [204800/345580]\n",
      "Train loss: 1.020575 [208000/345580]\n",
      "Train loss: 0.980804 [211200/345580]\n",
      "Train loss: 1.096478 [214400/345580]\n",
      "Train loss: 1.079424 [217600/345580]\n",
      "Train loss: 1.072301 [220800/345580]\n",
      "Train loss: 1.049754 [224000/345580]\n",
      "Train loss: 0.959865 [227200/345580]\n",
      "Train loss: 1.018422 [230400/345580]\n",
      "Train loss: 1.067728 [233600/345580]\n",
      "Train loss: 1.000074 [236800/345580]\n",
      "Train loss: 1.093452 [240000/345580]\n",
      "Train loss: 1.079229 [243200/345580]\n",
      "Train loss: 1.015348 [246400/345580]\n",
      "Train loss: 1.053481 [249600/345580]\n",
      "Train loss: 1.021435 [252800/345580]\n",
      "Train loss: 0.977784 [256000/345580]\n",
      "Train loss: 1.015636 [259200/345580]\n",
      "Train loss: 1.066384 [262400/345580]\n",
      "Train loss: 1.035235 [265600/345580]\n",
      "Train loss: 0.976480 [268800/345580]\n",
      "Train loss: 1.120159 [272000/345580]\n",
      "Train loss: 1.070816 [275200/345580]\n",
      "Train loss: 1.012867 [278400/345580]\n",
      "Train loss: 0.997270 [281600/345580]\n",
      "Train loss: 1.014992 [284800/345580]\n",
      "Train loss: 1.036994 [288000/345580]\n",
      "Train loss: 1.077931 [291200/345580]\n",
      "Train loss: 0.995706 [294400/345580]\n",
      "Train loss: 1.028739 [297600/345580]\n",
      "Train loss: 1.054192 [300800/345580]\n",
      "Train loss: 1.042448 [304000/345580]\n",
      "Train loss: 1.034603 [307200/345580]\n",
      "Train loss: 1.016481 [310400/345580]\n",
      "Train loss: 1.012124 [313600/345580]\n",
      "Train loss: 1.131263 [316800/345580]\n",
      "Train loss: 1.070802 [320000/345580]\n",
      "Train loss: 1.072948 [323200/345580]\n",
      "Train loss: 1.020228 [326400/345580]\n",
      "Train loss: 1.276813 [329600/345580]\n",
      "Train loss: 1.065672 [332800/345580]\n",
      "Train loss: 0.978585 [336000/345580]\n",
      "Train loss: 1.077885 [339200/345580]\n",
      "Train loss: 1.017412 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042582 \n",
      "\n",
      "Epoch 53\n",
      " -------------------\n",
      "Train loss: 1.046882 [    0/345580]\n",
      "Train loss: 0.954961 [ 3200/345580]\n",
      "Train loss: 1.003968 [ 6400/345580]\n",
      "Train loss: 0.936088 [ 9600/345580]\n",
      "Train loss: 1.057563 [12800/345580]\n",
      "Train loss: 1.060999 [16000/345580]\n",
      "Train loss: 0.966252 [19200/345580]\n",
      "Train loss: 1.113472 [22400/345580]\n",
      "Train loss: 1.089478 [25600/345580]\n",
      "Train loss: 1.112880 [28800/345580]\n",
      "Train loss: 1.023934 [32000/345580]\n",
      "Train loss: 1.103716 [35200/345580]\n",
      "Train loss: 0.971379 [38400/345580]\n",
      "Train loss: 1.059973 [41600/345580]\n",
      "Train loss: 1.124513 [44800/345580]\n",
      "Train loss: 1.155883 [48000/345580]\n",
      "Train loss: 0.973826 [51200/345580]\n",
      "Train loss: 1.015848 [54400/345580]\n",
      "Train loss: 1.006757 [57600/345580]\n",
      "Train loss: 1.008755 [60800/345580]\n",
      "Train loss: 1.035000 [64000/345580]\n",
      "Train loss: 1.153620 [67200/345580]\n",
      "Train loss: 1.065815 [70400/345580]\n",
      "Train loss: 1.174129 [73600/345580]\n",
      "Train loss: 1.096490 [76800/345580]\n",
      "Train loss: 0.973328 [80000/345580]\n",
      "Train loss: 1.064708 [83200/345580]\n",
      "Train loss: 1.109145 [86400/345580]\n",
      "Train loss: 0.948824 [89600/345580]\n",
      "Train loss: 1.042389 [92800/345580]\n",
      "Train loss: 1.076597 [96000/345580]\n",
      "Train loss: 1.023046 [99200/345580]\n",
      "Train loss: 1.081855 [102400/345580]\n",
      "Train loss: 1.053729 [105600/345580]\n",
      "Train loss: 1.162643 [108800/345580]\n",
      "Train loss: 1.063473 [112000/345580]\n",
      "Train loss: 1.014589 [115200/345580]\n",
      "Train loss: 1.006351 [118400/345580]\n",
      "Train loss: 1.086982 [121600/345580]\n",
      "Train loss: 1.021800 [124800/345580]\n",
      "Train loss: 1.025739 [128000/345580]\n",
      "Train loss: 1.005439 [131200/345580]\n",
      "Train loss: 1.028187 [134400/345580]\n",
      "Train loss: 0.994386 [137600/345580]\n",
      "Train loss: 1.068976 [140800/345580]\n",
      "Train loss: 1.093388 [144000/345580]\n",
      "Train loss: 1.066792 [147200/345580]\n",
      "Train loss: 1.067987 [150400/345580]\n",
      "Train loss: 1.082180 [153600/345580]\n",
      "Train loss: 1.065658 [156800/345580]\n",
      "Train loss: 1.149334 [160000/345580]\n",
      "Train loss: 1.167535 [163200/345580]\n",
      "Train loss: 1.061674 [166400/345580]\n",
      "Train loss: 0.974317 [169600/345580]\n",
      "Train loss: 1.157985 [172800/345580]\n",
      "Train loss: 1.046939 [176000/345580]\n",
      "Train loss: 1.014058 [179200/345580]\n",
      "Train loss: 1.114703 [182400/345580]\n",
      "Train loss: 1.062265 [185600/345580]\n",
      "Train loss: 1.080246 [188800/345580]\n",
      "Train loss: 1.073335 [192000/345580]\n",
      "Train loss: 0.987667 [195200/345580]\n",
      "Train loss: 1.126933 [198400/345580]\n",
      "Train loss: 1.032819 [201600/345580]\n",
      "Train loss: 1.116531 [204800/345580]\n",
      "Train loss: 1.105130 [208000/345580]\n",
      "Train loss: 1.011706 [211200/345580]\n",
      "Train loss: 0.999602 [214400/345580]\n",
      "Train loss: 1.037861 [217600/345580]\n",
      "Train loss: 1.095645 [220800/345580]\n",
      "Train loss: 0.991841 [224000/345580]\n",
      "Train loss: 1.041535 [227200/345580]\n",
      "Train loss: 0.950748 [230400/345580]\n",
      "Train loss: 0.939325 [233600/345580]\n",
      "Train loss: 1.036460 [236800/345580]\n",
      "Train loss: 1.054813 [240000/345580]\n",
      "Train loss: 0.922358 [243200/345580]\n",
      "Train loss: 1.094010 [246400/345580]\n",
      "Train loss: 1.074282 [249600/345580]\n",
      "Train loss: 1.038655 [252800/345580]\n",
      "Train loss: 0.999157 [256000/345580]\n",
      "Train loss: 1.086501 [259200/345580]\n",
      "Train loss: 1.033891 [262400/345580]\n",
      "Train loss: 1.021368 [265600/345580]\n",
      "Train loss: 1.083144 [268800/345580]\n",
      "Train loss: 0.998456 [272000/345580]\n",
      "Train loss: 0.999912 [275200/345580]\n",
      "Train loss: 0.990549 [278400/345580]\n",
      "Train loss: 0.972648 [281600/345580]\n",
      "Train loss: 1.120374 [284800/345580]\n",
      "Train loss: 1.023685 [288000/345580]\n",
      "Train loss: 1.089836 [291200/345580]\n",
      "Train loss: 1.077021 [294400/345580]\n",
      "Train loss: 1.086901 [297600/345580]\n",
      "Train loss: 1.041036 [300800/345580]\n",
      "Train loss: 0.991512 [304000/345580]\n",
      "Train loss: 1.003533 [307200/345580]\n",
      "Train loss: 1.031455 [310400/345580]\n",
      "Train loss: 1.126118 [313600/345580]\n",
      "Train loss: 1.096247 [316800/345580]\n",
      "Train loss: 1.030105 [320000/345580]\n",
      "Train loss: 1.069892 [323200/345580]\n",
      "Train loss: 1.001670 [326400/345580]\n",
      "Train loss: 1.032713 [329600/345580]\n",
      "Train loss: 1.083583 [332800/345580]\n",
      "Train loss: 1.082077 [336000/345580]\n",
      "Train loss: 1.021663 [339200/345580]\n",
      "Train loss: 0.999249 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.041367 \n",
      "\n",
      "Epoch 54\n",
      " -------------------\n",
      "Train loss: 0.953844 [    0/345580]\n",
      "Train loss: 1.131205 [ 3200/345580]\n",
      "Train loss: 0.996908 [ 6400/345580]\n",
      "Train loss: 0.956307 [ 9600/345580]\n",
      "Train loss: 1.035427 [12800/345580]\n",
      "Train loss: 1.120533 [16000/345580]\n",
      "Train loss: 1.082549 [19200/345580]\n",
      "Train loss: 1.057735 [22400/345580]\n",
      "Train loss: 1.045371 [25600/345580]\n",
      "Train loss: 0.993382 [28800/345580]\n",
      "Train loss: 1.129110 [32000/345580]\n",
      "Train loss: 0.960732 [35200/345580]\n",
      "Train loss: 0.983938 [38400/345580]\n",
      "Train loss: 0.990532 [41600/345580]\n",
      "Train loss: 1.142099 [44800/345580]\n",
      "Train loss: 0.979664 [48000/345580]\n",
      "Train loss: 0.942650 [51200/345580]\n",
      "Train loss: 1.138841 [54400/345580]\n",
      "Train loss: 0.982947 [57600/345580]\n",
      "Train loss: 1.164834 [60800/345580]\n",
      "Train loss: 1.019798 [64000/345580]\n",
      "Train loss: 1.018352 [67200/345580]\n",
      "Train loss: 1.076709 [70400/345580]\n",
      "Train loss: 1.092763 [73600/345580]\n",
      "Train loss: 0.990935 [76800/345580]\n",
      "Train loss: 1.002245 [80000/345580]\n",
      "Train loss: 1.078799 [83200/345580]\n",
      "Train loss: 0.998658 [86400/345580]\n",
      "Train loss: 1.024093 [89600/345580]\n",
      "Train loss: 1.079704 [92800/345580]\n",
      "Train loss: 0.966434 [96000/345580]\n",
      "Train loss: 1.039902 [99200/345580]\n",
      "Train loss: 1.035056 [102400/345580]\n",
      "Train loss: 1.031408 [105600/345580]\n",
      "Train loss: 1.081441 [108800/345580]\n",
      "Train loss: 1.023030 [112000/345580]\n",
      "Train loss: 0.933001 [115200/345580]\n",
      "Train loss: 1.068582 [118400/345580]\n",
      "Train loss: 1.025800 [121600/345580]\n",
      "Train loss: 0.990185 [124800/345580]\n",
      "Train loss: 1.074992 [128000/345580]\n",
      "Train loss: 1.131018 [131200/345580]\n",
      "Train loss: 1.107326 [134400/345580]\n",
      "Train loss: 1.074170 [137600/345580]\n",
      "Train loss: 1.049326 [140800/345580]\n",
      "Train loss: 1.039059 [144000/345580]\n",
      "Train loss: 1.053549 [147200/345580]\n",
      "Train loss: 1.036265 [150400/345580]\n",
      "Train loss: 1.171531 [153600/345580]\n",
      "Train loss: 0.993274 [156800/345580]\n",
      "Train loss: 1.053425 [160000/345580]\n",
      "Train loss: 1.020404 [163200/345580]\n",
      "Train loss: 0.973791 [166400/345580]\n",
      "Train loss: 1.045719 [169600/345580]\n",
      "Train loss: 1.185001 [172800/345580]\n",
      "Train loss: 1.079712 [176000/345580]\n",
      "Train loss: 1.038121 [179200/345580]\n",
      "Train loss: 1.120418 [182400/345580]\n",
      "Train loss: 1.043569 [185600/345580]\n",
      "Train loss: 1.028472 [188800/345580]\n",
      "Train loss: 1.015431 [192000/345580]\n",
      "Train loss: 1.088148 [195200/345580]\n",
      "Train loss: 1.096135 [198400/345580]\n",
      "Train loss: 1.067360 [201600/345580]\n",
      "Train loss: 1.007631 [204800/345580]\n",
      "Train loss: 1.029725 [208000/345580]\n",
      "Train loss: 1.085370 [211200/345580]\n",
      "Train loss: 1.024302 [214400/345580]\n",
      "Train loss: 1.090395 [217600/345580]\n",
      "Train loss: 0.984877 [220800/345580]\n",
      "Train loss: 1.117551 [224000/345580]\n",
      "Train loss: 1.143153 [227200/345580]\n",
      "Train loss: 1.056969 [230400/345580]\n",
      "Train loss: 1.095356 [233600/345580]\n",
      "Train loss: 1.054982 [236800/345580]\n",
      "Train loss: 1.043613 [240000/345580]\n",
      "Train loss: 1.031169 [243200/345580]\n",
      "Train loss: 1.003529 [246400/345580]\n",
      "Train loss: 1.112300 [249600/345580]\n",
      "Train loss: 1.074857 [252800/345580]\n",
      "Train loss: 1.136473 [256000/345580]\n",
      "Train loss: 1.061755 [259200/345580]\n",
      "Train loss: 1.135838 [262400/345580]\n",
      "Train loss: 0.985628 [265600/345580]\n",
      "Train loss: 1.028618 [268800/345580]\n",
      "Train loss: 1.036227 [272000/345580]\n",
      "Train loss: 0.974825 [275200/345580]\n",
      "Train loss: 1.057754 [278400/345580]\n",
      "Train loss: 1.025369 [281600/345580]\n",
      "Train loss: 1.080890 [284800/345580]\n",
      "Train loss: 1.007521 [288000/345580]\n",
      "Train loss: 1.024920 [291200/345580]\n",
      "Train loss: 1.072502 [294400/345580]\n",
      "Train loss: 1.122134 [297600/345580]\n",
      "Train loss: 1.050720 [300800/345580]\n",
      "Train loss: 1.126914 [304000/345580]\n",
      "Train loss: 1.033354 [307200/345580]\n",
      "Train loss: 1.019949 [310400/345580]\n",
      "Train loss: 1.036405 [313600/345580]\n",
      "Train loss: 1.086315 [316800/345580]\n",
      "Train loss: 0.986938 [320000/345580]\n",
      "Train loss: 0.982163 [323200/345580]\n",
      "Train loss: 1.086073 [326400/345580]\n",
      "Train loss: 1.052168 [329600/345580]\n",
      "Train loss: 1.008402 [332800/345580]\n",
      "Train loss: 1.121717 [336000/345580]\n",
      "Train loss: 1.056872 [339200/345580]\n",
      "Train loss: 1.127468 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.041491 \n",
      "\n",
      "Epoch 55\n",
      " -------------------\n",
      "Train loss: 0.940879 [    0/345580]\n",
      "Train loss: 1.164032 [ 3200/345580]\n",
      "Train loss: 1.095714 [ 6400/345580]\n",
      "Train loss: 1.035653 [ 9600/345580]\n",
      "Train loss: 1.074258 [12800/345580]\n",
      "Train loss: 1.022463 [16000/345580]\n",
      "Train loss: 1.010929 [19200/345580]\n",
      "Train loss: 1.018993 [22400/345580]\n",
      "Train loss: 0.975037 [25600/345580]\n",
      "Train loss: 0.984060 [28800/345580]\n",
      "Train loss: 1.067507 [32000/345580]\n",
      "Train loss: 1.013638 [35200/345580]\n",
      "Train loss: 1.035783 [38400/345580]\n",
      "Train loss: 1.021584 [41600/345580]\n",
      "Train loss: 1.087829 [44800/345580]\n",
      "Train loss: 0.995894 [48000/345580]\n",
      "Train loss: 1.033801 [51200/345580]\n",
      "Train loss: 0.973056 [54400/345580]\n",
      "Train loss: 1.011562 [57600/345580]\n",
      "Train loss: 1.039689 [60800/345580]\n",
      "Train loss: 1.083844 [64000/345580]\n",
      "Train loss: 1.121692 [67200/345580]\n",
      "Train loss: 1.107863 [70400/345580]\n",
      "Train loss: 0.965677 [73600/345580]\n",
      "Train loss: 0.952720 [76800/345580]\n",
      "Train loss: 1.084435 [80000/345580]\n",
      "Train loss: 1.045979 [83200/345580]\n",
      "Train loss: 1.076758 [86400/345580]\n",
      "Train loss: 1.096020 [89600/345580]\n",
      "Train loss: 0.923953 [92800/345580]\n",
      "Train loss: 1.075786 [96000/345580]\n",
      "Train loss: 1.087613 [99200/345580]\n",
      "Train loss: 1.071168 [102400/345580]\n",
      "Train loss: 1.095498 [105600/345580]\n",
      "Train loss: 1.022362 [108800/345580]\n",
      "Train loss: 1.072664 [112000/345580]\n",
      "Train loss: 1.120242 [115200/345580]\n",
      "Train loss: 0.991180 [118400/345580]\n",
      "Train loss: 1.019884 [121600/345580]\n",
      "Train loss: 1.056626 [124800/345580]\n",
      "Train loss: 1.030613 [128000/345580]\n",
      "Train loss: 1.119088 [131200/345580]\n",
      "Train loss: 1.085674 [134400/345580]\n",
      "Train loss: 1.123894 [137600/345580]\n",
      "Train loss: 1.018474 [140800/345580]\n",
      "Train loss: 1.011259 [144000/345580]\n",
      "Train loss: 1.066083 [147200/345580]\n",
      "Train loss: 0.952469 [150400/345580]\n",
      "Train loss: 0.896868 [153600/345580]\n",
      "Train loss: 1.058373 [156800/345580]\n",
      "Train loss: 0.999144 [160000/345580]\n",
      "Train loss: 1.097775 [163200/345580]\n",
      "Train loss: 0.996335 [166400/345580]\n",
      "Train loss: 0.987074 [169600/345580]\n",
      "Train loss: 0.982850 [172800/345580]\n",
      "Train loss: 1.076772 [176000/345580]\n",
      "Train loss: 1.055825 [179200/345580]\n",
      "Train loss: 0.969621 [182400/345580]\n",
      "Train loss: 1.034166 [185600/345580]\n",
      "Train loss: 1.068713 [188800/345580]\n",
      "Train loss: 1.049237 [192000/345580]\n",
      "Train loss: 1.137839 [195200/345580]\n",
      "Train loss: 1.091152 [198400/345580]\n",
      "Train loss: 0.991790 [201600/345580]\n",
      "Train loss: 1.020975 [204800/345580]\n",
      "Train loss: 1.038735 [208000/345580]\n",
      "Train loss: 0.938628 [211200/345580]\n",
      "Train loss: 1.006291 [214400/345580]\n",
      "Train loss: 1.019016 [217600/345580]\n",
      "Train loss: 1.023806 [220800/345580]\n",
      "Train loss: 1.082504 [224000/345580]\n",
      "Train loss: 0.988166 [227200/345580]\n",
      "Train loss: 1.066437 [230400/345580]\n",
      "Train loss: 0.961274 [233600/345580]\n",
      "Train loss: 1.071293 [236800/345580]\n",
      "Train loss: 1.057634 [240000/345580]\n",
      "Train loss: 1.053306 [243200/345580]\n",
      "Train loss: 1.050710 [246400/345580]\n",
      "Train loss: 1.080639 [249600/345580]\n",
      "Train loss: 1.053482 [252800/345580]\n",
      "Train loss: 1.088005 [256000/345580]\n",
      "Train loss: 1.033183 [259200/345580]\n",
      "Train loss: 1.062348 [262400/345580]\n",
      "Train loss: 1.045411 [265600/345580]\n",
      "Train loss: 1.095218 [268800/345580]\n",
      "Train loss: 0.967883 [272000/345580]\n",
      "Train loss: 1.101787 [275200/345580]\n",
      "Train loss: 1.062611 [278400/345580]\n",
      "Train loss: 0.989346 [281600/345580]\n",
      "Train loss: 0.990367 [284800/345580]\n",
      "Train loss: 1.008732 [288000/345580]\n",
      "Train loss: 1.058776 [291200/345580]\n",
      "Train loss: 0.924671 [294400/345580]\n",
      "Train loss: 0.988199 [297600/345580]\n",
      "Train loss: 1.079186 [300800/345580]\n",
      "Train loss: 1.124181 [304000/345580]\n",
      "Train loss: 1.002362 [307200/345580]\n",
      "Train loss: 1.064520 [310400/345580]\n",
      "Train loss: 1.075406 [313600/345580]\n",
      "Train loss: 1.120047 [316800/345580]\n",
      "Train loss: 1.045026 [320000/345580]\n",
      "Train loss: 1.077482 [323200/345580]\n",
      "Train loss: 1.064286 [326400/345580]\n",
      "Train loss: 1.152686 [329600/345580]\n",
      "Train loss: 1.085714 [332800/345580]\n",
      "Train loss: 1.056976 [336000/345580]\n",
      "Train loss: 1.062137 [339200/345580]\n",
      "Train loss: 1.088234 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.042105 \n",
      "\n",
      "Epoch 56\n",
      " -------------------\n",
      "Train loss: 1.025965 [    0/345580]\n",
      "Train loss: 1.064857 [ 3200/345580]\n",
      "Train loss: 1.069474 [ 6400/345580]\n",
      "Train loss: 1.043750 [ 9600/345580]\n",
      "Train loss: 0.920297 [12800/345580]\n",
      "Train loss: 1.098567 [16000/345580]\n",
      "Train loss: 0.986349 [19200/345580]\n",
      "Train loss: 1.055357 [22400/345580]\n",
      "Train loss: 0.932771 [25600/345580]\n",
      "Train loss: 1.126514 [28800/345580]\n",
      "Train loss: 1.015980 [32000/345580]\n",
      "Train loss: 1.044740 [35200/345580]\n",
      "Train loss: 1.114859 [38400/345580]\n",
      "Train loss: 1.091548 [41600/345580]\n",
      "Train loss: 1.042961 [44800/345580]\n",
      "Train loss: 1.001562 [48000/345580]\n",
      "Train loss: 1.063217 [51200/345580]\n",
      "Train loss: 1.065328 [54400/345580]\n",
      "Train loss: 1.033298 [57600/345580]\n",
      "Train loss: 1.228481 [60800/345580]\n",
      "Train loss: 1.077880 [64000/345580]\n",
      "Train loss: 1.075096 [67200/345580]\n",
      "Train loss: 0.999960 [70400/345580]\n",
      "Train loss: 0.991461 [73600/345580]\n",
      "Train loss: 1.066264 [76800/345580]\n",
      "Train loss: 1.080977 [80000/345580]\n",
      "Train loss: 1.237934 [83200/345580]\n",
      "Train loss: 1.090859 [86400/345580]\n",
      "Train loss: 1.117145 [89600/345580]\n",
      "Train loss: 1.055991 [92800/345580]\n",
      "Train loss: 1.003240 [96000/345580]\n",
      "Train loss: 1.042162 [99200/345580]\n",
      "Train loss: 0.929811 [102400/345580]\n",
      "Train loss: 1.099006 [105600/345580]\n",
      "Train loss: 1.001725 [108800/345580]\n",
      "Train loss: 0.973862 [112000/345580]\n",
      "Train loss: 0.989882 [115200/345580]\n",
      "Train loss: 1.058955 [118400/345580]\n",
      "Train loss: 0.947040 [121600/345580]\n",
      "Train loss: 1.047328 [124800/345580]\n",
      "Train loss: 1.062721 [128000/345580]\n",
      "Train loss: 1.075908 [131200/345580]\n",
      "Train loss: 0.973675 [134400/345580]\n",
      "Train loss: 1.016545 [137600/345580]\n",
      "Train loss: 1.013045 [140800/345580]\n",
      "Train loss: 0.970857 [144000/345580]\n",
      "Train loss: 1.087656 [147200/345580]\n",
      "Train loss: 1.048477 [150400/345580]\n",
      "Train loss: 1.046201 [153600/345580]\n",
      "Train loss: 1.038442 [156800/345580]\n",
      "Train loss: 1.081861 [160000/345580]\n",
      "Train loss: 0.967077 [163200/345580]\n",
      "Train loss: 1.089846 [166400/345580]\n",
      "Train loss: 1.016265 [169600/345580]\n",
      "Train loss: 1.022855 [172800/345580]\n",
      "Train loss: 1.114480 [176000/345580]\n",
      "Train loss: 1.026094 [179200/345580]\n",
      "Train loss: 1.014870 [182400/345580]\n",
      "Train loss: 1.032951 [185600/345580]\n",
      "Train loss: 1.021477 [188800/345580]\n",
      "Train loss: 1.073900 [192000/345580]\n",
      "Train loss: 1.034755 [195200/345580]\n",
      "Train loss: 1.047724 [198400/345580]\n",
      "Train loss: 1.011412 [201600/345580]\n",
      "Train loss: 1.090493 [204800/345580]\n",
      "Train loss: 1.022202 [208000/345580]\n",
      "Train loss: 1.116565 [211200/345580]\n",
      "Train loss: 1.062400 [214400/345580]\n",
      "Train loss: 1.016134 [217600/345580]\n",
      "Train loss: 1.039654 [220800/345580]\n",
      "Train loss: 1.009853 [224000/345580]\n",
      "Train loss: 0.984622 [227200/345580]\n",
      "Train loss: 1.121281 [230400/345580]\n",
      "Train loss: 0.977311 [233600/345580]\n",
      "Train loss: 1.023138 [236800/345580]\n",
      "Train loss: 0.970156 [240000/345580]\n",
      "Train loss: 1.128257 [243200/345580]\n",
      "Train loss: 1.061363 [246400/345580]\n",
      "Train loss: 1.102625 [249600/345580]\n",
      "Train loss: 1.098717 [252800/345580]\n",
      "Train loss: 1.051767 [256000/345580]\n",
      "Train loss: 1.087073 [259200/345580]\n",
      "Train loss: 1.040510 [262400/345580]\n",
      "Train loss: 1.034947 [265600/345580]\n",
      "Train loss: 1.007826 [268800/345580]\n",
      "Train loss: 0.984465 [272000/345580]\n",
      "Train loss: 1.018642 [275200/345580]\n",
      "Train loss: 1.029163 [278400/345580]\n",
      "Train loss: 1.052288 [281600/345580]\n",
      "Train loss: 0.983365 [284800/345580]\n",
      "Train loss: 1.040127 [288000/345580]\n",
      "Train loss: 1.099859 [291200/345580]\n",
      "Train loss: 1.010687 [294400/345580]\n",
      "Train loss: 1.090118 [297600/345580]\n",
      "Train loss: 0.915067 [300800/345580]\n",
      "Train loss: 1.038016 [304000/345580]\n",
      "Train loss: 1.040458 [307200/345580]\n",
      "Train loss: 0.976326 [310400/345580]\n",
      "Train loss: 1.025670 [313600/345580]\n",
      "Train loss: 1.017351 [316800/345580]\n",
      "Train loss: 1.079356 [320000/345580]\n",
      "Train loss: 1.062764 [323200/345580]\n",
      "Train loss: 1.091527 [326400/345580]\n",
      "Train loss: 1.044488 [329600/345580]\n",
      "Train loss: 1.052270 [332800/345580]\n",
      "Train loss: 0.955550 [336000/345580]\n",
      "Train loss: 0.965405 [339200/345580]\n",
      "Train loss: 1.142811 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.041805 \n",
      "\n",
      "Epoch 57\n",
      " -------------------\n",
      "Train loss: 0.966022 [    0/345580]\n",
      "Train loss: 1.130213 [ 3200/345580]\n",
      "Train loss: 1.135945 [ 6400/345580]\n",
      "Train loss: 1.004641 [ 9600/345580]\n",
      "Train loss: 1.100393 [12800/345580]\n",
      "Train loss: 0.979628 [16000/345580]\n",
      "Train loss: 0.972178 [19200/345580]\n",
      "Train loss: 1.027357 [22400/345580]\n",
      "Train loss: 1.182367 [25600/345580]\n",
      "Train loss: 1.026950 [28800/345580]\n",
      "Train loss: 1.083065 [32000/345580]\n",
      "Train loss: 1.052108 [35200/345580]\n",
      "Train loss: 1.035714 [38400/345580]\n",
      "Train loss: 1.005311 [41600/345580]\n",
      "Train loss: 1.122573 [44800/345580]\n",
      "Train loss: 1.043618 [48000/345580]\n",
      "Train loss: 1.079197 [51200/345580]\n",
      "Train loss: 1.074219 [54400/345580]\n",
      "Train loss: 1.045037 [57600/345580]\n",
      "Train loss: 1.117472 [60800/345580]\n",
      "Train loss: 1.087838 [64000/345580]\n",
      "Train loss: 1.008657 [67200/345580]\n",
      "Train loss: 1.042292 [70400/345580]\n",
      "Train loss: 1.039335 [73600/345580]\n",
      "Train loss: 1.060488 [76800/345580]\n",
      "Train loss: 0.983902 [80000/345580]\n",
      "Train loss: 1.056621 [83200/345580]\n",
      "Train loss: 1.112396 [86400/345580]\n",
      "Train loss: 1.074205 [89600/345580]\n",
      "Train loss: 0.997476 [92800/345580]\n",
      "Train loss: 1.005406 [96000/345580]\n",
      "Train loss: 1.155470 [99200/345580]\n",
      "Train loss: 1.115943 [102400/345580]\n",
      "Train loss: 1.092628 [105600/345580]\n",
      "Train loss: 1.043985 [108800/345580]\n",
      "Train loss: 1.095192 [112000/345580]\n",
      "Train loss: 1.117473 [115200/345580]\n",
      "Train loss: 1.114233 [118400/345580]\n",
      "Train loss: 1.002423 [121600/345580]\n",
      "Train loss: 1.070521 [124800/345580]\n",
      "Train loss: 1.117577 [128000/345580]\n",
      "Train loss: 0.936430 [131200/345580]\n",
      "Train loss: 1.061992 [134400/345580]\n",
      "Train loss: 1.064284 [137600/345580]\n",
      "Train loss: 1.059731 [140800/345580]\n",
      "Train loss: 1.010663 [144000/345580]\n",
      "Train loss: 0.981283 [147200/345580]\n",
      "Train loss: 1.053543 [150400/345580]\n",
      "Train loss: 1.035267 [153600/345580]\n",
      "Train loss: 0.968145 [156800/345580]\n",
      "Train loss: 1.080213 [160000/345580]\n",
      "Train loss: 1.086255 [163200/345580]\n",
      "Train loss: 0.926510 [166400/345580]\n",
      "Train loss: 1.005475 [169600/345580]\n",
      "Train loss: 1.067223 [172800/345580]\n",
      "Train loss: 1.009257 [176000/345580]\n",
      "Train loss: 0.982859 [179200/345580]\n",
      "Train loss: 1.030517 [182400/345580]\n",
      "Train loss: 1.134721 [185600/345580]\n",
      "Train loss: 1.073445 [188800/345580]\n",
      "Train loss: 1.103182 [192000/345580]\n",
      "Train loss: 0.992921 [195200/345580]\n",
      "Train loss: 1.127184 [198400/345580]\n",
      "Train loss: 0.948680 [201600/345580]\n",
      "Train loss: 1.086189 [204800/345580]\n",
      "Train loss: 1.045192 [208000/345580]\n",
      "Train loss: 0.928799 [211200/345580]\n",
      "Train loss: 1.119575 [214400/345580]\n",
      "Train loss: 1.167591 [217600/345580]\n",
      "Train loss: 1.064991 [220800/345580]\n",
      "Train loss: 1.054028 [224000/345580]\n",
      "Train loss: 1.055337 [227200/345580]\n",
      "Train loss: 1.078435 [230400/345580]\n",
      "Train loss: 1.105985 [233600/345580]\n",
      "Train loss: 1.048402 [236800/345580]\n",
      "Train loss: 1.002097 [240000/345580]\n",
      "Train loss: 1.308010 [243200/345580]\n",
      "Train loss: 1.174667 [246400/345580]\n",
      "Train loss: 1.045610 [249600/345580]\n",
      "Train loss: 0.956601 [252800/345580]\n",
      "Train loss: 0.957555 [256000/345580]\n",
      "Train loss: 0.930212 [259200/345580]\n",
      "Train loss: 0.975058 [262400/345580]\n",
      "Train loss: 1.021950 [265600/345580]\n",
      "Train loss: 1.050913 [268800/345580]\n",
      "Train loss: 1.210439 [272000/345580]\n",
      "Train loss: 1.095781 [275200/345580]\n",
      "Train loss: 0.941967 [278400/345580]\n",
      "Train loss: 1.059506 [281600/345580]\n",
      "Train loss: 1.087568 [284800/345580]\n",
      "Train loss: 1.046223 [288000/345580]\n",
      "Train loss: 0.985117 [291200/345580]\n",
      "Train loss: 1.177466 [294400/345580]\n",
      "Train loss: 1.036327 [297600/345580]\n",
      "Train loss: 1.052424 [300800/345580]\n",
      "Train loss: 1.014295 [304000/345580]\n",
      "Train loss: 1.086776 [307200/345580]\n",
      "Train loss: 1.059316 [310400/345580]\n",
      "Train loss: 1.134478 [313600/345580]\n",
      "Train loss: 1.060357 [316800/345580]\n",
      "Train loss: 0.928507 [320000/345580]\n",
      "Train loss: 0.966685 [323200/345580]\n",
      "Train loss: 1.017752 [326400/345580]\n",
      "Train loss: 1.073234 [329600/345580]\n",
      "Train loss: 1.012657 [332800/345580]\n",
      "Train loss: 1.007751 [336000/345580]\n",
      "Train loss: 1.053021 [339200/345580]\n",
      "Train loss: 1.064022 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.3%, Avg loss: 1.042479 \n",
      "\n",
      "Epoch 58\n",
      " -------------------\n",
      "Train loss: 0.989319 [    0/345580]\n",
      "Train loss: 1.075969 [ 3200/345580]\n",
      "Train loss: 1.037576 [ 6400/345580]\n",
      "Train loss: 0.987071 [ 9600/345580]\n",
      "Train loss: 0.976527 [12800/345580]\n",
      "Train loss: 1.107419 [16000/345580]\n",
      "Train loss: 1.082686 [19200/345580]\n",
      "Train loss: 1.009464 [22400/345580]\n",
      "Train loss: 0.987721 [25600/345580]\n",
      "Train loss: 1.100647 [28800/345580]\n",
      "Train loss: 1.017838 [32000/345580]\n",
      "Train loss: 1.070179 [35200/345580]\n",
      "Train loss: 1.101398 [38400/345580]\n",
      "Train loss: 0.997685 [41600/345580]\n",
      "Train loss: 1.044591 [44800/345580]\n",
      "Train loss: 1.048566 [48000/345580]\n",
      "Train loss: 1.053204 [51200/345580]\n",
      "Train loss: 1.022432 [54400/345580]\n",
      "Train loss: 1.124233 [57600/345580]\n",
      "Train loss: 1.080736 [60800/345580]\n",
      "Train loss: 1.018303 [64000/345580]\n",
      "Train loss: 0.971536 [67200/345580]\n",
      "Train loss: 1.017337 [70400/345580]\n",
      "Train loss: 0.995916 [73600/345580]\n",
      "Train loss: 1.054590 [76800/345580]\n",
      "Train loss: 1.169911 [80000/345580]\n",
      "Train loss: 0.960600 [83200/345580]\n",
      "Train loss: 1.051411 [86400/345580]\n",
      "Train loss: 1.034209 [89600/345580]\n",
      "Train loss: 0.916020 [92800/345580]\n",
      "Train loss: 1.083759 [96000/345580]\n",
      "Train loss: 0.995916 [99200/345580]\n",
      "Train loss: 0.988902 [102400/345580]\n",
      "Train loss: 1.023205 [105600/345580]\n",
      "Train loss: 1.152667 [108800/345580]\n",
      "Train loss: 1.065074 [112000/345580]\n",
      "Train loss: 0.993908 [115200/345580]\n",
      "Train loss: 0.999768 [118400/345580]\n",
      "Train loss: 1.012161 [121600/345580]\n",
      "Train loss: 1.161762 [124800/345580]\n",
      "Train loss: 1.075347 [128000/345580]\n",
      "Train loss: 1.019232 [131200/345580]\n",
      "Train loss: 0.988524 [134400/345580]\n",
      "Train loss: 1.013204 [137600/345580]\n",
      "Train loss: 1.128541 [140800/345580]\n",
      "Train loss: 1.005139 [144000/345580]\n",
      "Train loss: 1.009037 [147200/345580]\n",
      "Train loss: 1.054095 [150400/345580]\n",
      "Train loss: 1.065009 [153600/345580]\n",
      "Train loss: 0.979431 [156800/345580]\n",
      "Train loss: 1.138752 [160000/345580]\n",
      "Train loss: 1.061534 [163200/345580]\n",
      "Train loss: 0.997274 [166400/345580]\n",
      "Train loss: 1.041508 [169600/345580]\n",
      "Train loss: 1.055536 [172800/345580]\n",
      "Train loss: 1.049101 [176000/345580]\n",
      "Train loss: 1.058021 [179200/345580]\n",
      "Train loss: 0.952667 [182400/345580]\n",
      "Train loss: 1.127047 [185600/345580]\n",
      "Train loss: 1.047783 [188800/345580]\n",
      "Train loss: 1.036290 [192000/345580]\n",
      "Train loss: 1.081598 [195200/345580]\n",
      "Train loss: 1.103420 [198400/345580]\n",
      "Train loss: 1.142222 [201600/345580]\n",
      "Train loss: 1.074265 [204800/345580]\n",
      "Train loss: 1.080793 [208000/345580]\n",
      "Train loss: 1.097839 [211200/345580]\n",
      "Train loss: 1.020743 [214400/345580]\n",
      "Train loss: 1.115991 [217600/345580]\n",
      "Train loss: 1.037480 [220800/345580]\n",
      "Train loss: 1.066969 [224000/345580]\n",
      "Train loss: 1.084692 [227200/345580]\n",
      "Train loss: 1.111400 [230400/345580]\n",
      "Train loss: 1.056343 [233600/345580]\n",
      "Train loss: 1.082797 [236800/345580]\n",
      "Train loss: 0.996371 [240000/345580]\n",
      "Train loss: 1.086218 [243200/345580]\n",
      "Train loss: 1.022447 [246400/345580]\n",
      "Train loss: 1.036279 [249600/345580]\n",
      "Train loss: 1.088242 [252800/345580]\n",
      "Train loss: 1.001661 [256000/345580]\n",
      "Train loss: 1.053680 [259200/345580]\n",
      "Train loss: 0.945000 [262400/345580]\n",
      "Train loss: 0.993544 [265600/345580]\n",
      "Train loss: 1.013573 [268800/345580]\n",
      "Train loss: 1.053971 [272000/345580]\n",
      "Train loss: 1.109658 [275200/345580]\n",
      "Train loss: 1.014865 [278400/345580]\n",
      "Train loss: 1.105381 [281600/345580]\n",
      "Train loss: 1.008956 [284800/345580]\n",
      "Train loss: 1.039721 [288000/345580]\n",
      "Train loss: 1.053083 [291200/345580]\n",
      "Train loss: 1.073476 [294400/345580]\n",
      "Train loss: 0.923848 [297600/345580]\n",
      "Train loss: 1.044420 [300800/345580]\n",
      "Train loss: 1.072996 [304000/345580]\n",
      "Train loss: 1.070579 [307200/345580]\n",
      "Train loss: 1.020892 [310400/345580]\n",
      "Train loss: 0.994130 [313600/345580]\n",
      "Train loss: 1.018645 [316800/345580]\n",
      "Train loss: 1.032837 [320000/345580]\n",
      "Train loss: 1.056393 [323200/345580]\n",
      "Train loss: 1.044588 [326400/345580]\n",
      "Train loss: 1.125218 [329600/345580]\n",
      "Train loss: 1.076318 [332800/345580]\n",
      "Train loss: 1.127114 [336000/345580]\n",
      "Train loss: 1.050700 [339200/345580]\n",
      "Train loss: 0.946837 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.041463 \n",
      "\n",
      "Epoch 59\n",
      " -------------------\n",
      "Train loss: 1.085051 [    0/345580]\n",
      "Train loss: 1.097629 [ 3200/345580]\n",
      "Train loss: 1.100426 [ 6400/345580]\n",
      "Train loss: 0.930408 [ 9600/345580]\n",
      "Train loss: 0.958732 [12800/345580]\n",
      "Train loss: 1.034968 [16000/345580]\n",
      "Train loss: 1.007774 [19200/345580]\n",
      "Train loss: 0.901251 [22400/345580]\n",
      "Train loss: 0.967053 [25600/345580]\n",
      "Train loss: 1.075217 [28800/345580]\n",
      "Train loss: 1.033365 [32000/345580]\n",
      "Train loss: 1.083582 [35200/345580]\n",
      "Train loss: 1.096352 [38400/345580]\n",
      "Train loss: 1.027993 [41600/345580]\n",
      "Train loss: 1.070704 [44800/345580]\n",
      "Train loss: 0.979051 [48000/345580]\n",
      "Train loss: 1.109290 [51200/345580]\n",
      "Train loss: 1.095975 [54400/345580]\n",
      "Train loss: 1.021902 [57600/345580]\n",
      "Train loss: 0.931863 [60800/345580]\n",
      "Train loss: 1.044665 [64000/345580]\n",
      "Train loss: 1.013579 [67200/345580]\n",
      "Train loss: 0.955024 [70400/345580]\n",
      "Train loss: 0.998494 [73600/345580]\n",
      "Train loss: 1.069435 [76800/345580]\n",
      "Train loss: 1.076538 [80000/345580]\n",
      "Train loss: 0.940812 [83200/345580]\n",
      "Train loss: 1.080067 [86400/345580]\n",
      "Train loss: 1.114315 [89600/345580]\n",
      "Train loss: 1.152696 [92800/345580]\n",
      "Train loss: 0.985818 [96000/345580]\n",
      "Train loss: 0.951128 [99200/345580]\n",
      "Train loss: 1.002177 [102400/345580]\n",
      "Train loss: 0.923063 [105600/345580]\n",
      "Train loss: 1.071545 [108800/345580]\n",
      "Train loss: 1.021526 [112000/345580]\n",
      "Train loss: 1.115327 [115200/345580]\n",
      "Train loss: 0.938495 [118400/345580]\n",
      "Train loss: 1.028033 [121600/345580]\n",
      "Train loss: 1.077573 [124800/345580]\n",
      "Train loss: 1.021687 [128000/345580]\n",
      "Train loss: 1.109771 [131200/345580]\n",
      "Train loss: 1.061342 [134400/345580]\n",
      "Train loss: 1.077306 [137600/345580]\n",
      "Train loss: 1.100788 [140800/345580]\n",
      "Train loss: 1.113219 [144000/345580]\n",
      "Train loss: 1.095237 [147200/345580]\n",
      "Train loss: 1.030330 [150400/345580]\n",
      "Train loss: 1.130533 [153600/345580]\n",
      "Train loss: 0.998122 [156800/345580]\n",
      "Train loss: 0.961066 [160000/345580]\n",
      "Train loss: 1.054752 [163200/345580]\n",
      "Train loss: 1.018245 [166400/345580]\n",
      "Train loss: 1.101809 [169600/345580]\n",
      "Train loss: 1.042860 [172800/345580]\n",
      "Train loss: 1.164623 [176000/345580]\n",
      "Train loss: 1.034012 [179200/345580]\n",
      "Train loss: 1.068618 [182400/345580]\n",
      "Train loss: 1.095315 [185600/345580]\n",
      "Train loss: 1.160831 [188800/345580]\n",
      "Train loss: 1.042722 [192000/345580]\n",
      "Train loss: 1.042324 [195200/345580]\n",
      "Train loss: 1.036324 [198400/345580]\n",
      "Train loss: 1.099085 [201600/345580]\n",
      "Train loss: 1.090919 [204800/345580]\n",
      "Train loss: 1.080867 [208000/345580]\n",
      "Train loss: 1.004229 [211200/345580]\n",
      "Train loss: 1.136741 [214400/345580]\n",
      "Train loss: 1.127762 [217600/345580]\n",
      "Train loss: 1.046021 [220800/345580]\n",
      "Train loss: 0.994706 [224000/345580]\n",
      "Train loss: 1.014637 [227200/345580]\n",
      "Train loss: 1.037348 [230400/345580]\n",
      "Train loss: 0.983360 [233600/345580]\n",
      "Train loss: 1.025583 [236800/345580]\n",
      "Train loss: 1.030411 [240000/345580]\n",
      "Train loss: 1.068476 [243200/345580]\n",
      "Train loss: 1.065739 [246400/345580]\n",
      "Train loss: 1.136696 [249600/345580]\n",
      "Train loss: 1.069534 [252800/345580]\n",
      "Train loss: 0.989893 [256000/345580]\n",
      "Train loss: 1.053282 [259200/345580]\n",
      "Train loss: 1.021582 [262400/345580]\n",
      "Train loss: 1.129245 [265600/345580]\n",
      "Train loss: 0.992261 [268800/345580]\n",
      "Train loss: 0.964289 [272000/345580]\n",
      "Train loss: 1.016742 [275200/345580]\n",
      "Train loss: 1.099166 [278400/345580]\n",
      "Train loss: 1.051959 [281600/345580]\n",
      "Train loss: 1.021970 [284800/345580]\n",
      "Train loss: 1.109935 [288000/345580]\n",
      "Train loss: 1.054404 [291200/345580]\n",
      "Train loss: 1.043795 [294400/345580]\n",
      "Train loss: 0.972735 [297600/345580]\n",
      "Train loss: 1.023028 [300800/345580]\n",
      "Train loss: 1.017567 [304000/345580]\n",
      "Train loss: 1.116761 [307200/345580]\n",
      "Train loss: 1.236624 [310400/345580]\n",
      "Train loss: 0.990411 [313600/345580]\n",
      "Train loss: 1.170097 [316800/345580]\n",
      "Train loss: 1.152406 [320000/345580]\n",
      "Train loss: 1.010586 [323200/345580]\n",
      "Train loss: 1.023845 [326400/345580]\n",
      "Train loss: 1.060605 [329600/345580]\n",
      "Train loss: 1.132012 [332800/345580]\n",
      "Train loss: 1.039624 [336000/345580]\n",
      "Train loss: 1.081367 [339200/345580]\n",
      "Train loss: 1.034955 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.041795 \n",
      "\n",
      "Epoch 60\n",
      " -------------------\n",
      "Train loss: 1.097452 [    0/345580]\n",
      "Train loss: 1.005018 [ 3200/345580]\n",
      "Train loss: 1.066576 [ 6400/345580]\n",
      "Train loss: 1.101862 [ 9600/345580]\n",
      "Train loss: 1.072238 [12800/345580]\n",
      "Train loss: 0.945120 [16000/345580]\n",
      "Train loss: 1.000053 [19200/345580]\n",
      "Train loss: 1.031629 [22400/345580]\n",
      "Train loss: 1.080054 [25600/345580]\n",
      "Train loss: 1.048638 [28800/345580]\n",
      "Train loss: 0.940263 [32000/345580]\n",
      "Train loss: 1.000340 [35200/345580]\n",
      "Train loss: 0.954319 [38400/345580]\n",
      "Train loss: 0.961863 [41600/345580]\n",
      "Train loss: 1.081835 [44800/345580]\n",
      "Train loss: 1.121182 [48000/345580]\n",
      "Train loss: 1.070359 [51200/345580]\n",
      "Train loss: 1.135342 [54400/345580]\n",
      "Train loss: 1.121835 [57600/345580]\n",
      "Train loss: 1.041573 [60800/345580]\n",
      "Train loss: 1.050512 [64000/345580]\n",
      "Train loss: 1.036469 [67200/345580]\n",
      "Train loss: 1.001291 [70400/345580]\n",
      "Train loss: 1.019683 [73600/345580]\n",
      "Train loss: 1.065761 [76800/345580]\n",
      "Train loss: 0.956731 [80000/345580]\n",
      "Train loss: 1.083267 [83200/345580]\n",
      "Train loss: 1.028612 [86400/345580]\n",
      "Train loss: 1.012373 [89600/345580]\n",
      "Train loss: 1.035911 [92800/345580]\n",
      "Train loss: 1.020384 [96000/345580]\n",
      "Train loss: 1.014637 [99200/345580]\n",
      "Train loss: 1.066315 [102400/345580]\n",
      "Train loss: 1.044598 [105600/345580]\n",
      "Train loss: 0.979875 [108800/345580]\n",
      "Train loss: 1.123835 [112000/345580]\n",
      "Train loss: 0.991453 [115200/345580]\n",
      "Train loss: 1.051583 [118400/345580]\n",
      "Train loss: 1.047760 [121600/345580]\n",
      "Train loss: 1.049402 [124800/345580]\n",
      "Train loss: 1.078883 [128000/345580]\n",
      "Train loss: 1.036518 [131200/345580]\n",
      "Train loss: 1.039289 [134400/345580]\n",
      "Train loss: 1.090223 [137600/345580]\n",
      "Train loss: 0.980809 [140800/345580]\n",
      "Train loss: 1.094589 [144000/345580]\n",
      "Train loss: 1.017372 [147200/345580]\n",
      "Train loss: 1.048329 [150400/345580]\n",
      "Train loss: 1.006336 [153600/345580]\n",
      "Train loss: 1.003951 [156800/345580]\n",
      "Train loss: 1.156009 [160000/345580]\n",
      "Train loss: 1.045180 [163200/345580]\n",
      "Train loss: 1.035805 [166400/345580]\n",
      "Train loss: 1.009567 [169600/345580]\n",
      "Train loss: 1.034431 [172800/345580]\n",
      "Train loss: 1.068631 [176000/345580]\n",
      "Train loss: 1.084956 [179200/345580]\n",
      "Train loss: 0.986584 [182400/345580]\n",
      "Train loss: 1.010736 [185600/345580]\n",
      "Train loss: 0.934984 [188800/345580]\n",
      "Train loss: 1.072602 [192000/345580]\n",
      "Train loss: 1.038112 [195200/345580]\n",
      "Train loss: 0.927801 [198400/345580]\n",
      "Train loss: 1.190874 [201600/345580]\n",
      "Train loss: 1.083274 [204800/345580]\n",
      "Train loss: 1.040840 [208000/345580]\n",
      "Train loss: 1.066254 [211200/345580]\n",
      "Train loss: 1.046478 [214400/345580]\n",
      "Train loss: 1.083670 [217600/345580]\n",
      "Train loss: 0.985258 [220800/345580]\n",
      "Train loss: 0.979796 [224000/345580]\n",
      "Train loss: 1.045751 [227200/345580]\n",
      "Train loss: 1.051057 [230400/345580]\n",
      "Train loss: 1.002324 [233600/345580]\n",
      "Train loss: 1.089007 [236800/345580]\n",
      "Train loss: 1.006758 [240000/345580]\n",
      "Train loss: 1.027158 [243200/345580]\n",
      "Train loss: 1.104532 [246400/345580]\n",
      "Train loss: 1.010954 [249600/345580]\n",
      "Train loss: 1.013274 [252800/345580]\n",
      "Train loss: 1.011470 [256000/345580]\n",
      "Train loss: 1.031126 [259200/345580]\n",
      "Train loss: 1.094335 [262400/345580]\n",
      "Train loss: 1.040457 [265600/345580]\n",
      "Train loss: 0.933297 [268800/345580]\n",
      "Train loss: 1.091307 [272000/345580]\n",
      "Train loss: 1.061548 [275200/345580]\n",
      "Train loss: 1.083484 [278400/345580]\n",
      "Train loss: 0.889518 [281600/345580]\n",
      "Train loss: 0.987721 [284800/345580]\n",
      "Train loss: 1.007872 [288000/345580]\n",
      "Train loss: 1.049109 [291200/345580]\n",
      "Train loss: 1.019063 [294400/345580]\n",
      "Train loss: 0.999664 [297600/345580]\n",
      "Train loss: 0.968313 [300800/345580]\n",
      "Train loss: 0.927951 [304000/345580]\n",
      "Train loss: 1.132662 [307200/345580]\n",
      "Train loss: 1.012411 [310400/345580]\n",
      "Train loss: 1.035760 [313600/345580]\n",
      "Train loss: 1.002320 [316800/345580]\n",
      "Train loss: 1.072802 [320000/345580]\n",
      "Train loss: 1.055196 [323200/345580]\n",
      "Train loss: 1.016294 [326400/345580]\n",
      "Train loss: 1.032948 [329600/345580]\n",
      "Train loss: 1.036262 [332800/345580]\n",
      "Train loss: 1.082876 [336000/345580]\n",
      "Train loss: 0.987096 [339200/345580]\n",
      "Train loss: 1.033161 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.041593 \n",
      "\n",
      "Epoch 61\n",
      " -------------------\n",
      "Train loss: 1.029188 [    0/345580]\n",
      "Train loss: 1.157382 [ 3200/345580]\n",
      "Train loss: 1.045748 [ 6400/345580]\n",
      "Train loss: 1.030171 [ 9600/345580]\n",
      "Train loss: 1.025871 [12800/345580]\n",
      "Train loss: 1.128674 [16000/345580]\n",
      "Train loss: 1.188203 [19200/345580]\n",
      "Train loss: 1.015262 [22400/345580]\n",
      "Train loss: 1.027579 [25600/345580]\n",
      "Train loss: 1.010023 [28800/345580]\n",
      "Train loss: 1.051686 [32000/345580]\n",
      "Train loss: 1.158004 [35200/345580]\n",
      "Train loss: 1.065740 [38400/345580]\n",
      "Train loss: 1.060809 [41600/345580]\n",
      "Train loss: 1.142789 [44800/345580]\n",
      "Train loss: 1.145490 [48000/345580]\n",
      "Train loss: 0.953510 [51200/345580]\n",
      "Train loss: 1.012417 [54400/345580]\n",
      "Train loss: 1.000479 [57600/345580]\n",
      "Train loss: 1.013689 [60800/345580]\n",
      "Train loss: 1.076286 [64000/345580]\n",
      "Train loss: 0.959073 [67200/345580]\n",
      "Train loss: 1.014066 [70400/345580]\n",
      "Train loss: 1.084159 [73600/345580]\n",
      "Train loss: 0.935231 [76800/345580]\n",
      "Train loss: 1.107989 [80000/345580]\n",
      "Train loss: 1.041476 [83200/345580]\n",
      "Train loss: 1.050087 [86400/345580]\n",
      "Train loss: 1.071679 [89600/345580]\n",
      "Train loss: 1.041721 [92800/345580]\n",
      "Train loss: 1.098165 [96000/345580]\n",
      "Train loss: 1.093979 [99200/345580]\n",
      "Train loss: 1.057094 [102400/345580]\n",
      "Train loss: 1.104675 [105600/345580]\n",
      "Train loss: 0.969086 [108800/345580]\n",
      "Train loss: 1.027169 [112000/345580]\n",
      "Train loss: 1.006986 [115200/345580]\n",
      "Train loss: 1.123772 [118400/345580]\n",
      "Train loss: 0.977011 [121600/345580]\n",
      "Train loss: 1.071771 [124800/345580]\n",
      "Train loss: 1.038314 [128000/345580]\n",
      "Train loss: 1.025884 [131200/345580]\n",
      "Train loss: 1.015541 [134400/345580]\n",
      "Train loss: 1.044607 [137600/345580]\n",
      "Train loss: 0.971788 [140800/345580]\n",
      "Train loss: 1.043622 [144000/345580]\n",
      "Train loss: 1.062072 [147200/345580]\n",
      "Train loss: 1.082395 [150400/345580]\n",
      "Train loss: 0.998831 [153600/345580]\n",
      "Train loss: 1.095553 [156800/345580]\n",
      "Train loss: 1.089902 [160000/345580]\n",
      "Train loss: 1.038390 [163200/345580]\n",
      "Train loss: 1.103323 [166400/345580]\n",
      "Train loss: 1.070915 [169600/345580]\n",
      "Train loss: 0.972346 [172800/345580]\n",
      "Train loss: 1.003796 [176000/345580]\n",
      "Train loss: 1.026974 [179200/345580]\n",
      "Train loss: 0.998476 [182400/345580]\n",
      "Train loss: 1.004266 [185600/345580]\n",
      "Train loss: 1.008703 [188800/345580]\n",
      "Train loss: 1.015700 [192000/345580]\n",
      "Train loss: 1.122732 [195200/345580]\n",
      "Train loss: 0.942791 [198400/345580]\n",
      "Train loss: 1.000352 [201600/345580]\n",
      "Train loss: 0.978231 [204800/345580]\n",
      "Train loss: 1.142881 [208000/345580]\n",
      "Train loss: 1.091902 [211200/345580]\n",
      "Train loss: 1.048780 [214400/345580]\n",
      "Train loss: 0.999865 [217600/345580]\n",
      "Train loss: 0.996768 [220800/345580]\n",
      "Train loss: 0.939405 [224000/345580]\n",
      "Train loss: 1.132943 [227200/345580]\n",
      "Train loss: 1.075490 [230400/345580]\n",
      "Train loss: 0.925232 [233600/345580]\n",
      "Train loss: 1.049341 [236800/345580]\n",
      "Train loss: 1.040530 [240000/345580]\n",
      "Train loss: 0.978183 [243200/345580]\n",
      "Train loss: 1.068299 [246400/345580]\n",
      "Train loss: 1.104534 [249600/345580]\n",
      "Train loss: 1.014875 [252800/345580]\n",
      "Train loss: 1.115375 [256000/345580]\n",
      "Train loss: 1.032936 [259200/345580]\n",
      "Train loss: 1.021469 [262400/345580]\n",
      "Train loss: 1.150289 [265600/345580]\n",
      "Train loss: 1.118164 [268800/345580]\n",
      "Train loss: 0.997316 [272000/345580]\n",
      "Train loss: 0.984346 [275200/345580]\n",
      "Train loss: 0.975089 [278400/345580]\n",
      "Train loss: 1.066259 [281600/345580]\n",
      "Train loss: 1.006829 [284800/345580]\n",
      "Train loss: 0.990979 [288000/345580]\n",
      "Train loss: 0.969414 [291200/345580]\n",
      "Train loss: 1.084474 [294400/345580]\n",
      "Train loss: 1.108685 [297600/345580]\n",
      "Train loss: 0.996680 [300800/345580]\n",
      "Train loss: 1.023041 [304000/345580]\n",
      "Train loss: 0.941526 [307200/345580]\n",
      "Train loss: 1.024405 [310400/345580]\n",
      "Train loss: 1.235353 [313600/345580]\n",
      "Train loss: 1.067067 [316800/345580]\n",
      "Train loss: 1.054436 [320000/345580]\n",
      "Train loss: 1.019339 [323200/345580]\n",
      "Train loss: 1.045176 [326400/345580]\n",
      "Train loss: 1.059186 [329600/345580]\n",
      "Train loss: 1.063101 [332800/345580]\n",
      "Train loss: 1.086544 [336000/345580]\n",
      "Train loss: 1.089508 [339200/345580]\n",
      "Train loss: 1.154007 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 42.8%, Avg loss: 1.042311 \n",
      "\n",
      "Epoch 62\n",
      " -------------------\n",
      "Train loss: 1.034600 [    0/345580]\n",
      "Train loss: 1.013538 [ 3200/345580]\n",
      "Train loss: 0.967508 [ 6400/345580]\n",
      "Train loss: 1.076718 [ 9600/345580]\n",
      "Train loss: 1.066564 [12800/345580]\n",
      "Train loss: 1.088010 [16000/345580]\n",
      "Train loss: 1.021017 [19200/345580]\n",
      "Train loss: 1.052524 [22400/345580]\n",
      "Train loss: 1.153401 [25600/345580]\n",
      "Train loss: 0.988050 [28800/345580]\n",
      "Train loss: 1.156443 [32000/345580]\n",
      "Train loss: 0.989311 [35200/345580]\n",
      "Train loss: 1.030560 [38400/345580]\n",
      "Train loss: 1.062040 [41600/345580]\n",
      "Train loss: 1.009933 [44800/345580]\n",
      "Train loss: 1.135340 [48000/345580]\n",
      "Train loss: 1.001315 [51200/345580]\n",
      "Train loss: 1.076993 [54400/345580]\n",
      "Train loss: 1.128092 [57600/345580]\n",
      "Train loss: 1.050065 [60800/345580]\n",
      "Train loss: 1.080386 [64000/345580]\n",
      "Train loss: 1.049320 [67200/345580]\n",
      "Train loss: 1.106057 [70400/345580]\n",
      "Train loss: 1.098488 [73600/345580]\n",
      "Train loss: 1.043867 [76800/345580]\n",
      "Train loss: 1.134333 [80000/345580]\n",
      "Train loss: 0.996928 [83200/345580]\n",
      "Train loss: 1.047202 [86400/345580]\n",
      "Train loss: 1.031073 [89600/345580]\n",
      "Train loss: 1.071788 [92800/345580]\n",
      "Train loss: 1.046897 [96000/345580]\n",
      "Train loss: 1.100823 [99200/345580]\n",
      "Train loss: 0.937014 [102400/345580]\n",
      "Train loss: 1.045587 [105600/345580]\n",
      "Train loss: 1.084030 [108800/345580]\n",
      "Train loss: 0.961615 [112000/345580]\n",
      "Train loss: 1.117805 [115200/345580]\n",
      "Train loss: 1.056326 [118400/345580]\n",
      "Train loss: 1.029387 [121600/345580]\n",
      "Train loss: 1.081507 [124800/345580]\n",
      "Train loss: 0.926586 [128000/345580]\n",
      "Train loss: 1.192597 [131200/345580]\n",
      "Train loss: 1.016678 [134400/345580]\n",
      "Train loss: 1.000536 [137600/345580]\n",
      "Train loss: 1.008138 [140800/345580]\n",
      "Train loss: 1.036448 [144000/345580]\n",
      "Train loss: 0.983322 [147200/345580]\n",
      "Train loss: 0.933960 [150400/345580]\n",
      "Train loss: 1.106123 [153600/345580]\n",
      "Train loss: 1.114557 [156800/345580]\n",
      "Train loss: 1.111442 [160000/345580]\n",
      "Train loss: 1.073575 [163200/345580]\n",
      "Train loss: 1.051015 [166400/345580]\n",
      "Train loss: 0.999890 [169600/345580]\n",
      "Train loss: 1.041658 [172800/345580]\n",
      "Train loss: 1.006681 [176000/345580]\n",
      "Train loss: 0.954265 [179200/345580]\n",
      "Train loss: 1.036325 [182400/345580]\n",
      "Train loss: 1.065577 [185600/345580]\n",
      "Train loss: 1.029531 [188800/345580]\n",
      "Train loss: 1.068213 [192000/345580]\n",
      "Train loss: 1.102091 [195200/345580]\n",
      "Train loss: 1.021711 [198400/345580]\n",
      "Train loss: 0.927004 [201600/345580]\n",
      "Train loss: 1.125818 [204800/345580]\n",
      "Train loss: 0.998236 [208000/345580]\n",
      "Train loss: 1.022442 [211200/345580]\n",
      "Train loss: 0.976497 [214400/345580]\n",
      "Train loss: 0.960006 [217600/345580]\n",
      "Train loss: 0.977829 [220800/345580]\n",
      "Train loss: 0.943261 [224000/345580]\n",
      "Train loss: 1.073895 [227200/345580]\n",
      "Train loss: 1.024514 [230400/345580]\n",
      "Train loss: 1.100307 [233600/345580]\n",
      "Train loss: 1.077922 [236800/345580]\n",
      "Train loss: 0.987337 [240000/345580]\n",
      "Train loss: 1.015509 [243200/345580]\n",
      "Train loss: 1.109333 [246400/345580]\n",
      "Train loss: 1.091076 [249600/345580]\n",
      "Train loss: 1.075840 [252800/345580]\n",
      "Train loss: 1.044950 [256000/345580]\n",
      "Train loss: 1.018232 [259200/345580]\n",
      "Train loss: 1.044538 [262400/345580]\n",
      "Train loss: 1.074494 [265600/345580]\n",
      "Train loss: 1.015475 [268800/345580]\n",
      "Train loss: 1.005073 [272000/345580]\n",
      "Train loss: 1.019240 [275200/345580]\n",
      "Train loss: 1.034396 [278400/345580]\n",
      "Train loss: 1.039968 [281600/345580]\n",
      "Train loss: 1.050946 [284800/345580]\n",
      "Train loss: 1.041486 [288000/345580]\n",
      "Train loss: 0.965662 [291200/345580]\n",
      "Train loss: 0.949210 [294400/345580]\n",
      "Train loss: 1.116723 [297600/345580]\n",
      "Train loss: 1.046739 [300800/345580]\n",
      "Train loss: 1.010346 [304000/345580]\n",
      "Train loss: 1.090438 [307200/345580]\n",
      "Train loss: 1.106941 [310400/345580]\n",
      "Train loss: 1.036221 [313600/345580]\n",
      "Train loss: 1.048883 [316800/345580]\n",
      "Train loss: 1.096191 [320000/345580]\n",
      "Train loss: 0.962850 [323200/345580]\n",
      "Train loss: 1.090669 [326400/345580]\n",
      "Train loss: 1.095278 [329600/345580]\n",
      "Train loss: 1.026814 [332800/345580]\n",
      "Train loss: 1.021145 [336000/345580]\n",
      "Train loss: 1.101989 [339200/345580]\n",
      "Train loss: 1.005336 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.041777 \n",
      "\n",
      "Epoch 63\n",
      " -------------------\n",
      "Train loss: 0.985500 [    0/345580]\n",
      "Train loss: 0.942617 [ 3200/345580]\n",
      "Train loss: 1.038474 [ 6400/345580]\n",
      "Train loss: 0.982852 [ 9600/345580]\n",
      "Train loss: 1.080034 [12800/345580]\n",
      "Train loss: 0.993151 [16000/345580]\n",
      "Train loss: 1.074205 [19200/345580]\n",
      "Train loss: 1.095858 [22400/345580]\n",
      "Train loss: 1.097466 [25600/345580]\n",
      "Train loss: 1.007529 [28800/345580]\n",
      "Train loss: 1.125637 [32000/345580]\n",
      "Train loss: 1.041212 [35200/345580]\n",
      "Train loss: 1.029019 [38400/345580]\n",
      "Train loss: 1.003451 [41600/345580]\n",
      "Train loss: 1.045297 [44800/345580]\n",
      "Train loss: 1.108014 [48000/345580]\n",
      "Train loss: 1.000422 [51200/345580]\n",
      "Train loss: 1.095443 [54400/345580]\n",
      "Train loss: 1.089153 [57600/345580]\n",
      "Train loss: 1.044852 [60800/345580]\n",
      "Train loss: 0.988879 [64000/345580]\n",
      "Train loss: 1.012996 [67200/345580]\n",
      "Train loss: 1.053234 [70400/345580]\n",
      "Train loss: 1.036779 [73600/345580]\n",
      "Train loss: 1.012631 [76800/345580]\n",
      "Train loss: 1.095315 [80000/345580]\n",
      "Train loss: 1.061403 [83200/345580]\n",
      "Train loss: 1.020517 [86400/345580]\n",
      "Train loss: 0.944253 [89600/345580]\n",
      "Train loss: 0.998434 [92800/345580]\n",
      "Train loss: 1.046964 [96000/345580]\n",
      "Train loss: 1.000178 [99200/345580]\n",
      "Train loss: 0.912551 [102400/345580]\n",
      "Train loss: 1.004908 [105600/345580]\n",
      "Train loss: 1.103930 [108800/345580]\n",
      "Train loss: 1.100264 [112000/345580]\n",
      "Train loss: 1.166492 [115200/345580]\n",
      "Train loss: 1.028749 [118400/345580]\n",
      "Train loss: 1.075415 [121600/345580]\n",
      "Train loss: 1.043817 [124800/345580]\n",
      "Train loss: 0.984458 [128000/345580]\n",
      "Train loss: 1.058691 [131200/345580]\n",
      "Train loss: 1.021820 [134400/345580]\n",
      "Train loss: 1.022362 [137600/345580]\n",
      "Train loss: 1.031695 [140800/345580]\n",
      "Train loss: 0.994586 [144000/345580]\n",
      "Train loss: 1.006874 [147200/345580]\n",
      "Train loss: 0.968332 [150400/345580]\n",
      "Train loss: 1.165916 [153600/345580]\n",
      "Train loss: 0.955283 [156800/345580]\n",
      "Train loss: 1.105313 [160000/345580]\n",
      "Train loss: 1.159581 [163200/345580]\n",
      "Train loss: 1.049448 [166400/345580]\n",
      "Train loss: 1.052987 [169600/345580]\n",
      "Train loss: 1.080765 [172800/345580]\n",
      "Train loss: 1.050741 [176000/345580]\n",
      "Train loss: 0.973625 [179200/345580]\n",
      "Train loss: 1.141255 [182400/345580]\n",
      "Train loss: 1.056398 [185600/345580]\n",
      "Train loss: 1.122263 [188800/345580]\n",
      "Train loss: 1.031306 [192000/345580]\n",
      "Train loss: 1.068638 [195200/345580]\n",
      "Train loss: 1.049898 [198400/345580]\n",
      "Train loss: 1.051447 [201600/345580]\n",
      "Train loss: 1.043462 [204800/345580]\n",
      "Train loss: 1.092642 [208000/345580]\n",
      "Train loss: 0.989652 [211200/345580]\n",
      "Train loss: 1.068633 [214400/345580]\n",
      "Train loss: 1.094671 [217600/345580]\n",
      "Train loss: 0.928448 [220800/345580]\n",
      "Train loss: 0.950236 [224000/345580]\n",
      "Train loss: 0.970431 [227200/345580]\n",
      "Train loss: 1.024392 [230400/345580]\n",
      "Train loss: 1.036773 [233600/345580]\n",
      "Train loss: 1.019963 [236800/345580]\n",
      "Train loss: 1.051486 [240000/345580]\n",
      "Train loss: 1.110830 [243200/345580]\n",
      "Train loss: 0.991042 [246400/345580]\n",
      "Train loss: 1.049830 [249600/345580]\n",
      "Train loss: 1.049937 [252800/345580]\n",
      "Train loss: 0.989283 [256000/345580]\n",
      "Train loss: 0.994841 [259200/345580]\n",
      "Train loss: 0.966080 [262400/345580]\n",
      "Train loss: 1.076438 [265600/345580]\n",
      "Train loss: 1.058377 [268800/345580]\n",
      "Train loss: 1.109315 [272000/345580]\n",
      "Train loss: 1.011186 [275200/345580]\n",
      "Train loss: 1.017206 [278400/345580]\n",
      "Train loss: 1.062927 [281600/345580]\n",
      "Train loss: 1.038352 [284800/345580]\n",
      "Train loss: 1.045923 [288000/345580]\n",
      "Train loss: 0.994212 [291200/345580]\n",
      "Train loss: 1.004361 [294400/345580]\n",
      "Train loss: 1.076100 [297600/345580]\n",
      "Train loss: 0.966209 [300800/345580]\n",
      "Train loss: 1.003391 [304000/345580]\n",
      "Train loss: 1.012310 [307200/345580]\n",
      "Train loss: 1.040838 [310400/345580]\n",
      "Train loss: 1.087741 [313600/345580]\n",
      "Train loss: 1.031770 [316800/345580]\n",
      "Train loss: 1.083753 [320000/345580]\n",
      "Train loss: 1.052378 [323200/345580]\n",
      "Train loss: 1.035800 [326400/345580]\n",
      "Train loss: 1.171903 [329600/345580]\n",
      "Train loss: 1.050367 [332800/345580]\n",
      "Train loss: 1.044365 [336000/345580]\n",
      "Train loss: 0.990290 [339200/345580]\n",
      "Train loss: 0.993967 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.041340 \n",
      "\n",
      "Epoch 64\n",
      " -------------------\n",
      "Train loss: 0.964897 [    0/345580]\n",
      "Train loss: 1.040650 [ 3200/345580]\n",
      "Train loss: 1.045962 [ 6400/345580]\n",
      "Train loss: 1.063799 [ 9600/345580]\n",
      "Train loss: 1.047736 [12800/345580]\n",
      "Train loss: 1.057698 [16000/345580]\n",
      "Train loss: 0.953168 [19200/345580]\n",
      "Train loss: 0.958929 [22400/345580]\n",
      "Train loss: 1.010095 [25600/345580]\n",
      "Train loss: 1.020291 [28800/345580]\n",
      "Train loss: 0.987877 [32000/345580]\n",
      "Train loss: 1.171664 [35200/345580]\n",
      "Train loss: 1.122004 [38400/345580]\n",
      "Train loss: 1.020601 [41600/345580]\n",
      "Train loss: 1.174515 [44800/345580]\n",
      "Train loss: 1.093020 [48000/345580]\n",
      "Train loss: 1.058747 [51200/345580]\n",
      "Train loss: 1.114386 [54400/345580]\n",
      "Train loss: 1.064494 [57600/345580]\n",
      "Train loss: 1.086019 [60800/345580]\n",
      "Train loss: 1.047894 [64000/345580]\n",
      "Train loss: 1.089936 [67200/345580]\n",
      "Train loss: 1.095311 [70400/345580]\n",
      "Train loss: 1.053080 [73600/345580]\n",
      "Train loss: 0.947420 [76800/345580]\n",
      "Train loss: 0.975689 [80000/345580]\n",
      "Train loss: 1.116334 [83200/345580]\n",
      "Train loss: 1.061762 [86400/345580]\n",
      "Train loss: 1.135778 [89600/345580]\n",
      "Train loss: 0.990695 [92800/345580]\n",
      "Train loss: 1.028181 [96000/345580]\n",
      "Train loss: 1.131500 [99200/345580]\n",
      "Train loss: 1.107814 [102400/345580]\n",
      "Train loss: 1.068507 [105600/345580]\n",
      "Train loss: 0.962979 [108800/345580]\n",
      "Train loss: 0.949427 [112000/345580]\n",
      "Train loss: 0.989556 [115200/345580]\n",
      "Train loss: 1.069679 [118400/345580]\n",
      "Train loss: 1.079602 [121600/345580]\n",
      "Train loss: 0.913359 [124800/345580]\n",
      "Train loss: 1.106283 [128000/345580]\n",
      "Train loss: 1.021883 [131200/345580]\n",
      "Train loss: 1.078960 [134400/345580]\n",
      "Train loss: 1.040095 [137600/345580]\n",
      "Train loss: 1.052616 [140800/345580]\n",
      "Train loss: 0.983386 [144000/345580]\n",
      "Train loss: 0.934194 [147200/345580]\n",
      "Train loss: 1.043483 [150400/345580]\n",
      "Train loss: 1.091074 [153600/345580]\n",
      "Train loss: 1.057307 [156800/345580]\n",
      "Train loss: 1.082671 [160000/345580]\n",
      "Train loss: 1.015526 [163200/345580]\n",
      "Train loss: 1.087082 [166400/345580]\n",
      "Train loss: 1.099416 [169600/345580]\n",
      "Train loss: 1.011339 [172800/345580]\n",
      "Train loss: 1.062255 [176000/345580]\n",
      "Train loss: 1.044834 [179200/345580]\n",
      "Train loss: 1.089129 [182400/345580]\n",
      "Train loss: 1.089890 [185600/345580]\n",
      "Train loss: 1.085407 [188800/345580]\n",
      "Train loss: 1.097723 [192000/345580]\n",
      "Train loss: 1.143784 [195200/345580]\n",
      "Train loss: 1.130782 [198400/345580]\n",
      "Train loss: 0.891735 [201600/345580]\n",
      "Train loss: 0.963403 [204800/345580]\n",
      "Train loss: 0.936411 [208000/345580]\n",
      "Train loss: 0.956446 [211200/345580]\n",
      "Train loss: 1.112900 [214400/345580]\n",
      "Train loss: 1.088169 [217600/345580]\n",
      "Train loss: 0.981705 [220800/345580]\n",
      "Train loss: 1.057099 [224000/345580]\n",
      "Train loss: 1.061452 [227200/345580]\n",
      "Train loss: 0.999888 [230400/345580]\n",
      "Train loss: 1.030837 [233600/345580]\n",
      "Train loss: 1.011518 [236800/345580]\n",
      "Train loss: 1.087031 [240000/345580]\n",
      "Train loss: 1.095933 [243200/345580]\n",
      "Train loss: 1.013242 [246400/345580]\n",
      "Train loss: 1.090423 [249600/345580]\n",
      "Train loss: 0.955645 [252800/345580]\n",
      "Train loss: 1.016674 [256000/345580]\n",
      "Train loss: 0.991440 [259200/345580]\n",
      "Train loss: 1.045577 [262400/345580]\n",
      "Train loss: 1.075186 [265600/345580]\n",
      "Train loss: 1.040743 [268800/345580]\n",
      "Train loss: 1.019876 [272000/345580]\n",
      "Train loss: 0.999431 [275200/345580]\n",
      "Train loss: 0.988729 [278400/345580]\n",
      "Train loss: 1.031311 [281600/345580]\n",
      "Train loss: 1.048995 [284800/345580]\n",
      "Train loss: 1.081142 [288000/345580]\n",
      "Train loss: 1.066947 [291200/345580]\n",
      "Train loss: 1.022748 [294400/345580]\n",
      "Train loss: 1.010363 [297600/345580]\n",
      "Train loss: 0.972286 [300800/345580]\n",
      "Train loss: 1.057353 [304000/345580]\n",
      "Train loss: 0.988123 [307200/345580]\n",
      "Train loss: 1.110651 [310400/345580]\n",
      "Train loss: 1.008242 [313600/345580]\n",
      "Train loss: 1.047417 [316800/345580]\n",
      "Train loss: 1.101814 [320000/345580]\n",
      "Train loss: 1.036840 [323200/345580]\n",
      "Train loss: 1.020625 [326400/345580]\n",
      "Train loss: 1.053609 [329600/345580]\n",
      "Train loss: 0.947790 [332800/345580]\n",
      "Train loss: 1.079270 [336000/345580]\n",
      "Train loss: 1.029228 [339200/345580]\n",
      "Train loss: 1.046794 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042046 \n",
      "\n",
      "Epoch 65\n",
      " -------------------\n",
      "Train loss: 1.002906 [    0/345580]\n",
      "Train loss: 1.097470 [ 3200/345580]\n",
      "Train loss: 1.070428 [ 6400/345580]\n",
      "Train loss: 1.042050 [ 9600/345580]\n",
      "Train loss: 1.033768 [12800/345580]\n",
      "Train loss: 1.004201 [16000/345580]\n",
      "Train loss: 1.023481 [19200/345580]\n",
      "Train loss: 1.008790 [22400/345580]\n",
      "Train loss: 1.053261 [25600/345580]\n",
      "Train loss: 0.980213 [28800/345580]\n",
      "Train loss: 1.052005 [32000/345580]\n",
      "Train loss: 1.143353 [35200/345580]\n",
      "Train loss: 1.019846 [38400/345580]\n",
      "Train loss: 1.059744 [41600/345580]\n",
      "Train loss: 1.029078 [44800/345580]\n",
      "Train loss: 1.075006 [48000/345580]\n",
      "Train loss: 1.003841 [51200/345580]\n",
      "Train loss: 1.038984 [54400/345580]\n",
      "Train loss: 1.044654 [57600/345580]\n",
      "Train loss: 1.002181 [60800/345580]\n",
      "Train loss: 1.086358 [64000/345580]\n",
      "Train loss: 1.051118 [67200/345580]\n",
      "Train loss: 1.120022 [70400/345580]\n",
      "Train loss: 1.087742 [73600/345580]\n",
      "Train loss: 1.038685 [76800/345580]\n",
      "Train loss: 0.992061 [80000/345580]\n",
      "Train loss: 1.065841 [83200/345580]\n",
      "Train loss: 0.979896 [86400/345580]\n",
      "Train loss: 0.961022 [89600/345580]\n",
      "Train loss: 1.043501 [92800/345580]\n",
      "Train loss: 1.081376 [96000/345580]\n",
      "Train loss: 1.041147 [99200/345580]\n",
      "Train loss: 1.076868 [102400/345580]\n",
      "Train loss: 1.104359 [105600/345580]\n",
      "Train loss: 1.103290 [108800/345580]\n",
      "Train loss: 1.110490 [112000/345580]\n",
      "Train loss: 1.035232 [115200/345580]\n",
      "Train loss: 1.073569 [118400/345580]\n",
      "Train loss: 1.088669 [121600/345580]\n",
      "Train loss: 1.168517 [124800/345580]\n",
      "Train loss: 0.986750 [128000/345580]\n",
      "Train loss: 0.978092 [131200/345580]\n",
      "Train loss: 1.155022 [134400/345580]\n",
      "Train loss: 1.089741 [137600/345580]\n",
      "Train loss: 1.098793 [140800/345580]\n",
      "Train loss: 1.064560 [144000/345580]\n",
      "Train loss: 1.012086 [147200/345580]\n",
      "Train loss: 1.048013 [150400/345580]\n",
      "Train loss: 1.076139 [153600/345580]\n",
      "Train loss: 1.001488 [156800/345580]\n",
      "Train loss: 1.041258 [160000/345580]\n",
      "Train loss: 1.055301 [163200/345580]\n",
      "Train loss: 1.208203 [166400/345580]\n",
      "Train loss: 0.993313 [169600/345580]\n",
      "Train loss: 1.061317 [172800/345580]\n",
      "Train loss: 1.044852 [176000/345580]\n",
      "Train loss: 1.171840 [179200/345580]\n",
      "Train loss: 1.108484 [182400/345580]\n",
      "Train loss: 1.028901 [185600/345580]\n",
      "Train loss: 0.979819 [188800/345580]\n",
      "Train loss: 1.011018 [192000/345580]\n",
      "Train loss: 1.014054 [195200/345580]\n",
      "Train loss: 1.114340 [198400/345580]\n",
      "Train loss: 1.080634 [201600/345580]\n",
      "Train loss: 1.082443 [204800/345580]\n",
      "Train loss: 1.034900 [208000/345580]\n",
      "Train loss: 1.043306 [211200/345580]\n",
      "Train loss: 1.019621 [214400/345580]\n",
      "Train loss: 0.991994 [217600/345580]\n",
      "Train loss: 1.044781 [220800/345580]\n",
      "Train loss: 1.033050 [224000/345580]\n",
      "Train loss: 1.075251 [227200/345580]\n",
      "Train loss: 1.003941 [230400/345580]\n",
      "Train loss: 1.052875 [233600/345580]\n",
      "Train loss: 1.126893 [236800/345580]\n",
      "Train loss: 1.100847 [240000/345580]\n",
      "Train loss: 1.014316 [243200/345580]\n",
      "Train loss: 1.071864 [246400/345580]\n",
      "Train loss: 1.082869 [249600/345580]\n",
      "Train loss: 1.128593 [252800/345580]\n",
      "Train loss: 0.994495 [256000/345580]\n",
      "Train loss: 1.068893 [259200/345580]\n",
      "Train loss: 1.020567 [262400/345580]\n",
      "Train loss: 1.007990 [265600/345580]\n",
      "Train loss: 1.023789 [268800/345580]\n",
      "Train loss: 1.050237 [272000/345580]\n",
      "Train loss: 1.019515 [275200/345580]\n",
      "Train loss: 1.008506 [278400/345580]\n",
      "Train loss: 0.965053 [281600/345580]\n",
      "Train loss: 1.069219 [284800/345580]\n",
      "Train loss: 1.010308 [288000/345580]\n",
      "Train loss: 1.008372 [291200/345580]\n",
      "Train loss: 1.008262 [294400/345580]\n",
      "Train loss: 1.015138 [297600/345580]\n",
      "Train loss: 0.988609 [300800/345580]\n",
      "Train loss: 1.062536 [304000/345580]\n",
      "Train loss: 1.084197 [307200/345580]\n",
      "Train loss: 1.016123 [310400/345580]\n",
      "Train loss: 1.138572 [313600/345580]\n",
      "Train loss: 0.974522 [316800/345580]\n",
      "Train loss: 1.028022 [320000/345580]\n",
      "Train loss: 1.101150 [323200/345580]\n",
      "Train loss: 1.062689 [326400/345580]\n",
      "Train loss: 1.017591 [329600/345580]\n",
      "Train loss: 1.049715 [332800/345580]\n",
      "Train loss: 1.027987 [336000/345580]\n",
      "Train loss: 1.054543 [339200/345580]\n",
      "Train loss: 1.102266 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 42.9%, Avg loss: 1.041788 \n",
      "\n",
      "Epoch 66\n",
      " -------------------\n",
      "Train loss: 1.038371 [    0/345580]\n",
      "Train loss: 1.034012 [ 3200/345580]\n",
      "Train loss: 1.107419 [ 6400/345580]\n",
      "Train loss: 1.100037 [ 9600/345580]\n",
      "Train loss: 1.005055 [12800/345580]\n",
      "Train loss: 1.158817 [16000/345580]\n",
      "Train loss: 1.040678 [19200/345580]\n",
      "Train loss: 0.960186 [22400/345580]\n",
      "Train loss: 0.944500 [25600/345580]\n",
      "Train loss: 1.022136 [28800/345580]\n",
      "Train loss: 1.076109 [32000/345580]\n",
      "Train loss: 1.179082 [35200/345580]\n",
      "Train loss: 0.998033 [38400/345580]\n",
      "Train loss: 1.108708 [41600/345580]\n",
      "Train loss: 0.994640 [44800/345580]\n",
      "Train loss: 1.020695 [48000/345580]\n",
      "Train loss: 1.034657 [51200/345580]\n",
      "Train loss: 1.088454 [54400/345580]\n",
      "Train loss: 1.072476 [57600/345580]\n",
      "Train loss: 1.092306 [60800/345580]\n",
      "Train loss: 0.976311 [64000/345580]\n",
      "Train loss: 1.041396 [67200/345580]\n",
      "Train loss: 1.016430 [70400/345580]\n",
      "Train loss: 1.172770 [73600/345580]\n",
      "Train loss: 1.053943 [76800/345580]\n",
      "Train loss: 1.024764 [80000/345580]\n",
      "Train loss: 1.003282 [83200/345580]\n",
      "Train loss: 1.049487 [86400/345580]\n",
      "Train loss: 1.143743 [89600/345580]\n",
      "Train loss: 1.025923 [92800/345580]\n",
      "Train loss: 1.095624 [96000/345580]\n",
      "Train loss: 1.045689 [99200/345580]\n",
      "Train loss: 1.093837 [102400/345580]\n",
      "Train loss: 1.024055 [105600/345580]\n",
      "Train loss: 0.955516 [108800/345580]\n",
      "Train loss: 1.159682 [112000/345580]\n",
      "Train loss: 1.109019 [115200/345580]\n",
      "Train loss: 1.052404 [118400/345580]\n",
      "Train loss: 0.996363 [121600/345580]\n",
      "Train loss: 1.041633 [124800/345580]\n",
      "Train loss: 1.089808 [128000/345580]\n",
      "Train loss: 1.038643 [131200/345580]\n",
      "Train loss: 0.998776 [134400/345580]\n",
      "Train loss: 1.108617 [137600/345580]\n",
      "Train loss: 1.042335 [140800/345580]\n",
      "Train loss: 1.004628 [144000/345580]\n",
      "Train loss: 1.123960 [147200/345580]\n",
      "Train loss: 1.042399 [150400/345580]\n",
      "Train loss: 1.064479 [153600/345580]\n",
      "Train loss: 1.047886 [156800/345580]\n",
      "Train loss: 0.995747 [160000/345580]\n",
      "Train loss: 1.031248 [163200/345580]\n",
      "Train loss: 0.975376 [166400/345580]\n",
      "Train loss: 0.970296 [169600/345580]\n",
      "Train loss: 1.172524 [172800/345580]\n",
      "Train loss: 1.102594 [176000/345580]\n",
      "Train loss: 1.030228 [179200/345580]\n",
      "Train loss: 0.969282 [182400/345580]\n",
      "Train loss: 1.188302 [185600/345580]\n",
      "Train loss: 1.013270 [188800/345580]\n",
      "Train loss: 1.024012 [192000/345580]\n",
      "Train loss: 1.129848 [195200/345580]\n",
      "Train loss: 1.116650 [198400/345580]\n",
      "Train loss: 1.006233 [201600/345580]\n",
      "Train loss: 0.968854 [204800/345580]\n",
      "Train loss: 1.105245 [208000/345580]\n",
      "Train loss: 0.951260 [211200/345580]\n",
      "Train loss: 1.129397 [214400/345580]\n",
      "Train loss: 1.032854 [217600/345580]\n",
      "Train loss: 0.981378 [220800/345580]\n",
      "Train loss: 1.133351 [224000/345580]\n",
      "Train loss: 0.962501 [227200/345580]\n",
      "Train loss: 1.041008 [230400/345580]\n",
      "Train loss: 1.047817 [233600/345580]\n",
      "Train loss: 0.939301 [236800/345580]\n",
      "Train loss: 1.031024 [240000/345580]\n",
      "Train loss: 1.081192 [243200/345580]\n",
      "Train loss: 1.156626 [246400/345580]\n",
      "Train loss: 0.944938 [249600/345580]\n",
      "Train loss: 0.926246 [252800/345580]\n",
      "Train loss: 0.967122 [256000/345580]\n",
      "Train loss: 1.082062 [259200/345580]\n",
      "Train loss: 0.986668 [262400/345580]\n",
      "Train loss: 1.030348 [265600/345580]\n",
      "Train loss: 1.064177 [268800/345580]\n",
      "Train loss: 1.062776 [272000/345580]\n",
      "Train loss: 1.009126 [275200/345580]\n",
      "Train loss: 0.994117 [278400/345580]\n",
      "Train loss: 0.971777 [281600/345580]\n",
      "Train loss: 1.004726 [284800/345580]\n",
      "Train loss: 0.991813 [288000/345580]\n",
      "Train loss: 0.966952 [291200/345580]\n",
      "Train loss: 1.071840 [294400/345580]\n",
      "Train loss: 0.922316 [297600/345580]\n",
      "Train loss: 1.016728 [300800/345580]\n",
      "Train loss: 1.029606 [304000/345580]\n",
      "Train loss: 0.996286 [307200/345580]\n",
      "Train loss: 1.069058 [310400/345580]\n",
      "Train loss: 1.026003 [313600/345580]\n",
      "Train loss: 1.041511 [316800/345580]\n",
      "Train loss: 1.069312 [320000/345580]\n",
      "Train loss: 1.029267 [323200/345580]\n",
      "Train loss: 1.021592 [326400/345580]\n",
      "Train loss: 1.097807 [329600/345580]\n",
      "Train loss: 1.152635 [332800/345580]\n",
      "Train loss: 1.151627 [336000/345580]\n",
      "Train loss: 1.132238 [339200/345580]\n",
      "Train loss: 1.008812 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.042122 \n",
      "\n",
      "Epoch 67\n",
      " -------------------\n",
      "Train loss: 1.082270 [    0/345580]\n",
      "Train loss: 1.038539 [ 3200/345580]\n",
      "Train loss: 1.034351 [ 6400/345580]\n",
      "Train loss: 1.015268 [ 9600/345580]\n",
      "Train loss: 0.986890 [12800/345580]\n",
      "Train loss: 0.908555 [16000/345580]\n",
      "Train loss: 1.132357 [19200/345580]\n",
      "Train loss: 0.946800 [22400/345580]\n",
      "Train loss: 1.070868 [25600/345580]\n",
      "Train loss: 1.018506 [28800/345580]\n",
      "Train loss: 1.234373 [32000/345580]\n",
      "Train loss: 1.032308 [35200/345580]\n",
      "Train loss: 1.107117 [38400/345580]\n",
      "Train loss: 0.978351 [41600/345580]\n",
      "Train loss: 1.057316 [44800/345580]\n",
      "Train loss: 1.053482 [48000/345580]\n",
      "Train loss: 0.985293 [51200/345580]\n",
      "Train loss: 1.034738 [54400/345580]\n",
      "Train loss: 1.148951 [57600/345580]\n",
      "Train loss: 0.988003 [60800/345580]\n",
      "Train loss: 1.076153 [64000/345580]\n",
      "Train loss: 1.020383 [67200/345580]\n",
      "Train loss: 0.931015 [70400/345580]\n",
      "Train loss: 0.966280 [73600/345580]\n",
      "Train loss: 1.049888 [76800/345580]\n",
      "Train loss: 1.209233 [80000/345580]\n",
      "Train loss: 1.012609 [83200/345580]\n",
      "Train loss: 1.076047 [86400/345580]\n",
      "Train loss: 1.027273 [89600/345580]\n",
      "Train loss: 1.131100 [92800/345580]\n",
      "Train loss: 1.067423 [96000/345580]\n",
      "Train loss: 1.147972 [99200/345580]\n",
      "Train loss: 1.062641 [102400/345580]\n",
      "Train loss: 1.037392 [105600/345580]\n",
      "Train loss: 1.190949 [108800/345580]\n",
      "Train loss: 1.047122 [112000/345580]\n",
      "Train loss: 1.041237 [115200/345580]\n",
      "Train loss: 1.039960 [118400/345580]\n",
      "Train loss: 1.039045 [121600/345580]\n",
      "Train loss: 1.032525 [124800/345580]\n",
      "Train loss: 1.148360 [128000/345580]\n",
      "Train loss: 0.972455 [131200/345580]\n",
      "Train loss: 1.042585 [134400/345580]\n",
      "Train loss: 0.994312 [137600/345580]\n",
      "Train loss: 0.986136 [140800/345580]\n",
      "Train loss: 1.084060 [144000/345580]\n",
      "Train loss: 1.076121 [147200/345580]\n",
      "Train loss: 0.932782 [150400/345580]\n",
      "Train loss: 1.073961 [153600/345580]\n",
      "Train loss: 1.024247 [156800/345580]\n",
      "Train loss: 1.159234 [160000/345580]\n",
      "Train loss: 1.009349 [163200/345580]\n",
      "Train loss: 1.159026 [166400/345580]\n",
      "Train loss: 1.038822 [169600/345580]\n",
      "Train loss: 1.040658 [172800/345580]\n",
      "Train loss: 0.966106 [176000/345580]\n",
      "Train loss: 1.031209 [179200/345580]\n",
      "Train loss: 1.049731 [182400/345580]\n",
      "Train loss: 1.046669 [185600/345580]\n",
      "Train loss: 0.991130 [188800/345580]\n",
      "Train loss: 1.081072 [192000/345580]\n",
      "Train loss: 1.032395 [195200/345580]\n",
      "Train loss: 1.032931 [198400/345580]\n",
      "Train loss: 1.044685 [201600/345580]\n",
      "Train loss: 0.991392 [204800/345580]\n",
      "Train loss: 1.016367 [208000/345580]\n",
      "Train loss: 1.071560 [211200/345580]\n",
      "Train loss: 1.018314 [214400/345580]\n",
      "Train loss: 1.179547 [217600/345580]\n",
      "Train loss: 1.169418 [220800/345580]\n",
      "Train loss: 1.019574 [224000/345580]\n",
      "Train loss: 1.020674 [227200/345580]\n",
      "Train loss: 1.077784 [230400/345580]\n",
      "Train loss: 1.021450 [233600/345580]\n",
      "Train loss: 0.979189 [236800/345580]\n",
      "Train loss: 1.078907 [240000/345580]\n",
      "Train loss: 1.068160 [243200/345580]\n",
      "Train loss: 0.939743 [246400/345580]\n",
      "Train loss: 0.992724 [249600/345580]\n",
      "Train loss: 1.054762 [252800/345580]\n",
      "Train loss: 1.102068 [256000/345580]\n",
      "Train loss: 0.987734 [259200/345580]\n",
      "Train loss: 1.177096 [262400/345580]\n",
      "Train loss: 1.091973 [265600/345580]\n",
      "Train loss: 1.001418 [268800/345580]\n",
      "Train loss: 1.132296 [272000/345580]\n",
      "Train loss: 1.114031 [275200/345580]\n",
      "Train loss: 1.029755 [278400/345580]\n",
      "Train loss: 0.947672 [281600/345580]\n",
      "Train loss: 0.970611 [284800/345580]\n",
      "Train loss: 1.032275 [288000/345580]\n",
      "Train loss: 1.063194 [291200/345580]\n",
      "Train loss: 1.207563 [294400/345580]\n",
      "Train loss: 0.975747 [297600/345580]\n",
      "Train loss: 0.971019 [300800/345580]\n",
      "Train loss: 1.097436 [304000/345580]\n",
      "Train loss: 1.137280 [307200/345580]\n",
      "Train loss: 1.000701 [310400/345580]\n",
      "Train loss: 1.012055 [313600/345580]\n",
      "Train loss: 1.023560 [316800/345580]\n",
      "Train loss: 0.943831 [320000/345580]\n",
      "Train loss: 1.039575 [323200/345580]\n",
      "Train loss: 1.012178 [326400/345580]\n",
      "Train loss: 1.007451 [329600/345580]\n",
      "Train loss: 1.101197 [332800/345580]\n",
      "Train loss: 1.002557 [336000/345580]\n",
      "Train loss: 1.034685 [339200/345580]\n",
      "Train loss: 0.984092 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 42.9%, Avg loss: 1.041604 \n",
      "\n",
      "Epoch 68\n",
      " -------------------\n",
      "Train loss: 1.159255 [    0/345580]\n",
      "Train loss: 1.064931 [ 3200/345580]\n",
      "Train loss: 0.972801 [ 6400/345580]\n",
      "Train loss: 1.025676 [ 9600/345580]\n",
      "Train loss: 1.018018 [12800/345580]\n",
      "Train loss: 0.980962 [16000/345580]\n",
      "Train loss: 1.019444 [19200/345580]\n",
      "Train loss: 0.984311 [22400/345580]\n",
      "Train loss: 1.084801 [25600/345580]\n",
      "Train loss: 1.155707 [28800/345580]\n",
      "Train loss: 0.947356 [32000/345580]\n",
      "Train loss: 1.069774 [35200/345580]\n",
      "Train loss: 0.930035 [38400/345580]\n",
      "Train loss: 1.019001 [41600/345580]\n",
      "Train loss: 1.041506 [44800/345580]\n",
      "Train loss: 1.052762 [48000/345580]\n",
      "Train loss: 1.033510 [51200/345580]\n",
      "Train loss: 1.075586 [54400/345580]\n",
      "Train loss: 1.061663 [57600/345580]\n",
      "Train loss: 1.056403 [60800/345580]\n",
      "Train loss: 1.059544 [64000/345580]\n",
      "Train loss: 1.060288 [67200/345580]\n",
      "Train loss: 1.140362 [70400/345580]\n",
      "Train loss: 1.004684 [73600/345580]\n",
      "Train loss: 0.945373 [76800/345580]\n",
      "Train loss: 1.036789 [80000/345580]\n",
      "Train loss: 1.081993 [83200/345580]\n",
      "Train loss: 1.011384 [86400/345580]\n",
      "Train loss: 1.077334 [89600/345580]\n",
      "Train loss: 0.996825 [92800/345580]\n",
      "Train loss: 1.116389 [96000/345580]\n",
      "Train loss: 1.114574 [99200/345580]\n",
      "Train loss: 1.009496 [102400/345580]\n",
      "Train loss: 1.051502 [105600/345580]\n",
      "Train loss: 1.040680 [108800/345580]\n",
      "Train loss: 1.013713 [112000/345580]\n",
      "Train loss: 1.083469 [115200/345580]\n",
      "Train loss: 1.130950 [118400/345580]\n",
      "Train loss: 1.172240 [121600/345580]\n",
      "Train loss: 1.011206 [124800/345580]\n",
      "Train loss: 0.951881 [128000/345580]\n",
      "Train loss: 1.041238 [131200/345580]\n",
      "Train loss: 1.085924 [134400/345580]\n",
      "Train loss: 1.043876 [137600/345580]\n",
      "Train loss: 0.993116 [140800/345580]\n",
      "Train loss: 1.034464 [144000/345580]\n",
      "Train loss: 1.026627 [147200/345580]\n",
      "Train loss: 1.051246 [150400/345580]\n",
      "Train loss: 0.995380 [153600/345580]\n",
      "Train loss: 1.094451 [156800/345580]\n",
      "Train loss: 1.014353 [160000/345580]\n",
      "Train loss: 1.013642 [163200/345580]\n",
      "Train loss: 1.019684 [166400/345580]\n",
      "Train loss: 1.025278 [169600/345580]\n",
      "Train loss: 1.104678 [172800/345580]\n",
      "Train loss: 1.124862 [176000/345580]\n",
      "Train loss: 1.027815 [179200/345580]\n",
      "Train loss: 1.068903 [182400/345580]\n",
      "Train loss: 1.063327 [185600/345580]\n",
      "Train loss: 0.996365 [188800/345580]\n",
      "Train loss: 1.024919 [192000/345580]\n",
      "Train loss: 0.979400 [195200/345580]\n",
      "Train loss: 1.016357 [198400/345580]\n",
      "Train loss: 1.116671 [201600/345580]\n",
      "Train loss: 1.102613 [204800/345580]\n",
      "Train loss: 1.017038 [208000/345580]\n",
      "Train loss: 1.036656 [211200/345580]\n",
      "Train loss: 1.053771 [214400/345580]\n",
      "Train loss: 0.931650 [217600/345580]\n",
      "Train loss: 1.037005 [220800/345580]\n",
      "Train loss: 1.133129 [224000/345580]\n",
      "Train loss: 1.101016 [227200/345580]\n",
      "Train loss: 1.156196 [230400/345580]\n",
      "Train loss: 1.059226 [233600/345580]\n",
      "Train loss: 0.995396 [236800/345580]\n",
      "Train loss: 1.028921 [240000/345580]\n",
      "Train loss: 1.093142 [243200/345580]\n",
      "Train loss: 1.112025 [246400/345580]\n",
      "Train loss: 1.048856 [249600/345580]\n",
      "Train loss: 1.065139 [252800/345580]\n",
      "Train loss: 1.146300 [256000/345580]\n",
      "Train loss: 1.064465 [259200/345580]\n",
      "Train loss: 0.976230 [262400/345580]\n",
      "Train loss: 1.014177 [265600/345580]\n",
      "Train loss: 1.003577 [268800/345580]\n",
      "Train loss: 1.014164 [272000/345580]\n",
      "Train loss: 1.046858 [275200/345580]\n",
      "Train loss: 1.118549 [278400/345580]\n",
      "Train loss: 1.050098 [281600/345580]\n",
      "Train loss: 1.023117 [284800/345580]\n",
      "Train loss: 1.076669 [288000/345580]\n",
      "Train loss: 1.028346 [291200/345580]\n",
      "Train loss: 0.973136 [294400/345580]\n",
      "Train loss: 1.071815 [297600/345580]\n",
      "Train loss: 1.065929 [300800/345580]\n",
      "Train loss: 0.982802 [304000/345580]\n",
      "Train loss: 1.124413 [307200/345580]\n",
      "Train loss: 0.950301 [310400/345580]\n",
      "Train loss: 1.071220 [313600/345580]\n",
      "Train loss: 0.943839 [316800/345580]\n",
      "Train loss: 1.058652 [320000/345580]\n",
      "Train loss: 0.994890 [323200/345580]\n",
      "Train loss: 1.088372 [326400/345580]\n",
      "Train loss: 0.985430 [329600/345580]\n",
      "Train loss: 1.029342 [332800/345580]\n",
      "Train loss: 1.026867 [336000/345580]\n",
      "Train loss: 1.043974 [339200/345580]\n",
      "Train loss: 1.033692 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.041121 \n",
      "\n",
      "Epoch 69\n",
      " -------------------\n",
      "Train loss: 1.037235 [    0/345580]\n",
      "Train loss: 0.963580 [ 3200/345580]\n",
      "Train loss: 1.131149 [ 6400/345580]\n",
      "Train loss: 1.110934 [ 9600/345580]\n",
      "Train loss: 1.070961 [12800/345580]\n",
      "Train loss: 1.063537 [16000/345580]\n",
      "Train loss: 1.107159 [19200/345580]\n",
      "Train loss: 1.128939 [22400/345580]\n",
      "Train loss: 1.061627 [25600/345580]\n",
      "Train loss: 0.994124 [28800/345580]\n",
      "Train loss: 0.967085 [32000/345580]\n",
      "Train loss: 1.064019 [35200/345580]\n",
      "Train loss: 1.101727 [38400/345580]\n",
      "Train loss: 1.148730 [41600/345580]\n",
      "Train loss: 1.125027 [44800/345580]\n",
      "Train loss: 1.184076 [48000/345580]\n",
      "Train loss: 1.105837 [51200/345580]\n",
      "Train loss: 0.975846 [54400/345580]\n",
      "Train loss: 1.046122 [57600/345580]\n",
      "Train loss: 1.035821 [60800/345580]\n",
      "Train loss: 1.043870 [64000/345580]\n",
      "Train loss: 1.137400 [67200/345580]\n",
      "Train loss: 1.020329 [70400/345580]\n",
      "Train loss: 1.015408 [73600/345580]\n",
      "Train loss: 0.952193 [76800/345580]\n",
      "Train loss: 1.038736 [80000/345580]\n",
      "Train loss: 1.203573 [83200/345580]\n",
      "Train loss: 1.048581 [86400/345580]\n",
      "Train loss: 0.990161 [89600/345580]\n",
      "Train loss: 1.098398 [92800/345580]\n",
      "Train loss: 1.101399 [96000/345580]\n",
      "Train loss: 1.077223 [99200/345580]\n",
      "Train loss: 0.956496 [102400/345580]\n",
      "Train loss: 0.978429 [105600/345580]\n",
      "Train loss: 1.030581 [108800/345580]\n",
      "Train loss: 1.055973 [112000/345580]\n",
      "Train loss: 1.089816 [115200/345580]\n",
      "Train loss: 1.025085 [118400/345580]\n",
      "Train loss: 1.009896 [121600/345580]\n",
      "Train loss: 1.001211 [124800/345580]\n",
      "Train loss: 1.056167 [128000/345580]\n",
      "Train loss: 0.952212 [131200/345580]\n",
      "Train loss: 1.094425 [134400/345580]\n",
      "Train loss: 1.074961 [137600/345580]\n",
      "Train loss: 0.957580 [140800/345580]\n",
      "Train loss: 1.079052 [144000/345580]\n",
      "Train loss: 1.076182 [147200/345580]\n",
      "Train loss: 1.080193 [150400/345580]\n",
      "Train loss: 1.033336 [153600/345580]\n",
      "Train loss: 1.058099 [156800/345580]\n",
      "Train loss: 1.028762 [160000/345580]\n",
      "Train loss: 1.123347 [163200/345580]\n",
      "Train loss: 1.024352 [166400/345580]\n",
      "Train loss: 1.056538 [169600/345580]\n",
      "Train loss: 1.114807 [172800/345580]\n",
      "Train loss: 1.055737 [176000/345580]\n",
      "Train loss: 1.054940 [179200/345580]\n",
      "Train loss: 1.137083 [182400/345580]\n",
      "Train loss: 1.047141 [185600/345580]\n",
      "Train loss: 1.030795 [188800/345580]\n",
      "Train loss: 1.092562 [192000/345580]\n",
      "Train loss: 1.045098 [195200/345580]\n",
      "Train loss: 1.044792 [198400/345580]\n",
      "Train loss: 1.011545 [201600/345580]\n",
      "Train loss: 1.014812 [204800/345580]\n",
      "Train loss: 1.071448 [208000/345580]\n",
      "Train loss: 1.004310 [211200/345580]\n",
      "Train loss: 1.019062 [214400/345580]\n",
      "Train loss: 1.058021 [217600/345580]\n",
      "Train loss: 1.097392 [220800/345580]\n",
      "Train loss: 1.102496 [224000/345580]\n",
      "Train loss: 1.106313 [227200/345580]\n",
      "Train loss: 1.130803 [230400/345580]\n",
      "Train loss: 1.042747 [233600/345580]\n",
      "Train loss: 1.006996 [236800/345580]\n",
      "Train loss: 1.135125 [240000/345580]\n",
      "Train loss: 1.026006 [243200/345580]\n",
      "Train loss: 1.042403 [246400/345580]\n",
      "Train loss: 1.089284 [249600/345580]\n",
      "Train loss: 1.025442 [252800/345580]\n",
      "Train loss: 1.062934 [256000/345580]\n",
      "Train loss: 1.027732 [259200/345580]\n",
      "Train loss: 1.052719 [262400/345580]\n",
      "Train loss: 1.069367 [265600/345580]\n",
      "Train loss: 1.030435 [268800/345580]\n",
      "Train loss: 1.072145 [272000/345580]\n",
      "Train loss: 1.018989 [275200/345580]\n",
      "Train loss: 1.076032 [278400/345580]\n",
      "Train loss: 1.063270 [281600/345580]\n",
      "Train loss: 1.027325 [284800/345580]\n",
      "Train loss: 1.011798 [288000/345580]\n",
      "Train loss: 0.927282 [291200/345580]\n",
      "Train loss: 0.958606 [294400/345580]\n",
      "Train loss: 0.951584 [297600/345580]\n",
      "Train loss: 1.006029 [300800/345580]\n",
      "Train loss: 0.998239 [304000/345580]\n",
      "Train loss: 1.088841 [307200/345580]\n",
      "Train loss: 1.035973 [310400/345580]\n",
      "Train loss: 1.158924 [313600/345580]\n",
      "Train loss: 1.086594 [316800/345580]\n",
      "Train loss: 0.985484 [320000/345580]\n",
      "Train loss: 1.040458 [323200/345580]\n",
      "Train loss: 1.078284 [326400/345580]\n",
      "Train loss: 1.190302 [329600/345580]\n",
      "Train loss: 1.039619 [332800/345580]\n",
      "Train loss: 1.070150 [336000/345580]\n",
      "Train loss: 0.963486 [339200/345580]\n",
      "Train loss: 1.131911 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.041346 \n",
      "\n",
      "Epoch 70\n",
      " -------------------\n",
      "Train loss: 1.069822 [    0/345580]\n",
      "Train loss: 1.043877 [ 3200/345580]\n",
      "Train loss: 1.083206 [ 6400/345580]\n",
      "Train loss: 1.023751 [ 9600/345580]\n",
      "Train loss: 1.061915 [12800/345580]\n",
      "Train loss: 1.096709 [16000/345580]\n",
      "Train loss: 1.002427 [19200/345580]\n",
      "Train loss: 1.098655 [22400/345580]\n",
      "Train loss: 1.133382 [25600/345580]\n",
      "Train loss: 1.093703 [28800/345580]\n",
      "Train loss: 1.111074 [32000/345580]\n",
      "Train loss: 1.117709 [35200/345580]\n",
      "Train loss: 0.966698 [38400/345580]\n",
      "Train loss: 0.963383 [41600/345580]\n",
      "Train loss: 1.037489 [44800/345580]\n",
      "Train loss: 1.060919 [48000/345580]\n",
      "Train loss: 1.101823 [51200/345580]\n",
      "Train loss: 1.103176 [54400/345580]\n",
      "Train loss: 1.013691 [57600/345580]\n",
      "Train loss: 0.954994 [60800/345580]\n",
      "Train loss: 1.094972 [64000/345580]\n",
      "Train loss: 1.106821 [67200/345580]\n",
      "Train loss: 1.093249 [70400/345580]\n",
      "Train loss: 1.131455 [73600/345580]\n",
      "Train loss: 0.937200 [76800/345580]\n",
      "Train loss: 1.148625 [80000/345580]\n",
      "Train loss: 1.123179 [83200/345580]\n",
      "Train loss: 1.014785 [86400/345580]\n",
      "Train loss: 0.957213 [89600/345580]\n",
      "Train loss: 0.967477 [92800/345580]\n",
      "Train loss: 1.043445 [96000/345580]\n",
      "Train loss: 0.978155 [99200/345580]\n",
      "Train loss: 1.091475 [102400/345580]\n",
      "Train loss: 0.913366 [105600/345580]\n",
      "Train loss: 1.049460 [108800/345580]\n",
      "Train loss: 1.007165 [112000/345580]\n",
      "Train loss: 1.196143 [115200/345580]\n",
      "Train loss: 1.009202 [118400/345580]\n",
      "Train loss: 1.043143 [121600/345580]\n",
      "Train loss: 1.023885 [124800/345580]\n",
      "Train loss: 1.133569 [128000/345580]\n",
      "Train loss: 1.036988 [131200/345580]\n",
      "Train loss: 1.048162 [134400/345580]\n",
      "Train loss: 1.028468 [137600/345580]\n",
      "Train loss: 1.027484 [140800/345580]\n",
      "Train loss: 1.096865 [144000/345580]\n",
      "Train loss: 1.062944 [147200/345580]\n",
      "Train loss: 1.049794 [150400/345580]\n",
      "Train loss: 1.148671 [153600/345580]\n",
      "Train loss: 1.060176 [156800/345580]\n",
      "Train loss: 1.032987 [160000/345580]\n",
      "Train loss: 1.035229 [163200/345580]\n",
      "Train loss: 1.089588 [166400/345580]\n",
      "Train loss: 1.062846 [169600/345580]\n",
      "Train loss: 1.097374 [172800/345580]\n",
      "Train loss: 1.009308 [176000/345580]\n",
      "Train loss: 1.008191 [179200/345580]\n",
      "Train loss: 1.142526 [182400/345580]\n",
      "Train loss: 0.929394 [185600/345580]\n",
      "Train loss: 1.003677 [188800/345580]\n",
      "Train loss: 1.093771 [192000/345580]\n",
      "Train loss: 1.079109 [195200/345580]\n",
      "Train loss: 1.069688 [198400/345580]\n",
      "Train loss: 1.089718 [201600/345580]\n",
      "Train loss: 1.122399 [204800/345580]\n",
      "Train loss: 0.923174 [208000/345580]\n",
      "Train loss: 1.088736 [211200/345580]\n",
      "Train loss: 0.975736 [214400/345580]\n",
      "Train loss: 1.005139 [217600/345580]\n",
      "Train loss: 1.094264 [220800/345580]\n",
      "Train loss: 0.971116 [224000/345580]\n",
      "Train loss: 1.055472 [227200/345580]\n",
      "Train loss: 1.014097 [230400/345580]\n",
      "Train loss: 1.019054 [233600/345580]\n",
      "Train loss: 1.027918 [236800/345580]\n",
      "Train loss: 0.995138 [240000/345580]\n",
      "Train loss: 1.051545 [243200/345580]\n",
      "Train loss: 1.013624 [246400/345580]\n",
      "Train loss: 1.062579 [249600/345580]\n",
      "Train loss: 1.121498 [252800/345580]\n",
      "Train loss: 0.938085 [256000/345580]\n",
      "Train loss: 1.099777 [259200/345580]\n",
      "Train loss: 0.981747 [262400/345580]\n",
      "Train loss: 1.041346 [265600/345580]\n",
      "Train loss: 1.021473 [268800/345580]\n",
      "Train loss: 1.059593 [272000/345580]\n",
      "Train loss: 1.078439 [275200/345580]\n",
      "Train loss: 1.068906 [278400/345580]\n",
      "Train loss: 1.028019 [281600/345580]\n",
      "Train loss: 0.947241 [284800/345580]\n",
      "Train loss: 0.977715 [288000/345580]\n",
      "Train loss: 1.046411 [291200/345580]\n",
      "Train loss: 1.140179 [294400/345580]\n",
      "Train loss: 1.028095 [297600/345580]\n",
      "Train loss: 1.000588 [300800/345580]\n",
      "Train loss: 1.088534 [304000/345580]\n",
      "Train loss: 1.014852 [307200/345580]\n",
      "Train loss: 0.959311 [310400/345580]\n",
      "Train loss: 1.105824 [313600/345580]\n",
      "Train loss: 1.096870 [316800/345580]\n",
      "Train loss: 1.009343 [320000/345580]\n",
      "Train loss: 1.082824 [323200/345580]\n",
      "Train loss: 1.071077 [326400/345580]\n",
      "Train loss: 1.081150 [329600/345580]\n",
      "Train loss: 0.957811 [332800/345580]\n",
      "Train loss: 1.059952 [336000/345580]\n",
      "Train loss: 1.011352 [339200/345580]\n",
      "Train loss: 1.013442 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.042255 \n",
      "\n",
      "Epoch 71\n",
      " -------------------\n",
      "Train loss: 1.090085 [    0/345580]\n",
      "Train loss: 1.028832 [ 3200/345580]\n",
      "Train loss: 1.000635 [ 6400/345580]\n",
      "Train loss: 1.049759 [ 9600/345580]\n",
      "Train loss: 1.017089 [12800/345580]\n",
      "Train loss: 0.980943 [16000/345580]\n",
      "Train loss: 1.026850 [19200/345580]\n",
      "Train loss: 1.136825 [22400/345580]\n",
      "Train loss: 1.038396 [25600/345580]\n",
      "Train loss: 1.015913 [28800/345580]\n",
      "Train loss: 1.091608 [32000/345580]\n",
      "Train loss: 1.021363 [35200/345580]\n",
      "Train loss: 1.111387 [38400/345580]\n",
      "Train loss: 1.116593 [41600/345580]\n",
      "Train loss: 1.008835 [44800/345580]\n",
      "Train loss: 1.051274 [48000/345580]\n",
      "Train loss: 1.135429 [51200/345580]\n",
      "Train loss: 1.123627 [54400/345580]\n",
      "Train loss: 1.067718 [57600/345580]\n",
      "Train loss: 1.145174 [60800/345580]\n",
      "Train loss: 1.097627 [64000/345580]\n",
      "Train loss: 0.963147 [67200/345580]\n",
      "Train loss: 1.061379 [70400/345580]\n",
      "Train loss: 1.098907 [73600/345580]\n",
      "Train loss: 0.998957 [76800/345580]\n",
      "Train loss: 0.976829 [80000/345580]\n",
      "Train loss: 1.042841 [83200/345580]\n",
      "Train loss: 1.076640 [86400/345580]\n",
      "Train loss: 0.974234 [89600/345580]\n",
      "Train loss: 1.132371 [92800/345580]\n",
      "Train loss: 0.952822 [96000/345580]\n",
      "Train loss: 0.996808 [99200/345580]\n",
      "Train loss: 1.067933 [102400/345580]\n",
      "Train loss: 1.082141 [105600/345580]\n",
      "Train loss: 1.046119 [108800/345580]\n",
      "Train loss: 1.212754 [112000/345580]\n",
      "Train loss: 1.118833 [115200/345580]\n",
      "Train loss: 1.103185 [118400/345580]\n",
      "Train loss: 1.155049 [121600/345580]\n",
      "Train loss: 1.068852 [124800/345580]\n",
      "Train loss: 1.031849 [128000/345580]\n",
      "Train loss: 1.083938 [131200/345580]\n",
      "Train loss: 0.998241 [134400/345580]\n",
      "Train loss: 1.052840 [137600/345580]\n",
      "Train loss: 1.032457 [140800/345580]\n",
      "Train loss: 1.092481 [144000/345580]\n",
      "Train loss: 1.006419 [147200/345580]\n",
      "Train loss: 0.968909 [150400/345580]\n",
      "Train loss: 1.052415 [153600/345580]\n",
      "Train loss: 1.035905 [156800/345580]\n",
      "Train loss: 1.025765 [160000/345580]\n",
      "Train loss: 1.029205 [163200/345580]\n",
      "Train loss: 1.115996 [166400/345580]\n",
      "Train loss: 1.120793 [169600/345580]\n",
      "Train loss: 1.027679 [172800/345580]\n",
      "Train loss: 1.039596 [176000/345580]\n",
      "Train loss: 1.059091 [179200/345580]\n",
      "Train loss: 1.113167 [182400/345580]\n",
      "Train loss: 1.005453 [185600/345580]\n",
      "Train loss: 1.017437 [188800/345580]\n",
      "Train loss: 1.014510 [192000/345580]\n",
      "Train loss: 1.016709 [195200/345580]\n",
      "Train loss: 1.019234 [198400/345580]\n",
      "Train loss: 1.126250 [201600/345580]\n",
      "Train loss: 1.128837 [204800/345580]\n",
      "Train loss: 1.067615 [208000/345580]\n",
      "Train loss: 1.026405 [211200/345580]\n",
      "Train loss: 1.115858 [214400/345580]\n",
      "Train loss: 1.091911 [217600/345580]\n",
      "Train loss: 1.085310 [220800/345580]\n",
      "Train loss: 0.966447 [224000/345580]\n",
      "Train loss: 1.123897 [227200/345580]\n",
      "Train loss: 0.982082 [230400/345580]\n",
      "Train loss: 0.965379 [233600/345580]\n",
      "Train loss: 0.984544 [236800/345580]\n",
      "Train loss: 1.089825 [240000/345580]\n",
      "Train loss: 1.134191 [243200/345580]\n",
      "Train loss: 1.064206 [246400/345580]\n",
      "Train loss: 0.986834 [249600/345580]\n",
      "Train loss: 1.108375 [252800/345580]\n",
      "Train loss: 1.102904 [256000/345580]\n",
      "Train loss: 0.960325 [259200/345580]\n",
      "Train loss: 0.992146 [262400/345580]\n",
      "Train loss: 1.024395 [265600/345580]\n",
      "Train loss: 1.105182 [268800/345580]\n",
      "Train loss: 0.964389 [272000/345580]\n",
      "Train loss: 1.051873 [275200/345580]\n",
      "Train loss: 0.976052 [278400/345580]\n",
      "Train loss: 0.984771 [281600/345580]\n",
      "Train loss: 1.098999 [284800/345580]\n",
      "Train loss: 1.052788 [288000/345580]\n",
      "Train loss: 0.978192 [291200/345580]\n",
      "Train loss: 1.099038 [294400/345580]\n",
      "Train loss: 1.032202 [297600/345580]\n",
      "Train loss: 0.976031 [300800/345580]\n",
      "Train loss: 1.037840 [304000/345580]\n",
      "Train loss: 0.969416 [307200/345580]\n",
      "Train loss: 1.015684 [310400/345580]\n",
      "Train loss: 1.054114 [313600/345580]\n",
      "Train loss: 1.044485 [316800/345580]\n",
      "Train loss: 1.037986 [320000/345580]\n",
      "Train loss: 1.096532 [323200/345580]\n",
      "Train loss: 1.092088 [326400/345580]\n",
      "Train loss: 1.080897 [329600/345580]\n",
      "Train loss: 1.034813 [332800/345580]\n",
      "Train loss: 1.037308 [336000/345580]\n",
      "Train loss: 0.995745 [339200/345580]\n",
      "Train loss: 1.014781 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.041603 \n",
      "\n",
      "Epoch 72\n",
      " -------------------\n",
      "Train loss: 1.098835 [    0/345580]\n",
      "Train loss: 1.070949 [ 3200/345580]\n",
      "Train loss: 0.968397 [ 6400/345580]\n",
      "Train loss: 1.018670 [ 9600/345580]\n",
      "Train loss: 1.008865 [12800/345580]\n",
      "Train loss: 1.037084 [16000/345580]\n",
      "Train loss: 0.997032 [19200/345580]\n",
      "Train loss: 1.136494 [22400/345580]\n",
      "Train loss: 1.019069 [25600/345580]\n",
      "Train loss: 1.105878 [28800/345580]\n",
      "Train loss: 0.970677 [32000/345580]\n",
      "Train loss: 1.024683 [35200/345580]\n",
      "Train loss: 1.076244 [38400/345580]\n",
      "Train loss: 1.042554 [41600/345580]\n",
      "Train loss: 1.099122 [44800/345580]\n",
      "Train loss: 0.995628 [48000/345580]\n",
      "Train loss: 1.119384 [51200/345580]\n",
      "Train loss: 1.020604 [54400/345580]\n",
      "Train loss: 1.022577 [57600/345580]\n",
      "Train loss: 0.958672 [60800/345580]\n",
      "Train loss: 0.959956 [64000/345580]\n",
      "Train loss: 1.108799 [67200/345580]\n",
      "Train loss: 1.085947 [70400/345580]\n",
      "Train loss: 1.040657 [73600/345580]\n",
      "Train loss: 0.979260 [76800/345580]\n",
      "Train loss: 1.100516 [80000/345580]\n",
      "Train loss: 1.014045 [83200/345580]\n",
      "Train loss: 0.992215 [86400/345580]\n",
      "Train loss: 1.167408 [89600/345580]\n",
      "Train loss: 1.111278 [92800/345580]\n",
      "Train loss: 1.060792 [96000/345580]\n",
      "Train loss: 1.107214 [99200/345580]\n",
      "Train loss: 1.055692 [102400/345580]\n",
      "Train loss: 0.990636 [105600/345580]\n",
      "Train loss: 0.967073 [108800/345580]\n",
      "Train loss: 1.046550 [112000/345580]\n",
      "Train loss: 1.095804 [115200/345580]\n",
      "Train loss: 1.029591 [118400/345580]\n",
      "Train loss: 1.071092 [121600/345580]\n",
      "Train loss: 1.093906 [124800/345580]\n",
      "Train loss: 1.107621 [128000/345580]\n",
      "Train loss: 0.940168 [131200/345580]\n",
      "Train loss: 1.010401 [134400/345580]\n",
      "Train loss: 0.903200 [137600/345580]\n",
      "Train loss: 1.066865 [140800/345580]\n",
      "Train loss: 1.066742 [144000/345580]\n",
      "Train loss: 1.007472 [147200/345580]\n",
      "Train loss: 0.957510 [150400/345580]\n",
      "Train loss: 0.912484 [153600/345580]\n",
      "Train loss: 1.037179 [156800/345580]\n",
      "Train loss: 1.037845 [160000/345580]\n",
      "Train loss: 1.003133 [163200/345580]\n",
      "Train loss: 1.018097 [166400/345580]\n",
      "Train loss: 1.015084 [169600/345580]\n",
      "Train loss: 0.995305 [172800/345580]\n",
      "Train loss: 1.011624 [176000/345580]\n",
      "Train loss: 1.095444 [179200/345580]\n",
      "Train loss: 1.011568 [182400/345580]\n",
      "Train loss: 1.082225 [185600/345580]\n",
      "Train loss: 1.013521 [188800/345580]\n",
      "Train loss: 0.994803 [192000/345580]\n",
      "Train loss: 1.090393 [195200/345580]\n",
      "Train loss: 0.930365 [198400/345580]\n",
      "Train loss: 0.993677 [201600/345580]\n",
      "Train loss: 1.024171 [204800/345580]\n",
      "Train loss: 0.938611 [208000/345580]\n",
      "Train loss: 1.048170 [211200/345580]\n",
      "Train loss: 1.182409 [214400/345580]\n",
      "Train loss: 1.020874 [217600/345580]\n",
      "Train loss: 1.010259 [220800/345580]\n",
      "Train loss: 1.030194 [224000/345580]\n",
      "Train loss: 1.135515 [227200/345580]\n",
      "Train loss: 1.083191 [230400/345580]\n",
      "Train loss: 1.009934 [233600/345580]\n",
      "Train loss: 0.979557 [236800/345580]\n",
      "Train loss: 1.031403 [240000/345580]\n",
      "Train loss: 1.121115 [243200/345580]\n",
      "Train loss: 1.004693 [246400/345580]\n",
      "Train loss: 1.083130 [249600/345580]\n",
      "Train loss: 1.065673 [252800/345580]\n",
      "Train loss: 1.045998 [256000/345580]\n",
      "Train loss: 1.052889 [259200/345580]\n",
      "Train loss: 0.987828 [262400/345580]\n",
      "Train loss: 1.074474 [265600/345580]\n",
      "Train loss: 1.106910 [268800/345580]\n",
      "Train loss: 1.002754 [272000/345580]\n",
      "Train loss: 1.060662 [275200/345580]\n",
      "Train loss: 1.089178 [278400/345580]\n",
      "Train loss: 0.973521 [281600/345580]\n",
      "Train loss: 1.038215 [284800/345580]\n",
      "Train loss: 0.993493 [288000/345580]\n",
      "Train loss: 0.964550 [291200/345580]\n",
      "Train loss: 1.068490 [294400/345580]\n",
      "Train loss: 1.001372 [297600/345580]\n",
      "Train loss: 1.071375 [300800/345580]\n",
      "Train loss: 1.012619 [304000/345580]\n",
      "Train loss: 1.154481 [307200/345580]\n",
      "Train loss: 1.083708 [310400/345580]\n",
      "Train loss: 0.959449 [313600/345580]\n",
      "Train loss: 1.003837 [316800/345580]\n",
      "Train loss: 1.045524 [320000/345580]\n",
      "Train loss: 1.017725 [323200/345580]\n",
      "Train loss: 1.103208 [326400/345580]\n",
      "Train loss: 0.989327 [329600/345580]\n",
      "Train loss: 1.083817 [332800/345580]\n",
      "Train loss: 0.991682 [336000/345580]\n",
      "Train loss: 1.062429 [339200/345580]\n",
      "Train loss: 0.961004 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 42.7%, Avg loss: 1.041878 \n",
      "\n",
      "Epoch 73\n",
      " -------------------\n",
      "Train loss: 1.143361 [    0/345580]\n",
      "Train loss: 0.999958 [ 3200/345580]\n",
      "Train loss: 1.044885 [ 6400/345580]\n",
      "Train loss: 1.024005 [ 9600/345580]\n",
      "Train loss: 1.024625 [12800/345580]\n",
      "Train loss: 0.988272 [16000/345580]\n",
      "Train loss: 1.102427 [19200/345580]\n",
      "Train loss: 1.062952 [22400/345580]\n",
      "Train loss: 1.055027 [25600/345580]\n",
      "Train loss: 1.057906 [28800/345580]\n",
      "Train loss: 1.115444 [32000/345580]\n",
      "Train loss: 0.990178 [35200/345580]\n",
      "Train loss: 1.073295 [38400/345580]\n",
      "Train loss: 1.008604 [41600/345580]\n",
      "Train loss: 1.032482 [44800/345580]\n",
      "Train loss: 1.001164 [48000/345580]\n",
      "Train loss: 0.921753 [51200/345580]\n",
      "Train loss: 1.004609 [54400/345580]\n",
      "Train loss: 1.044547 [57600/345580]\n",
      "Train loss: 0.978811 [60800/345580]\n",
      "Train loss: 1.019894 [64000/345580]\n",
      "Train loss: 1.068918 [67200/345580]\n",
      "Train loss: 1.126744 [70400/345580]\n",
      "Train loss: 1.010735 [73600/345580]\n",
      "Train loss: 1.009144 [76800/345580]\n",
      "Train loss: 1.041934 [80000/345580]\n",
      "Train loss: 1.014662 [83200/345580]\n",
      "Train loss: 1.028691 [86400/345580]\n",
      "Train loss: 1.083148 [89600/345580]\n",
      "Train loss: 1.011509 [92800/345580]\n",
      "Train loss: 1.166919 [96000/345580]\n",
      "Train loss: 1.020464 [99200/345580]\n",
      "Train loss: 1.073764 [102400/345580]\n",
      "Train loss: 0.932953 [105600/345580]\n",
      "Train loss: 1.126568 [108800/345580]\n",
      "Train loss: 1.046002 [112000/345580]\n",
      "Train loss: 1.075499 [115200/345580]\n",
      "Train loss: 1.050467 [118400/345580]\n",
      "Train loss: 0.955212 [121600/345580]\n",
      "Train loss: 1.046865 [124800/345580]\n",
      "Train loss: 1.066754 [128000/345580]\n",
      "Train loss: 1.050640 [131200/345580]\n",
      "Train loss: 1.019356 [134400/345580]\n",
      "Train loss: 1.100690 [137600/345580]\n",
      "Train loss: 1.115210 [140800/345580]\n",
      "Train loss: 1.013768 [144000/345580]\n",
      "Train loss: 1.053862 [147200/345580]\n",
      "Train loss: 1.081156 [150400/345580]\n",
      "Train loss: 1.072031 [153600/345580]\n",
      "Train loss: 1.038588 [156800/345580]\n",
      "Train loss: 0.954981 [160000/345580]\n",
      "Train loss: 1.026138 [163200/345580]\n",
      "Train loss: 1.085641 [166400/345580]\n",
      "Train loss: 1.003724 [169600/345580]\n",
      "Train loss: 1.005389 [172800/345580]\n",
      "Train loss: 1.050753 [176000/345580]\n",
      "Train loss: 1.043903 [179200/345580]\n",
      "Train loss: 0.989127 [182400/345580]\n",
      "Train loss: 1.029489 [185600/345580]\n",
      "Train loss: 1.019406 [188800/345580]\n",
      "Train loss: 1.090409 [192000/345580]\n",
      "Train loss: 0.965925 [195200/345580]\n",
      "Train loss: 0.920313 [198400/345580]\n",
      "Train loss: 1.114630 [201600/345580]\n",
      "Train loss: 1.027039 [204800/345580]\n",
      "Train loss: 1.032167 [208000/345580]\n",
      "Train loss: 1.108976 [211200/345580]\n",
      "Train loss: 1.042643 [214400/345580]\n",
      "Train loss: 1.082541 [217600/345580]\n",
      "Train loss: 1.037270 [220800/345580]\n",
      "Train loss: 0.954816 [224000/345580]\n",
      "Train loss: 1.013412 [227200/345580]\n",
      "Train loss: 1.139006 [230400/345580]\n",
      "Train loss: 1.081539 [233600/345580]\n",
      "Train loss: 1.052818 [236800/345580]\n",
      "Train loss: 0.959143 [240000/345580]\n",
      "Train loss: 0.935392 [243200/345580]\n",
      "Train loss: 1.048112 [246400/345580]\n",
      "Train loss: 1.016001 [249600/345580]\n",
      "Train loss: 1.123180 [252800/345580]\n",
      "Train loss: 1.048819 [256000/345580]\n",
      "Train loss: 1.059481 [259200/345580]\n",
      "Train loss: 1.076120 [262400/345580]\n",
      "Train loss: 1.078594 [265600/345580]\n",
      "Train loss: 1.007784 [268800/345580]\n",
      "Train loss: 1.010627 [272000/345580]\n",
      "Train loss: 1.077851 [275200/345580]\n",
      "Train loss: 0.929840 [278400/345580]\n",
      "Train loss: 0.981610 [281600/345580]\n",
      "Train loss: 0.950210 [284800/345580]\n",
      "Train loss: 1.086043 [288000/345580]\n",
      "Train loss: 1.010667 [291200/345580]\n",
      "Train loss: 0.996236 [294400/345580]\n",
      "Train loss: 1.013178 [297600/345580]\n",
      "Train loss: 0.934484 [300800/345580]\n",
      "Train loss: 1.017581 [304000/345580]\n",
      "Train loss: 1.041503 [307200/345580]\n",
      "Train loss: 1.050351 [310400/345580]\n",
      "Train loss: 1.027747 [313600/345580]\n",
      "Train loss: 1.023264 [316800/345580]\n",
      "Train loss: 1.025982 [320000/345580]\n",
      "Train loss: 1.084492 [323200/345580]\n",
      "Train loss: 1.039246 [326400/345580]\n",
      "Train loss: 1.069183 [329600/345580]\n",
      "Train loss: 1.062909 [332800/345580]\n",
      "Train loss: 1.018949 [336000/345580]\n",
      "Train loss: 1.018536 [339200/345580]\n",
      "Train loss: 1.025909 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.042236 \n",
      "\n",
      "Epoch 74\n",
      " -------------------\n",
      "Train loss: 1.027389 [    0/345580]\n",
      "Train loss: 1.080206 [ 3200/345580]\n",
      "Train loss: 1.072887 [ 6400/345580]\n",
      "Train loss: 1.020280 [ 9600/345580]\n",
      "Train loss: 1.132401 [12800/345580]\n",
      "Train loss: 0.943482 [16000/345580]\n",
      "Train loss: 1.078001 [19200/345580]\n",
      "Train loss: 1.082053 [22400/345580]\n",
      "Train loss: 1.057328 [25600/345580]\n",
      "Train loss: 1.015297 [28800/345580]\n",
      "Train loss: 1.038426 [32000/345580]\n",
      "Train loss: 1.060897 [35200/345580]\n",
      "Train loss: 0.976653 [38400/345580]\n",
      "Train loss: 1.021071 [41600/345580]\n",
      "Train loss: 1.063020 [44800/345580]\n",
      "Train loss: 1.109155 [48000/345580]\n",
      "Train loss: 0.985135 [51200/345580]\n",
      "Train loss: 0.999027 [54400/345580]\n",
      "Train loss: 1.037957 [57600/345580]\n",
      "Train loss: 1.060572 [60800/345580]\n",
      "Train loss: 1.066200 [64000/345580]\n",
      "Train loss: 1.026301 [67200/345580]\n",
      "Train loss: 1.045498 [70400/345580]\n",
      "Train loss: 1.013888 [73600/345580]\n",
      "Train loss: 1.087007 [76800/345580]\n",
      "Train loss: 0.992477 [80000/345580]\n",
      "Train loss: 1.143576 [83200/345580]\n",
      "Train loss: 0.993799 [86400/345580]\n",
      "Train loss: 1.004277 [89600/345580]\n",
      "Train loss: 1.147532 [92800/345580]\n",
      "Train loss: 0.924955 [96000/345580]\n",
      "Train loss: 0.994640 [99200/345580]\n",
      "Train loss: 1.010997 [102400/345580]\n",
      "Train loss: 1.020622 [105600/345580]\n",
      "Train loss: 1.024266 [108800/345580]\n",
      "Train loss: 1.050417 [112000/345580]\n",
      "Train loss: 1.026278 [115200/345580]\n",
      "Train loss: 1.041984 [118400/345580]\n",
      "Train loss: 1.036442 [121600/345580]\n",
      "Train loss: 1.047237 [124800/345580]\n",
      "Train loss: 0.924309 [128000/345580]\n",
      "Train loss: 1.063105 [131200/345580]\n",
      "Train loss: 1.050550 [134400/345580]\n",
      "Train loss: 1.041497 [137600/345580]\n",
      "Train loss: 1.106333 [140800/345580]\n",
      "Train loss: 0.972684 [144000/345580]\n",
      "Train loss: 1.082308 [147200/345580]\n",
      "Train loss: 1.083289 [150400/345580]\n",
      "Train loss: 1.039592 [153600/345580]\n",
      "Train loss: 1.101201 [156800/345580]\n",
      "Train loss: 0.977750 [160000/345580]\n",
      "Train loss: 1.018367 [163200/345580]\n",
      "Train loss: 1.025102 [166400/345580]\n",
      "Train loss: 1.003978 [169600/345580]\n",
      "Train loss: 0.983374 [172800/345580]\n",
      "Train loss: 1.085395 [176000/345580]\n",
      "Train loss: 0.954281 [179200/345580]\n",
      "Train loss: 1.004518 [182400/345580]\n",
      "Train loss: 1.110502 [185600/345580]\n",
      "Train loss: 0.989898 [188800/345580]\n",
      "Train loss: 1.046511 [192000/345580]\n",
      "Train loss: 1.066461 [195200/345580]\n",
      "Train loss: 0.915155 [198400/345580]\n",
      "Train loss: 1.056320 [201600/345580]\n",
      "Train loss: 1.062452 [204800/345580]\n",
      "Train loss: 0.999904 [208000/345580]\n",
      "Train loss: 0.990632 [211200/345580]\n",
      "Train loss: 1.028800 [214400/345580]\n",
      "Train loss: 1.026510 [217600/345580]\n",
      "Train loss: 1.093755 [220800/345580]\n",
      "Train loss: 1.104897 [224000/345580]\n",
      "Train loss: 1.059620 [227200/345580]\n",
      "Train loss: 1.052660 [230400/345580]\n",
      "Train loss: 1.063401 [233600/345580]\n",
      "Train loss: 1.068339 [236800/345580]\n",
      "Train loss: 0.981834 [240000/345580]\n",
      "Train loss: 1.003085 [243200/345580]\n",
      "Train loss: 0.917140 [246400/345580]\n",
      "Train loss: 1.048356 [249600/345580]\n",
      "Train loss: 1.038232 [252800/345580]\n",
      "Train loss: 1.048146 [256000/345580]\n",
      "Train loss: 0.996434 [259200/345580]\n",
      "Train loss: 1.056527 [262400/345580]\n",
      "Train loss: 1.040613 [265600/345580]\n",
      "Train loss: 0.974107 [268800/345580]\n",
      "Train loss: 1.134661 [272000/345580]\n",
      "Train loss: 1.145810 [275200/345580]\n",
      "Train loss: 1.094423 [278400/345580]\n",
      "Train loss: 0.988825 [281600/345580]\n",
      "Train loss: 0.967436 [284800/345580]\n",
      "Train loss: 1.105560 [288000/345580]\n",
      "Train loss: 1.015731 [291200/345580]\n",
      "Train loss: 1.092476 [294400/345580]\n",
      "Train loss: 1.075817 [297600/345580]\n",
      "Train loss: 1.125806 [300800/345580]\n",
      "Train loss: 1.172290 [304000/345580]\n",
      "Train loss: 1.124290 [307200/345580]\n",
      "Train loss: 1.052894 [310400/345580]\n",
      "Train loss: 1.077508 [313600/345580]\n",
      "Train loss: 1.072037 [316800/345580]\n",
      "Train loss: 1.064432 [320000/345580]\n",
      "Train loss: 1.082404 [323200/345580]\n",
      "Train loss: 1.041075 [326400/345580]\n",
      "Train loss: 1.110922 [329600/345580]\n",
      "Train loss: 1.126546 [332800/345580]\n",
      "Train loss: 1.074441 [336000/345580]\n",
      "Train loss: 1.069889 [339200/345580]\n",
      "Train loss: 1.002407 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 42.9%, Avg loss: 1.042475 \n",
      "\n",
      "Epoch 75\n",
      " -------------------\n",
      "Train loss: 1.047462 [    0/345580]\n",
      "Train loss: 1.017555 [ 3200/345580]\n",
      "Train loss: 1.016815 [ 6400/345580]\n",
      "Train loss: 1.063991 [ 9600/345580]\n",
      "Train loss: 1.092160 [12800/345580]\n",
      "Train loss: 1.085412 [16000/345580]\n",
      "Train loss: 1.063196 [19200/345580]\n",
      "Train loss: 1.081675 [22400/345580]\n",
      "Train loss: 1.089820 [25600/345580]\n",
      "Train loss: 1.054948 [28800/345580]\n",
      "Train loss: 0.975987 [32000/345580]\n",
      "Train loss: 1.047645 [35200/345580]\n",
      "Train loss: 1.033130 [38400/345580]\n",
      "Train loss: 1.059580 [41600/345580]\n",
      "Train loss: 1.027914 [44800/345580]\n",
      "Train loss: 0.964118 [48000/345580]\n",
      "Train loss: 0.912139 [51200/345580]\n",
      "Train loss: 1.039366 [54400/345580]\n",
      "Train loss: 1.126397 [57600/345580]\n",
      "Train loss: 0.982989 [60800/345580]\n",
      "Train loss: 1.041723 [64000/345580]\n",
      "Train loss: 1.073745 [67200/345580]\n",
      "Train loss: 1.139775 [70400/345580]\n",
      "Train loss: 0.974674 [73600/345580]\n",
      "Train loss: 1.066617 [76800/345580]\n",
      "Train loss: 1.076428 [80000/345580]\n",
      "Train loss: 1.062125 [83200/345580]\n",
      "Train loss: 1.060000 [86400/345580]\n",
      "Train loss: 1.043931 [89600/345580]\n",
      "Train loss: 1.091399 [92800/345580]\n",
      "Train loss: 1.100368 [96000/345580]\n",
      "Train loss: 1.032365 [99200/345580]\n",
      "Train loss: 1.027280 [102400/345580]\n",
      "Train loss: 0.982907 [105600/345580]\n",
      "Train loss: 1.058615 [108800/345580]\n",
      "Train loss: 1.097307 [112000/345580]\n",
      "Train loss: 0.997353 [115200/345580]\n",
      "Train loss: 1.082359 [118400/345580]\n",
      "Train loss: 1.113676 [121600/345580]\n",
      "Train loss: 1.020873 [124800/345580]\n",
      "Train loss: 1.095951 [128000/345580]\n",
      "Train loss: 1.067153 [131200/345580]\n",
      "Train loss: 1.031800 [134400/345580]\n",
      "Train loss: 0.901260 [137600/345580]\n",
      "Train loss: 1.032978 [140800/345580]\n",
      "Train loss: 1.064873 [144000/345580]\n",
      "Train loss: 1.096232 [147200/345580]\n",
      "Train loss: 0.988973 [150400/345580]\n",
      "Train loss: 1.042727 [153600/345580]\n",
      "Train loss: 1.060459 [156800/345580]\n",
      "Train loss: 1.095925 [160000/345580]\n",
      "Train loss: 0.961467 [163200/345580]\n",
      "Train loss: 1.023261 [166400/345580]\n",
      "Train loss: 1.058268 [169600/345580]\n",
      "Train loss: 1.013498 [172800/345580]\n",
      "Train loss: 1.073373 [176000/345580]\n",
      "Train loss: 0.988649 [179200/345580]\n",
      "Train loss: 1.045322 [182400/345580]\n",
      "Train loss: 1.059150 [185600/345580]\n",
      "Train loss: 0.968567 [188800/345580]\n",
      "Train loss: 1.102270 [192000/345580]\n",
      "Train loss: 1.088196 [195200/345580]\n",
      "Train loss: 1.127494 [198400/345580]\n",
      "Train loss: 1.038898 [201600/345580]\n",
      "Train loss: 1.154534 [204800/345580]\n",
      "Train loss: 1.115364 [208000/345580]\n",
      "Train loss: 1.121003 [211200/345580]\n",
      "Train loss: 1.101163 [214400/345580]\n",
      "Train loss: 1.101713 [217600/345580]\n",
      "Train loss: 1.071990 [220800/345580]\n",
      "Train loss: 1.073051 [224000/345580]\n",
      "Train loss: 1.110253 [227200/345580]\n",
      "Train loss: 1.032312 [230400/345580]\n",
      "Train loss: 1.065790 [233600/345580]\n",
      "Train loss: 1.140173 [236800/345580]\n",
      "Train loss: 1.018145 [240000/345580]\n",
      "Train loss: 1.063898 [243200/345580]\n",
      "Train loss: 1.055791 [246400/345580]\n",
      "Train loss: 1.056880 [249600/345580]\n",
      "Train loss: 0.993765 [252800/345580]\n",
      "Train loss: 1.007196 [256000/345580]\n",
      "Train loss: 1.030377 [259200/345580]\n",
      "Train loss: 0.991187 [262400/345580]\n",
      "Train loss: 1.146774 [265600/345580]\n",
      "Train loss: 1.021150 [268800/345580]\n",
      "Train loss: 0.956213 [272000/345580]\n",
      "Train loss: 0.961949 [275200/345580]\n",
      "Train loss: 0.996196 [278400/345580]\n",
      "Train loss: 1.082681 [281600/345580]\n",
      "Train loss: 1.038500 [284800/345580]\n",
      "Train loss: 1.011556 [288000/345580]\n",
      "Train loss: 1.037384 [291200/345580]\n",
      "Train loss: 1.090850 [294400/345580]\n",
      "Train loss: 0.992469 [297600/345580]\n",
      "Train loss: 0.975501 [300800/345580]\n",
      "Train loss: 1.058413 [304000/345580]\n",
      "Train loss: 1.060231 [307200/345580]\n",
      "Train loss: 1.025437 [310400/345580]\n",
      "Train loss: 1.101782 [313600/345580]\n",
      "Train loss: 0.988955 [316800/345580]\n",
      "Train loss: 1.129465 [320000/345580]\n",
      "Train loss: 0.941159 [323200/345580]\n",
      "Train loss: 1.054170 [326400/345580]\n",
      "Train loss: 1.102928 [329600/345580]\n",
      "Train loss: 1.059450 [332800/345580]\n",
      "Train loss: 0.980449 [336000/345580]\n",
      "Train loss: 1.110234 [339200/345580]\n",
      "Train loss: 0.940035 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.041338 \n",
      "\n",
      "Epoch 76\n",
      " -------------------\n",
      "Train loss: 1.080824 [    0/345580]\n",
      "Train loss: 1.074721 [ 3200/345580]\n",
      "Train loss: 1.099646 [ 6400/345580]\n",
      "Train loss: 1.072621 [ 9600/345580]\n",
      "Train loss: 1.001468 [12800/345580]\n",
      "Train loss: 0.989531 [16000/345580]\n",
      "Train loss: 1.068669 [19200/345580]\n",
      "Train loss: 1.080758 [22400/345580]\n",
      "Train loss: 0.955647 [25600/345580]\n",
      "Train loss: 1.044880 [28800/345580]\n",
      "Train loss: 1.064692 [32000/345580]\n",
      "Train loss: 1.028179 [35200/345580]\n",
      "Train loss: 1.079736 [38400/345580]\n",
      "Train loss: 1.027603 [41600/345580]\n",
      "Train loss: 1.011280 [44800/345580]\n",
      "Train loss: 1.032451 [48000/345580]\n",
      "Train loss: 1.004418 [51200/345580]\n",
      "Train loss: 1.074926 [54400/345580]\n",
      "Train loss: 1.171400 [57600/345580]\n",
      "Train loss: 1.068216 [60800/345580]\n",
      "Train loss: 0.924405 [64000/345580]\n",
      "Train loss: 1.100311 [67200/345580]\n",
      "Train loss: 1.031708 [70400/345580]\n",
      "Train loss: 1.055226 [73600/345580]\n",
      "Train loss: 1.119570 [76800/345580]\n",
      "Train loss: 1.104629 [80000/345580]\n",
      "Train loss: 1.107512 [83200/345580]\n",
      "Train loss: 1.079498 [86400/345580]\n",
      "Train loss: 1.027242 [89600/345580]\n",
      "Train loss: 1.064465 [92800/345580]\n",
      "Train loss: 1.010888 [96000/345580]\n",
      "Train loss: 0.962837 [99200/345580]\n",
      "Train loss: 1.120914 [102400/345580]\n",
      "Train loss: 1.061559 [105600/345580]\n",
      "Train loss: 0.962391 [108800/345580]\n",
      "Train loss: 1.106485 [112000/345580]\n",
      "Train loss: 1.061421 [115200/345580]\n",
      "Train loss: 0.990619 [118400/345580]\n",
      "Train loss: 1.048031 [121600/345580]\n",
      "Train loss: 1.016959 [124800/345580]\n",
      "Train loss: 1.088015 [128000/345580]\n",
      "Train loss: 1.032901 [131200/345580]\n",
      "Train loss: 1.082009 [134400/345580]\n",
      "Train loss: 0.994080 [137600/345580]\n",
      "Train loss: 1.044838 [140800/345580]\n",
      "Train loss: 1.058080 [144000/345580]\n",
      "Train loss: 1.075552 [147200/345580]\n",
      "Train loss: 1.061942 [150400/345580]\n",
      "Train loss: 0.965390 [153600/345580]\n",
      "Train loss: 0.992292 [156800/345580]\n",
      "Train loss: 1.063784 [160000/345580]\n",
      "Train loss: 1.043734 [163200/345580]\n",
      "Train loss: 1.123217 [166400/345580]\n",
      "Train loss: 1.040926 [169600/345580]\n",
      "Train loss: 0.926476 [172800/345580]\n",
      "Train loss: 1.118513 [176000/345580]\n",
      "Train loss: 1.056237 [179200/345580]\n",
      "Train loss: 1.057157 [182400/345580]\n",
      "Train loss: 0.998367 [185600/345580]\n",
      "Train loss: 0.976737 [188800/345580]\n",
      "Train loss: 1.002924 [192000/345580]\n",
      "Train loss: 1.132250 [195200/345580]\n",
      "Train loss: 1.109653 [198400/345580]\n",
      "Train loss: 0.917468 [201600/345580]\n",
      "Train loss: 1.022796 [204800/345580]\n",
      "Train loss: 0.906483 [208000/345580]\n",
      "Train loss: 0.910064 [211200/345580]\n",
      "Train loss: 1.108349 [214400/345580]\n",
      "Train loss: 1.148549 [217600/345580]\n",
      "Train loss: 1.020124 [220800/345580]\n",
      "Train loss: 0.999086 [224000/345580]\n",
      "Train loss: 1.003817 [227200/345580]\n",
      "Train loss: 0.989744 [230400/345580]\n",
      "Train loss: 0.987349 [233600/345580]\n",
      "Train loss: 1.033815 [236800/345580]\n",
      "Train loss: 1.103588 [240000/345580]\n",
      "Train loss: 1.050451 [243200/345580]\n",
      "Train loss: 1.032480 [246400/345580]\n",
      "Train loss: 1.091588 [249600/345580]\n",
      "Train loss: 0.943797 [252800/345580]\n",
      "Train loss: 1.093171 [256000/345580]\n",
      "Train loss: 1.116440 [259200/345580]\n",
      "Train loss: 0.990580 [262400/345580]\n",
      "Train loss: 0.991460 [265600/345580]\n",
      "Train loss: 1.018192 [268800/345580]\n",
      "Train loss: 1.042728 [272000/345580]\n",
      "Train loss: 1.018763 [275200/345580]\n",
      "Train loss: 0.962529 [278400/345580]\n",
      "Train loss: 1.090886 [281600/345580]\n",
      "Train loss: 0.960699 [284800/345580]\n",
      "Train loss: 1.106190 [288000/345580]\n",
      "Train loss: 1.015355 [291200/345580]\n",
      "Train loss: 0.984097 [294400/345580]\n",
      "Train loss: 1.024280 [297600/345580]\n",
      "Train loss: 1.037971 [300800/345580]\n",
      "Train loss: 1.024271 [304000/345580]\n",
      "Train loss: 1.033210 [307200/345580]\n",
      "Train loss: 1.011359 [310400/345580]\n",
      "Train loss: 1.093727 [313600/345580]\n",
      "Train loss: 1.084141 [316800/345580]\n",
      "Train loss: 0.947046 [320000/345580]\n",
      "Train loss: 0.906992 [323200/345580]\n",
      "Train loss: 1.101495 [326400/345580]\n",
      "Train loss: 1.035750 [329600/345580]\n",
      "Train loss: 1.060502 [332800/345580]\n",
      "Train loss: 1.000454 [336000/345580]\n",
      "Train loss: 1.107973 [339200/345580]\n",
      "Train loss: 1.004407 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.041600 \n",
      "\n",
      "Epoch 77\n",
      " -------------------\n",
      "Train loss: 1.103595 [    0/345580]\n",
      "Train loss: 0.996262 [ 3200/345580]\n",
      "Train loss: 1.020913 [ 6400/345580]\n",
      "Train loss: 1.035322 [ 9600/345580]\n",
      "Train loss: 1.076165 [12800/345580]\n",
      "Train loss: 1.059487 [16000/345580]\n",
      "Train loss: 1.129639 [19200/345580]\n",
      "Train loss: 1.009771 [22400/345580]\n",
      "Train loss: 1.101930 [25600/345580]\n",
      "Train loss: 0.997142 [28800/345580]\n",
      "Train loss: 1.115712 [32000/345580]\n",
      "Train loss: 1.013379 [35200/345580]\n",
      "Train loss: 1.073727 [38400/345580]\n",
      "Train loss: 0.979291 [41600/345580]\n",
      "Train loss: 1.137609 [44800/345580]\n",
      "Train loss: 1.021080 [48000/345580]\n",
      "Train loss: 1.057034 [51200/345580]\n",
      "Train loss: 1.089838 [54400/345580]\n",
      "Train loss: 0.901543 [57600/345580]\n",
      "Train loss: 1.128449 [60800/345580]\n",
      "Train loss: 1.103652 [64000/345580]\n",
      "Train loss: 1.067819 [67200/345580]\n",
      "Train loss: 1.009989 [70400/345580]\n",
      "Train loss: 1.038259 [73600/345580]\n",
      "Train loss: 1.082804 [76800/345580]\n",
      "Train loss: 1.145355 [80000/345580]\n",
      "Train loss: 1.061473 [83200/345580]\n",
      "Train loss: 1.114779 [86400/345580]\n",
      "Train loss: 1.038043 [89600/345580]\n",
      "Train loss: 1.044508 [92800/345580]\n",
      "Train loss: 1.021821 [96000/345580]\n",
      "Train loss: 0.962156 [99200/345580]\n",
      "Train loss: 1.067593 [102400/345580]\n",
      "Train loss: 1.105005 [105600/345580]\n",
      "Train loss: 0.998251 [108800/345580]\n",
      "Train loss: 1.048805 [112000/345580]\n",
      "Train loss: 1.106012 [115200/345580]\n",
      "Train loss: 1.017731 [118400/345580]\n",
      "Train loss: 1.043547 [121600/345580]\n",
      "Train loss: 0.931245 [124800/345580]\n",
      "Train loss: 0.997367 [128000/345580]\n",
      "Train loss: 1.035674 [131200/345580]\n",
      "Train loss: 1.037544 [134400/345580]\n",
      "Train loss: 1.162998 [137600/345580]\n",
      "Train loss: 1.009807 [140800/345580]\n",
      "Train loss: 1.032286 [144000/345580]\n",
      "Train loss: 1.102368 [147200/345580]\n",
      "Train loss: 1.028578 [150400/345580]\n",
      "Train loss: 1.019822 [153600/345580]\n",
      "Train loss: 0.992286 [156800/345580]\n",
      "Train loss: 1.020533 [160000/345580]\n",
      "Train loss: 0.976955 [163200/345580]\n",
      "Train loss: 1.002420 [166400/345580]\n",
      "Train loss: 1.026633 [169600/345580]\n",
      "Train loss: 1.023945 [172800/345580]\n",
      "Train loss: 1.011194 [176000/345580]\n",
      "Train loss: 1.102915 [179200/345580]\n",
      "Train loss: 1.050599 [182400/345580]\n",
      "Train loss: 1.142463 [185600/345580]\n",
      "Train loss: 1.067962 [188800/345580]\n",
      "Train loss: 1.089236 [192000/345580]\n",
      "Train loss: 1.046830 [195200/345580]\n",
      "Train loss: 0.977288 [198400/345580]\n",
      "Train loss: 1.105024 [201600/345580]\n",
      "Train loss: 1.104944 [204800/345580]\n",
      "Train loss: 1.077993 [208000/345580]\n",
      "Train loss: 1.115488 [211200/345580]\n",
      "Train loss: 1.010611 [214400/345580]\n",
      "Train loss: 1.083194 [217600/345580]\n",
      "Train loss: 1.037170 [220800/345580]\n",
      "Train loss: 1.063261 [224000/345580]\n",
      "Train loss: 1.120142 [227200/345580]\n",
      "Train loss: 0.994513 [230400/345580]\n",
      "Train loss: 1.110775 [233600/345580]\n",
      "Train loss: 1.052796 [236800/345580]\n",
      "Train loss: 1.065072 [240000/345580]\n",
      "Train loss: 1.001537 [243200/345580]\n",
      "Train loss: 0.957297 [246400/345580]\n",
      "Train loss: 0.935617 [249600/345580]\n",
      "Train loss: 0.946964 [252800/345580]\n",
      "Train loss: 1.017604 [256000/345580]\n",
      "Train loss: 1.121919 [259200/345580]\n",
      "Train loss: 1.074296 [262400/345580]\n",
      "Train loss: 0.989094 [265600/345580]\n",
      "Train loss: 1.016796 [268800/345580]\n",
      "Train loss: 1.066912 [272000/345580]\n",
      "Train loss: 1.056306 [275200/345580]\n",
      "Train loss: 0.909976 [278400/345580]\n",
      "Train loss: 1.024563 [281600/345580]\n",
      "Train loss: 1.100746 [284800/345580]\n",
      "Train loss: 1.243582 [288000/345580]\n",
      "Train loss: 0.916256 [291200/345580]\n",
      "Train loss: 1.068806 [294400/345580]\n",
      "Train loss: 1.089246 [297600/345580]\n",
      "Train loss: 1.015584 [300800/345580]\n",
      "Train loss: 1.081908 [304000/345580]\n",
      "Train loss: 0.940432 [307200/345580]\n",
      "Train loss: 1.017116 [310400/345580]\n",
      "Train loss: 1.062589 [313600/345580]\n",
      "Train loss: 1.136830 [316800/345580]\n",
      "Train loss: 1.082853 [320000/345580]\n",
      "Train loss: 1.112600 [323200/345580]\n",
      "Train loss: 1.084625 [326400/345580]\n",
      "Train loss: 1.015096 [329600/345580]\n",
      "Train loss: 1.025799 [332800/345580]\n",
      "Train loss: 0.981621 [336000/345580]\n",
      "Train loss: 1.005063 [339200/345580]\n",
      "Train loss: 0.942862 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 42.9%, Avg loss: 1.041957 \n",
      "\n",
      "Epoch 78\n",
      " -------------------\n",
      "Train loss: 1.059557 [    0/345580]\n",
      "Train loss: 1.020177 [ 3200/345580]\n",
      "Train loss: 0.972716 [ 6400/345580]\n",
      "Train loss: 1.007073 [ 9600/345580]\n",
      "Train loss: 1.136001 [12800/345580]\n",
      "Train loss: 1.050775 [16000/345580]\n",
      "Train loss: 1.057121 [19200/345580]\n",
      "Train loss: 1.076372 [22400/345580]\n",
      "Train loss: 1.070675 [25600/345580]\n",
      "Train loss: 0.999593 [28800/345580]\n",
      "Train loss: 1.088254 [32000/345580]\n",
      "Train loss: 1.104928 [35200/345580]\n",
      "Train loss: 1.049943 [38400/345580]\n",
      "Train loss: 1.040651 [41600/345580]\n",
      "Train loss: 0.989166 [44800/345580]\n",
      "Train loss: 1.019567 [48000/345580]\n",
      "Train loss: 1.013814 [51200/345580]\n",
      "Train loss: 1.026697 [54400/345580]\n",
      "Train loss: 1.121206 [57600/345580]\n",
      "Train loss: 0.939708 [60800/345580]\n",
      "Train loss: 1.163836 [64000/345580]\n",
      "Train loss: 1.185312 [67200/345580]\n",
      "Train loss: 1.061733 [70400/345580]\n",
      "Train loss: 1.059199 [73600/345580]\n",
      "Train loss: 1.000997 [76800/345580]\n",
      "Train loss: 1.061935 [80000/345580]\n",
      "Train loss: 1.047365 [83200/345580]\n",
      "Train loss: 1.090838 [86400/345580]\n",
      "Train loss: 1.157774 [89600/345580]\n",
      "Train loss: 1.109709 [92800/345580]\n",
      "Train loss: 1.035320 [96000/345580]\n",
      "Train loss: 1.061809 [99200/345580]\n",
      "Train loss: 1.021342 [102400/345580]\n",
      "Train loss: 1.015722 [105600/345580]\n",
      "Train loss: 0.996196 [108800/345580]\n",
      "Train loss: 1.073990 [112000/345580]\n",
      "Train loss: 1.110323 [115200/345580]\n",
      "Train loss: 1.016014 [118400/345580]\n",
      "Train loss: 1.016367 [121600/345580]\n",
      "Train loss: 1.069877 [124800/345580]\n",
      "Train loss: 1.037366 [128000/345580]\n",
      "Train loss: 1.072163 [131200/345580]\n",
      "Train loss: 1.040525 [134400/345580]\n",
      "Train loss: 1.050892 [137600/345580]\n",
      "Train loss: 0.996962 [140800/345580]\n",
      "Train loss: 1.055233 [144000/345580]\n",
      "Train loss: 0.926520 [147200/345580]\n",
      "Train loss: 0.989223 [150400/345580]\n",
      "Train loss: 0.979480 [153600/345580]\n",
      "Train loss: 1.030813 [156800/345580]\n",
      "Train loss: 1.091531 [160000/345580]\n",
      "Train loss: 1.065356 [163200/345580]\n",
      "Train loss: 1.036157 [166400/345580]\n",
      "Train loss: 1.002121 [169600/345580]\n",
      "Train loss: 1.060393 [172800/345580]\n",
      "Train loss: 1.031590 [176000/345580]\n",
      "Train loss: 1.208680 [179200/345580]\n",
      "Train loss: 1.069400 [182400/345580]\n",
      "Train loss: 1.043018 [185600/345580]\n",
      "Train loss: 1.065421 [188800/345580]\n",
      "Train loss: 1.068693 [192000/345580]\n",
      "Train loss: 1.078636 [195200/345580]\n",
      "Train loss: 1.131282 [198400/345580]\n",
      "Train loss: 0.994924 [201600/345580]\n",
      "Train loss: 1.079208 [204800/345580]\n",
      "Train loss: 1.069170 [208000/345580]\n",
      "Train loss: 1.133512 [211200/345580]\n",
      "Train loss: 0.973846 [214400/345580]\n",
      "Train loss: 1.009879 [217600/345580]\n",
      "Train loss: 1.161458 [220800/345580]\n",
      "Train loss: 1.128097 [224000/345580]\n",
      "Train loss: 1.168167 [227200/345580]\n",
      "Train loss: 1.128137 [230400/345580]\n",
      "Train loss: 1.134622 [233600/345580]\n",
      "Train loss: 1.023633 [236800/345580]\n",
      "Train loss: 0.983463 [240000/345580]\n",
      "Train loss: 1.132244 [243200/345580]\n",
      "Train loss: 0.975303 [246400/345580]\n",
      "Train loss: 1.103108 [249600/345580]\n",
      "Train loss: 1.148156 [252800/345580]\n",
      "Train loss: 1.053562 [256000/345580]\n",
      "Train loss: 1.094710 [259200/345580]\n",
      "Train loss: 0.991809 [262400/345580]\n",
      "Train loss: 1.006653 [265600/345580]\n",
      "Train loss: 1.110492 [268800/345580]\n",
      "Train loss: 1.013397 [272000/345580]\n",
      "Train loss: 1.082223 [275200/345580]\n",
      "Train loss: 1.082491 [278400/345580]\n",
      "Train loss: 1.057796 [281600/345580]\n",
      "Train loss: 1.020911 [284800/345580]\n",
      "Train loss: 1.092105 [288000/345580]\n",
      "Train loss: 1.030756 [291200/345580]\n",
      "Train loss: 0.980317 [294400/345580]\n",
      "Train loss: 1.030086 [297600/345580]\n",
      "Train loss: 0.970813 [300800/345580]\n",
      "Train loss: 1.001056 [304000/345580]\n",
      "Train loss: 1.073828 [307200/345580]\n",
      "Train loss: 1.087377 [310400/345580]\n",
      "Train loss: 1.045123 [313600/345580]\n",
      "Train loss: 1.050287 [316800/345580]\n",
      "Train loss: 1.065911 [320000/345580]\n",
      "Train loss: 1.112208 [323200/345580]\n",
      "Train loss: 1.089948 [326400/345580]\n",
      "Train loss: 1.053725 [329600/345580]\n",
      "Train loss: 0.945421 [332800/345580]\n",
      "Train loss: 1.029019 [336000/345580]\n",
      "Train loss: 1.110912 [339200/345580]\n",
      "Train loss: 1.100920 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.042232 \n",
      "\n",
      "Epoch 79\n",
      " -------------------\n",
      "Train loss: 1.010858 [    0/345580]\n",
      "Train loss: 1.069926 [ 3200/345580]\n",
      "Train loss: 1.014895 [ 6400/345580]\n",
      "Train loss: 0.964103 [ 9600/345580]\n",
      "Train loss: 1.054468 [12800/345580]\n",
      "Train loss: 1.105281 [16000/345580]\n",
      "Train loss: 1.113174 [19200/345580]\n",
      "Train loss: 0.888069 [22400/345580]\n",
      "Train loss: 1.047318 [25600/345580]\n",
      "Train loss: 0.969317 [28800/345580]\n",
      "Train loss: 1.116600 [32000/345580]\n",
      "Train loss: 1.099814 [35200/345580]\n",
      "Train loss: 1.019150 [38400/345580]\n",
      "Train loss: 1.013351 [41600/345580]\n",
      "Train loss: 1.039262 [44800/345580]\n",
      "Train loss: 1.118027 [48000/345580]\n",
      "Train loss: 1.026885 [51200/345580]\n",
      "Train loss: 1.112029 [54400/345580]\n",
      "Train loss: 1.056387 [57600/345580]\n",
      "Train loss: 1.040662 [60800/345580]\n",
      "Train loss: 1.084753 [64000/345580]\n",
      "Train loss: 1.068767 [67200/345580]\n",
      "Train loss: 1.074150 [70400/345580]\n",
      "Train loss: 1.086555 [73600/345580]\n",
      "Train loss: 1.103408 [76800/345580]\n",
      "Train loss: 1.002577 [80000/345580]\n",
      "Train loss: 1.092808 [83200/345580]\n",
      "Train loss: 1.074249 [86400/345580]\n",
      "Train loss: 0.969406 [89600/345580]\n",
      "Train loss: 1.054430 [92800/345580]\n",
      "Train loss: 1.066851 [96000/345580]\n",
      "Train loss: 1.039668 [99200/345580]\n",
      "Train loss: 1.133394 [102400/345580]\n",
      "Train loss: 1.068509 [105600/345580]\n",
      "Train loss: 1.044439 [108800/345580]\n",
      "Train loss: 1.066223 [112000/345580]\n",
      "Train loss: 0.941349 [115200/345580]\n",
      "Train loss: 1.035685 [118400/345580]\n",
      "Train loss: 1.142476 [121600/345580]\n",
      "Train loss: 0.992654 [124800/345580]\n",
      "Train loss: 1.100500 [128000/345580]\n",
      "Train loss: 1.090191 [131200/345580]\n",
      "Train loss: 1.094177 [134400/345580]\n",
      "Train loss: 1.052531 [137600/345580]\n",
      "Train loss: 1.036899 [140800/345580]\n",
      "Train loss: 1.019075 [144000/345580]\n",
      "Train loss: 1.041740 [147200/345580]\n",
      "Train loss: 1.057958 [150400/345580]\n",
      "Train loss: 1.045675 [153600/345580]\n",
      "Train loss: 1.106660 [156800/345580]\n",
      "Train loss: 1.069127 [160000/345580]\n",
      "Train loss: 1.002891 [163200/345580]\n",
      "Train loss: 1.080198 [166400/345580]\n",
      "Train loss: 1.054048 [169600/345580]\n",
      "Train loss: 1.140001 [172800/345580]\n",
      "Train loss: 0.889189 [176000/345580]\n",
      "Train loss: 1.009919 [179200/345580]\n",
      "Train loss: 1.040613 [182400/345580]\n",
      "Train loss: 1.118387 [185600/345580]\n",
      "Train loss: 1.046492 [188800/345580]\n",
      "Train loss: 1.005533 [192000/345580]\n",
      "Train loss: 0.990901 [195200/345580]\n",
      "Train loss: 0.975943 [198400/345580]\n",
      "Train loss: 1.004058 [201600/345580]\n",
      "Train loss: 1.019239 [204800/345580]\n",
      "Train loss: 1.050583 [208000/345580]\n",
      "Train loss: 1.084904 [211200/345580]\n",
      "Train loss: 1.110919 [214400/345580]\n",
      "Train loss: 1.018342 [217600/345580]\n",
      "Train loss: 1.192210 [220800/345580]\n",
      "Train loss: 1.004012 [224000/345580]\n",
      "Train loss: 1.081985 [227200/345580]\n",
      "Train loss: 1.049661 [230400/345580]\n",
      "Train loss: 1.000513 [233600/345580]\n",
      "Train loss: 1.021255 [236800/345580]\n",
      "Train loss: 1.044972 [240000/345580]\n",
      "Train loss: 1.033576 [243200/345580]\n",
      "Train loss: 1.004655 [246400/345580]\n",
      "Train loss: 1.056199 [249600/345580]\n",
      "Train loss: 1.010500 [252800/345580]\n",
      "Train loss: 1.084120 [256000/345580]\n",
      "Train loss: 1.070507 [259200/345580]\n",
      "Train loss: 1.016645 [262400/345580]\n",
      "Train loss: 1.012075 [265600/345580]\n",
      "Train loss: 1.039828 [268800/345580]\n",
      "Train loss: 1.047433 [272000/345580]\n",
      "Train loss: 0.990841 [275200/345580]\n",
      "Train loss: 0.950021 [278400/345580]\n",
      "Train loss: 1.012102 [281600/345580]\n",
      "Train loss: 0.987848 [284800/345580]\n",
      "Train loss: 1.084731 [288000/345580]\n",
      "Train loss: 0.993929 [291200/345580]\n",
      "Train loss: 1.025666 [294400/345580]\n",
      "Train loss: 1.117991 [297600/345580]\n",
      "Train loss: 1.067330 [300800/345580]\n",
      "Train loss: 1.168305 [304000/345580]\n",
      "Train loss: 1.040250 [307200/345580]\n",
      "Train loss: 1.106551 [310400/345580]\n",
      "Train loss: 1.002764 [313600/345580]\n",
      "Train loss: 1.026344 [316800/345580]\n",
      "Train loss: 0.982664 [320000/345580]\n",
      "Train loss: 1.057598 [323200/345580]\n",
      "Train loss: 1.138181 [326400/345580]\n",
      "Train loss: 0.963533 [329600/345580]\n",
      "Train loss: 1.111503 [332800/345580]\n",
      "Train loss: 1.046441 [336000/345580]\n",
      "Train loss: 0.948008 [339200/345580]\n",
      "Train loss: 1.065115 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.041901 \n",
      "\n",
      "Epoch 80\n",
      " -------------------\n",
      "Train loss: 1.250497 [    0/345580]\n",
      "Train loss: 1.008487 [ 3200/345580]\n",
      "Train loss: 1.066804 [ 6400/345580]\n",
      "Train loss: 0.985576 [ 9600/345580]\n",
      "Train loss: 0.975683 [12800/345580]\n",
      "Train loss: 0.964827 [16000/345580]\n",
      "Train loss: 1.019061 [19200/345580]\n",
      "Train loss: 1.130119 [22400/345580]\n",
      "Train loss: 1.010671 [25600/345580]\n",
      "Train loss: 1.062358 [28800/345580]\n",
      "Train loss: 1.058249 [32000/345580]\n",
      "Train loss: 1.010775 [35200/345580]\n",
      "Train loss: 1.025967 [38400/345580]\n",
      "Train loss: 1.069634 [41600/345580]\n",
      "Train loss: 0.943611 [44800/345580]\n",
      "Train loss: 1.032503 [48000/345580]\n",
      "Train loss: 1.104824 [51200/345580]\n",
      "Train loss: 1.071938 [54400/345580]\n",
      "Train loss: 1.075913 [57600/345580]\n",
      "Train loss: 0.984757 [60800/345580]\n",
      "Train loss: 1.141109 [64000/345580]\n",
      "Train loss: 1.007263 [67200/345580]\n",
      "Train loss: 1.028589 [70400/345580]\n",
      "Train loss: 0.984619 [73600/345580]\n",
      "Train loss: 1.043984 [76800/345580]\n",
      "Train loss: 0.982496 [80000/345580]\n",
      "Train loss: 1.008483 [83200/345580]\n",
      "Train loss: 1.184255 [86400/345580]\n",
      "Train loss: 1.065162 [89600/345580]\n",
      "Train loss: 1.051878 [92800/345580]\n",
      "Train loss: 1.100099 [96000/345580]\n",
      "Train loss: 1.099496 [99200/345580]\n",
      "Train loss: 1.024704 [102400/345580]\n",
      "Train loss: 1.042565 [105600/345580]\n",
      "Train loss: 1.011992 [108800/345580]\n",
      "Train loss: 1.130104 [112000/345580]\n",
      "Train loss: 0.999269 [115200/345580]\n",
      "Train loss: 1.086425 [118400/345580]\n",
      "Train loss: 1.090197 [121600/345580]\n",
      "Train loss: 1.009353 [124800/345580]\n",
      "Train loss: 0.980536 [128000/345580]\n",
      "Train loss: 1.121950 [131200/345580]\n",
      "Train loss: 1.037349 [134400/345580]\n",
      "Train loss: 0.963871 [137600/345580]\n",
      "Train loss: 1.085600 [140800/345580]\n",
      "Train loss: 1.073648 [144000/345580]\n",
      "Train loss: 1.019247 [147200/345580]\n",
      "Train loss: 1.053166 [150400/345580]\n",
      "Train loss: 1.050987 [153600/345580]\n",
      "Train loss: 0.967234 [156800/345580]\n",
      "Train loss: 1.079318 [160000/345580]\n",
      "Train loss: 1.137671 [163200/345580]\n",
      "Train loss: 0.977472 [166400/345580]\n",
      "Train loss: 1.086377 [169600/345580]\n",
      "Train loss: 0.947048 [172800/345580]\n",
      "Train loss: 1.094223 [176000/345580]\n",
      "Train loss: 1.100302 [179200/345580]\n",
      "Train loss: 1.030302 [182400/345580]\n",
      "Train loss: 1.029223 [185600/345580]\n",
      "Train loss: 1.086009 [188800/345580]\n",
      "Train loss: 1.057336 [192000/345580]\n",
      "Train loss: 1.109977 [195200/345580]\n",
      "Train loss: 1.073734 [198400/345580]\n",
      "Train loss: 0.964064 [201600/345580]\n",
      "Train loss: 1.101613 [204800/345580]\n",
      "Train loss: 0.996572 [208000/345580]\n",
      "Train loss: 1.080996 [211200/345580]\n",
      "Train loss: 1.058109 [214400/345580]\n",
      "Train loss: 1.175278 [217600/345580]\n",
      "Train loss: 1.098051 [220800/345580]\n",
      "Train loss: 1.001260 [224000/345580]\n",
      "Train loss: 1.080508 [227200/345580]\n",
      "Train loss: 1.085663 [230400/345580]\n",
      "Train loss: 1.034207 [233600/345580]\n",
      "Train loss: 1.030326 [236800/345580]\n",
      "Train loss: 0.969877 [240000/345580]\n",
      "Train loss: 1.031472 [243200/345580]\n",
      "Train loss: 1.154804 [246400/345580]\n",
      "Train loss: 1.054126 [249600/345580]\n",
      "Train loss: 1.050556 [252800/345580]\n",
      "Train loss: 1.035514 [256000/345580]\n",
      "Train loss: 1.120110 [259200/345580]\n",
      "Train loss: 0.977580 [262400/345580]\n",
      "Train loss: 1.065081 [265600/345580]\n",
      "Train loss: 1.041877 [268800/345580]\n",
      "Train loss: 1.050217 [272000/345580]\n",
      "Train loss: 1.102240 [275200/345580]\n",
      "Train loss: 0.990830 [278400/345580]\n",
      "Train loss: 1.195483 [281600/345580]\n",
      "Train loss: 0.983466 [284800/345580]\n",
      "Train loss: 1.093372 [288000/345580]\n",
      "Train loss: 1.006610 [291200/345580]\n",
      "Train loss: 1.101112 [294400/345580]\n",
      "Train loss: 1.011805 [297600/345580]\n",
      "Train loss: 1.084632 [300800/345580]\n",
      "Train loss: 1.073586 [304000/345580]\n",
      "Train loss: 1.066856 [307200/345580]\n",
      "Train loss: 1.117521 [310400/345580]\n",
      "Train loss: 1.050842 [313600/345580]\n",
      "Train loss: 1.010805 [316800/345580]\n",
      "Train loss: 1.075948 [320000/345580]\n",
      "Train loss: 1.135331 [323200/345580]\n",
      "Train loss: 1.037779 [326400/345580]\n",
      "Train loss: 1.076237 [329600/345580]\n",
      "Train loss: 1.098763 [332800/345580]\n",
      "Train loss: 1.085851 [336000/345580]\n",
      "Train loss: 1.026290 [339200/345580]\n",
      "Train loss: 1.044507 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.2%, Avg loss: 1.041429 \n",
      "\n",
      "Epoch 81\n",
      " -------------------\n",
      "Train loss: 1.133471 [    0/345580]\n",
      "Train loss: 0.956202 [ 3200/345580]\n",
      "Train loss: 0.924230 [ 6400/345580]\n",
      "Train loss: 1.033059 [ 9600/345580]\n",
      "Train loss: 1.066261 [12800/345580]\n",
      "Train loss: 1.117704 [16000/345580]\n",
      "Train loss: 1.037748 [19200/345580]\n",
      "Train loss: 1.001156 [22400/345580]\n",
      "Train loss: 1.014588 [25600/345580]\n",
      "Train loss: 1.069922 [28800/345580]\n",
      "Train loss: 1.110126 [32000/345580]\n",
      "Train loss: 1.032529 [35200/345580]\n",
      "Train loss: 0.971809 [38400/345580]\n",
      "Train loss: 1.027704 [41600/345580]\n",
      "Train loss: 1.035182 [44800/345580]\n",
      "Train loss: 0.989331 [48000/345580]\n",
      "Train loss: 0.987782 [51200/345580]\n",
      "Train loss: 1.030928 [54400/345580]\n",
      "Train loss: 1.078305 [57600/345580]\n",
      "Train loss: 1.048407 [60800/345580]\n",
      "Train loss: 1.051451 [64000/345580]\n",
      "Train loss: 1.061545 [67200/345580]\n",
      "Train loss: 0.963237 [70400/345580]\n",
      "Train loss: 1.054432 [73600/345580]\n",
      "Train loss: 1.063733 [76800/345580]\n",
      "Train loss: 0.940715 [80000/345580]\n",
      "Train loss: 1.101265 [83200/345580]\n",
      "Train loss: 1.041765 [86400/345580]\n",
      "Train loss: 0.940247 [89600/345580]\n",
      "Train loss: 0.983577 [92800/345580]\n",
      "Train loss: 0.961105 [96000/345580]\n",
      "Train loss: 1.009699 [99200/345580]\n",
      "Train loss: 1.009252 [102400/345580]\n",
      "Train loss: 1.016044 [105600/345580]\n",
      "Train loss: 1.099512 [108800/345580]\n",
      "Train loss: 1.001337 [112000/345580]\n",
      "Train loss: 1.012678 [115200/345580]\n",
      "Train loss: 1.060033 [118400/345580]\n",
      "Train loss: 1.126158 [121600/345580]\n",
      "Train loss: 1.065783 [124800/345580]\n",
      "Train loss: 1.038668 [128000/345580]\n",
      "Train loss: 1.067758 [131200/345580]\n",
      "Train loss: 1.090598 [134400/345580]\n",
      "Train loss: 0.980835 [137600/345580]\n",
      "Train loss: 1.097809 [140800/345580]\n",
      "Train loss: 1.043969 [144000/345580]\n",
      "Train loss: 1.166831 [147200/345580]\n",
      "Train loss: 1.112668 [150400/345580]\n",
      "Train loss: 1.033658 [153600/345580]\n",
      "Train loss: 1.104508 [156800/345580]\n",
      "Train loss: 1.025839 [160000/345580]\n",
      "Train loss: 1.068198 [163200/345580]\n",
      "Train loss: 1.118952 [166400/345580]\n",
      "Train loss: 1.055259 [169600/345580]\n",
      "Train loss: 1.046253 [172800/345580]\n",
      "Train loss: 1.088943 [176000/345580]\n",
      "Train loss: 1.096826 [179200/345580]\n",
      "Train loss: 1.073466 [182400/345580]\n",
      "Train loss: 1.099869 [185600/345580]\n",
      "Train loss: 1.112707 [188800/345580]\n",
      "Train loss: 1.050447 [192000/345580]\n",
      "Train loss: 1.031119 [195200/345580]\n",
      "Train loss: 1.044057 [198400/345580]\n",
      "Train loss: 1.114836 [201600/345580]\n",
      "Train loss: 1.102414 [204800/345580]\n",
      "Train loss: 1.011586 [208000/345580]\n",
      "Train loss: 1.016509 [211200/345580]\n",
      "Train loss: 0.962146 [214400/345580]\n",
      "Train loss: 1.060135 [217600/345580]\n",
      "Train loss: 0.953899 [220800/345580]\n",
      "Train loss: 1.042729 [224000/345580]\n",
      "Train loss: 0.976009 [227200/345580]\n",
      "Train loss: 0.929401 [230400/345580]\n",
      "Train loss: 1.106999 [233600/345580]\n",
      "Train loss: 1.048317 [236800/345580]\n",
      "Train loss: 1.061847 [240000/345580]\n",
      "Train loss: 1.005913 [243200/345580]\n",
      "Train loss: 1.004018 [246400/345580]\n",
      "Train loss: 1.160031 [249600/345580]\n",
      "Train loss: 0.999903 [252800/345580]\n",
      "Train loss: 0.957023 [256000/345580]\n",
      "Train loss: 0.977707 [259200/345580]\n",
      "Train loss: 0.982340 [262400/345580]\n",
      "Train loss: 1.001742 [265600/345580]\n",
      "Train loss: 1.112524 [268800/345580]\n",
      "Train loss: 0.969354 [272000/345580]\n",
      "Train loss: 1.058107 [275200/345580]\n",
      "Train loss: 1.023451 [278400/345580]\n",
      "Train loss: 0.939010 [281600/345580]\n",
      "Train loss: 1.119570 [284800/345580]\n",
      "Train loss: 1.058381 [288000/345580]\n",
      "Train loss: 1.022658 [291200/345580]\n",
      "Train loss: 1.001458 [294400/345580]\n",
      "Train loss: 1.111449 [297600/345580]\n",
      "Train loss: 1.096140 [300800/345580]\n",
      "Train loss: 1.078359 [304000/345580]\n",
      "Train loss: 1.059479 [307200/345580]\n",
      "Train loss: 1.071793 [310400/345580]\n",
      "Train loss: 1.036523 [313600/345580]\n",
      "Train loss: 0.970806 [316800/345580]\n",
      "Train loss: 1.068771 [320000/345580]\n",
      "Train loss: 1.038617 [323200/345580]\n",
      "Train loss: 1.052760 [326400/345580]\n",
      "Train loss: 1.005975 [329600/345580]\n",
      "Train loss: 1.011508 [332800/345580]\n",
      "Train loss: 0.985897 [336000/345580]\n",
      "Train loss: 1.130716 [339200/345580]\n",
      "Train loss: 1.055782 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.1%, Avg loss: 1.041882 \n",
      "\n",
      "Epoch 82\n",
      " -------------------\n",
      "Train loss: 1.055234 [    0/345580]\n",
      "Train loss: 1.022700 [ 3200/345580]\n",
      "Train loss: 0.993558 [ 6400/345580]\n",
      "Train loss: 0.951077 [ 9600/345580]\n",
      "Train loss: 1.029552 [12800/345580]\n",
      "Train loss: 1.010013 [16000/345580]\n",
      "Train loss: 1.028905 [19200/345580]\n",
      "Train loss: 1.045606 [22400/345580]\n",
      "Train loss: 0.948645 [25600/345580]\n",
      "Train loss: 0.981774 [28800/345580]\n",
      "Train loss: 1.077792 [32000/345580]\n",
      "Train loss: 0.897699 [35200/345580]\n",
      "Train loss: 0.967806 [38400/345580]\n",
      "Train loss: 1.052672 [41600/345580]\n",
      "Train loss: 0.984328 [44800/345580]\n",
      "Train loss: 1.053131 [48000/345580]\n",
      "Train loss: 1.064020 [51200/345580]\n",
      "Train loss: 1.059973 [54400/345580]\n",
      "Train loss: 1.080295 [57600/345580]\n",
      "Train loss: 1.008919 [60800/345580]\n",
      "Train loss: 1.085055 [64000/345580]\n",
      "Train loss: 1.075860 [67200/345580]\n",
      "Train loss: 0.965767 [70400/345580]\n",
      "Train loss: 0.982323 [73600/345580]\n",
      "Train loss: 1.093205 [76800/345580]\n",
      "Train loss: 1.111546 [80000/345580]\n",
      "Train loss: 1.054467 [83200/345580]\n",
      "Train loss: 1.095462 [86400/345580]\n",
      "Train loss: 1.047415 [89600/345580]\n",
      "Train loss: 1.015179 [92800/345580]\n",
      "Train loss: 1.057051 [96000/345580]\n",
      "Train loss: 1.131153 [99200/345580]\n",
      "Train loss: 0.962943 [102400/345580]\n",
      "Train loss: 1.048639 [105600/345580]\n",
      "Train loss: 1.055040 [108800/345580]\n",
      "Train loss: 1.012989 [112000/345580]\n",
      "Train loss: 1.039033 [115200/345580]\n",
      "Train loss: 1.048545 [118400/345580]\n",
      "Train loss: 1.052597 [121600/345580]\n",
      "Train loss: 1.106181 [124800/345580]\n",
      "Train loss: 1.100728 [128000/345580]\n",
      "Train loss: 1.019160 [131200/345580]\n",
      "Train loss: 1.184368 [134400/345580]\n",
      "Train loss: 1.088066 [137600/345580]\n",
      "Train loss: 0.899110 [140800/345580]\n",
      "Train loss: 1.001787 [144000/345580]\n",
      "Train loss: 0.986367 [147200/345580]\n",
      "Train loss: 0.929711 [150400/345580]\n",
      "Train loss: 1.015417 [153600/345580]\n",
      "Train loss: 0.991155 [156800/345580]\n",
      "Train loss: 1.022578 [160000/345580]\n",
      "Train loss: 1.130944 [163200/345580]\n",
      "Train loss: 0.985511 [166400/345580]\n",
      "Train loss: 1.084930 [169600/345580]\n",
      "Train loss: 0.992715 [172800/345580]\n",
      "Train loss: 0.997505 [176000/345580]\n",
      "Train loss: 1.097804 [179200/345580]\n",
      "Train loss: 1.100797 [182400/345580]\n",
      "Train loss: 1.004289 [185600/345580]\n",
      "Train loss: 1.070093 [188800/345580]\n",
      "Train loss: 1.002445 [192000/345580]\n",
      "Train loss: 1.106078 [195200/345580]\n",
      "Train loss: 1.100432 [198400/345580]\n",
      "Train loss: 1.009577 [201600/345580]\n",
      "Train loss: 1.095258 [204800/345580]\n",
      "Train loss: 1.068229 [208000/345580]\n",
      "Train loss: 1.109925 [211200/345580]\n",
      "Train loss: 1.050753 [214400/345580]\n",
      "Train loss: 1.028423 [217600/345580]\n",
      "Train loss: 1.049222 [220800/345580]\n",
      "Train loss: 1.035963 [224000/345580]\n",
      "Train loss: 0.988175 [227200/345580]\n",
      "Train loss: 1.070786 [230400/345580]\n",
      "Train loss: 0.998992 [233600/345580]\n",
      "Train loss: 1.071407 [236800/345580]\n",
      "Train loss: 0.984075 [240000/345580]\n",
      "Train loss: 1.053604 [243200/345580]\n",
      "Train loss: 1.056359 [246400/345580]\n",
      "Train loss: 1.089653 [249600/345580]\n",
      "Train loss: 0.986689 [252800/345580]\n",
      "Train loss: 1.061815 [256000/345580]\n",
      "Train loss: 1.054622 [259200/345580]\n",
      "Train loss: 1.125011 [262400/345580]\n",
      "Train loss: 1.029619 [265600/345580]\n",
      "Train loss: 0.992329 [268800/345580]\n",
      "Train loss: 1.041784 [272000/345580]\n",
      "Train loss: 0.947932 [275200/345580]\n",
      "Train loss: 1.000342 [278400/345580]\n",
      "Train loss: 1.070154 [281600/345580]\n",
      "Train loss: 1.024399 [284800/345580]\n",
      "Train loss: 1.066600 [288000/345580]\n",
      "Train loss: 1.061124 [291200/345580]\n",
      "Train loss: 1.128369 [294400/345580]\n",
      "Train loss: 0.962028 [297600/345580]\n",
      "Train loss: 1.020079 [300800/345580]\n",
      "Train loss: 1.037586 [304000/345580]\n",
      "Train loss: 0.909825 [307200/345580]\n",
      "Train loss: 0.951934 [310400/345580]\n",
      "Train loss: 1.067538 [313600/345580]\n",
      "Train loss: 1.064762 [316800/345580]\n",
      "Train loss: 1.084245 [320000/345580]\n",
      "Train loss: 0.958211 [323200/345580]\n",
      "Train loss: 1.172668 [326400/345580]\n",
      "Train loss: 1.050635 [329600/345580]\n",
      "Train loss: 1.072624 [332800/345580]\n",
      "Train loss: 0.985256 [336000/345580]\n",
      "Train loss: 1.255855 [339200/345580]\n",
      "Train loss: 1.067045 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 43.0%, Avg loss: 1.041385 \n",
      "\n",
      "Epoch 83\n",
      " -------------------\n",
      "Train loss: 1.028700 [    0/345580]\n",
      "Train loss: 1.094335 [ 3200/345580]\n",
      "Train loss: 1.105239 [ 6400/345580]\n",
      "Train loss: 0.954707 [ 9600/345580]\n",
      "Train loss: 1.123106 [12800/345580]\n",
      "Train loss: 1.053345 [16000/345580]\n",
      "Train loss: 1.095815 [19200/345580]\n",
      "Train loss: 1.020968 [22400/345580]\n",
      "Train loss: 1.015703 [25600/345580]\n",
      "Train loss: 0.907117 [28800/345580]\n",
      "Train loss: 1.037280 [32000/345580]\n",
      "Train loss: 1.061968 [35200/345580]\n",
      "Train loss: 1.094950 [38400/345580]\n",
      "Train loss: 1.055686 [41600/345580]\n",
      "Train loss: 0.989124 [44800/345580]\n",
      "Train loss: 1.090203 [48000/345580]\n",
      "Train loss: 1.110579 [51200/345580]\n",
      "Train loss: 1.098232 [54400/345580]\n",
      "Train loss: 1.051698 [57600/345580]\n",
      "Train loss: 1.035505 [60800/345580]\n",
      "Train loss: 1.024853 [64000/345580]\n",
      "Train loss: 1.104614 [67200/345580]\n",
      "Train loss: 0.979296 [70400/345580]\n",
      "Train loss: 1.098910 [73600/345580]\n",
      "Train loss: 1.045630 [76800/345580]\n",
      "Train loss: 1.017674 [80000/345580]\n",
      "Train loss: 1.166612 [83200/345580]\n",
      "Train loss: 1.074651 [86400/345580]\n",
      "Train loss: 1.025802 [89600/345580]\n",
      "Train loss: 1.126048 [92800/345580]\n",
      "Train loss: 1.027293 [96000/345580]\n",
      "Train loss: 1.052601 [99200/345580]\n",
      "Train loss: 0.980080 [102400/345580]\n",
      "Train loss: 1.146237 [105600/345580]\n",
      "Train loss: 1.094895 [108800/345580]\n",
      "Train loss: 1.079736 [112000/345580]\n",
      "Train loss: 1.081998 [115200/345580]\n",
      "Train loss: 1.088215 [118400/345580]\n",
      "Train loss: 0.940381 [121600/345580]\n",
      "Train loss: 0.991680 [124800/345580]\n",
      "Train loss: 1.077952 [128000/345580]\n",
      "Train loss: 1.114761 [131200/345580]\n",
      "Train loss: 0.951651 [134400/345580]\n",
      "Train loss: 1.058251 [137600/345580]\n",
      "Train loss: 1.022427 [140800/345580]\n",
      "Train loss: 1.089232 [144000/345580]\n",
      "Train loss: 1.046979 [147200/345580]\n",
      "Train loss: 1.097253 [150400/345580]\n",
      "Train loss: 0.992304 [153600/345580]\n",
      "Train loss: 1.037095 [156800/345580]\n",
      "Train loss: 1.017220 [160000/345580]\n",
      "Train loss: 1.026876 [163200/345580]\n",
      "Train loss: 1.128787 [166400/345580]\n",
      "Train loss: 1.048246 [169600/345580]\n",
      "Train loss: 0.987638 [172800/345580]\n",
      "Train loss: 0.982660 [176000/345580]\n",
      "Train loss: 1.025461 [179200/345580]\n",
      "Train loss: 1.060741 [182400/345580]\n",
      "Train loss: 0.979016 [185600/345580]\n",
      "Train loss: 0.985887 [188800/345580]\n",
      "Train loss: 1.122382 [192000/345580]\n",
      "Train loss: 1.059873 [195200/345580]\n",
      "Train loss: 0.979834 [198400/345580]\n",
      "Train loss: 1.030751 [201600/345580]\n",
      "Train loss: 0.944119 [204800/345580]\n",
      "Train loss: 1.064457 [208000/345580]\n",
      "Train loss: 1.005407 [211200/345580]\n",
      "Train loss: 1.056270 [214400/345580]\n",
      "Train loss: 1.109484 [217600/345580]\n",
      "Train loss: 1.068319 [220800/345580]\n",
      "Train loss: 1.038967 [224000/345580]\n",
      "Train loss: 1.010336 [227200/345580]\n",
      "Train loss: 0.955375 [230400/345580]\n",
      "Train loss: 1.100517 [233600/345580]\n",
      "Train loss: 0.938169 [236800/345580]\n",
      "Train loss: 0.991373 [240000/345580]\n",
      "Train loss: 1.026961 [243200/345580]\n",
      "Train loss: 1.024182 [246400/345580]\n",
      "Train loss: 1.051220 [249600/345580]\n",
      "Train loss: 1.091354 [252800/345580]\n",
      "Train loss: 1.003721 [256000/345580]\n",
      "Train loss: 1.063839 [259200/345580]\n",
      "Train loss: 1.093346 [262400/345580]\n",
      "Train loss: 1.100904 [265600/345580]\n",
      "Train loss: 1.036560 [268800/345580]\n",
      "Train loss: 1.067910 [272000/345580]\n",
      "Train loss: 0.979206 [275200/345580]\n",
      "Train loss: 1.047747 [278400/345580]\n",
      "Train loss: 1.006906 [281600/345580]\n",
      "Train loss: 1.011622 [284800/345580]\n",
      "Train loss: 0.993249 [288000/345580]\n",
      "Train loss: 1.011599 [291200/345580]\n",
      "Train loss: 0.939405 [294400/345580]\n",
      "Train loss: 1.068468 [297600/345580]\n",
      "Train loss: 0.997710 [300800/345580]\n",
      "Train loss: 1.118250 [304000/345580]\n",
      "Train loss: 1.046262 [307200/345580]\n",
      "Train loss: 0.988280 [310400/345580]\n",
      "Train loss: 1.039378 [313600/345580]\n",
      "Train loss: 1.087975 [316800/345580]\n",
      "Train loss: 0.999005 [320000/345580]\n",
      "Train loss: 1.049445 [323200/345580]\n",
      "Train loss: 1.082817 [326400/345580]\n",
      "Train loss: 1.005580 [329600/345580]\n",
      "Train loss: 0.984327 [332800/345580]\n",
      "Train loss: 1.089434 [336000/345580]\n",
      "Train loss: 1.004177 [339200/345580]\n",
      "Train loss: 1.066890 [342400/345580]\n",
      "Test Error:\n",
      " Accuracy: 42.9%, Avg loss: 1.042204 \n",
      "\n",
      "Epoch 84\n",
      " -------------------\n",
      "Train loss: 1.098287 [    0/345580]\n",
      "Train loss: 1.022552 [ 3200/345580]\n",
      "Train loss: 1.092630 [ 6400/345580]\n",
      "Train loss: 0.980844 [ 9600/345580]\n",
      "Train loss: 0.921183 [12800/345580]\n",
      "Train loss: 1.044470 [16000/345580]\n",
      "Train loss: 1.043540 [19200/345580]\n",
      "Train loss: 0.994657 [22400/345580]\n",
      "Train loss: 0.975024 [25600/345580]\n",
      "Train loss: 1.099382 [28800/345580]\n",
      "Train loss: 1.082660 [32000/345580]\n",
      "Train loss: 1.078055 [35200/345580]\n",
      "Train loss: 1.013369 [38400/345580]\n",
      "Train loss: 1.025091 [41600/345580]\n",
      "Train loss: 1.041938 [44800/345580]\n",
      "Train loss: 1.064871 [48000/345580]\n",
      "Train loss: 1.061120 [51200/345580]\n",
      "Train loss: 1.082071 [54400/345580]\n",
      "Train loss: 1.069556 [57600/345580]\n",
      "Train loss: 1.053917 [60800/345580]\n",
      "Train loss: 0.983262 [64000/345580]\n",
      "Train loss: 1.078296 [67200/345580]\n",
      "Train loss: 1.090245 [70400/345580]\n",
      "Train loss: 1.018607 [73600/345580]\n",
      "Train loss: 0.960638 [76800/345580]\n",
      "Train loss: 0.984814 [80000/345580]\n",
      "Train loss: 1.042724 [83200/345580]\n",
      "Train loss: 1.081944 [86400/345580]\n",
      "Train loss: 0.967905 [89600/345580]\n",
      "Train loss: 1.001059 [92800/345580]\n",
      "Train loss: 1.123032 [96000/345580]\n",
      "Train loss: 0.976241 [99200/345580]\n",
      "Train loss: 1.040144 [102400/345580]\n",
      "Train loss: 1.033154 [105600/345580]\n",
      "Train loss: 1.107028 [108800/345580]\n",
      "Train loss: 1.046102 [112000/345580]\n",
      "Train loss: 1.022802 [115200/345580]\n",
      "Train loss: 0.999420 [118400/345580]\n",
      "Train loss: 1.056682 [121600/345580]\n",
      "Train loss: 1.078882 [124800/345580]\n",
      "Train loss: 1.094123 [128000/345580]\n",
      "Train loss: 0.985957 [131200/345580]\n",
      "Train loss: 0.967249 [134400/345580]\n",
      "Train loss: 0.996519 [137600/345580]\n",
      "Train loss: 0.873456 [140800/345580]\n",
      "Train loss: 0.923424 [144000/345580]\n",
      "Train loss: 0.934875 [147200/345580]\n",
      "Train loss: 1.083054 [150400/345580]\n",
      "Train loss: 1.035537 [153600/345580]\n",
      "Train loss: 1.016228 [156800/345580]\n",
      "Train loss: 1.007765 [160000/345580]\n",
      "Train loss: 1.143547 [163200/345580]\n",
      "Train loss: 1.014527 [166400/345580]\n",
      "Train loss: 1.108657 [169600/345580]\n",
      "Train loss: 1.031664 [172800/345580]\n",
      "Train loss: 1.150592 [176000/345580]\n",
      "Train loss: 1.024809 [179200/345580]\n",
      "Train loss: 1.023845 [182400/345580]\n",
      "Train loss: 1.050931 [185600/345580]\n",
      "Train loss: 1.021988 [188800/345580]\n",
      "Train loss: 0.981208 [192000/345580]\n",
      "Train loss: 1.024836 [195200/345580]\n",
      "Train loss: 0.949631 [198400/345580]\n",
      "Train loss: 1.125676 [201600/345580]\n",
      "Train loss: 0.974133 [204800/345580]\n",
      "Train loss: 1.012975 [208000/345580]\n",
      "Train loss: 1.026682 [211200/345580]\n",
      "Train loss: 0.922117 [214400/345580]\n",
      "Train loss: 1.111776 [217600/345580]\n",
      "Train loss: 0.962183 [220800/345580]\n",
      "Train loss: 1.015037 [224000/345580]\n",
      "Train loss: 1.077841 [227200/345580]\n",
      "Train loss: 1.089049 [230400/345580]\n",
      "Train loss: 1.133901 [233600/345580]\n",
      "Train loss: 1.147882 [236800/345580]\n",
      "Train loss: 1.000528 [240000/345580]\n",
      "Train loss: 1.093250 [243200/345580]\n",
      "Train loss: 1.007790 [246400/345580]\n",
      "Train loss: 0.940668 [249600/345580]\n",
      "Train loss: 1.007660 [252800/345580]\n",
      "Train loss: 0.938374 [256000/345580]\n",
      "Train loss: 1.077159 [259200/345580]\n",
      "Train loss: 1.022101 [262400/345580]\n",
      "Train loss: 1.047458 [265600/345580]\n",
      "Train loss: 0.982353 [268800/345580]\n"
     ]
    }
   ],
   "source": [
    "epochs = 100\n",
    "total_train_loss = []\n",
    "total_val_loss = []\n",
    "total_train_acc = []\n",
    "total_val_acc = []\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n -------------------\")\n",
    "    train_loss, train_acc = train(train_loader, model, crossentropy_loss, optimizer)\n",
    "    total_train_loss.append(train_loss)\n",
    "    total_train_acc.append(train_acc)\n",
    "    val_loss, val_acc = test(val_loader, model, crossentropy_loss) \n",
    "    total_val_loss.append(val_loss)\n",
    "    total_val_acc.append(val_acc)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "021175b8-0146-4248-bb17-63bce48a7a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"mimic.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "ef9c3a38-ef2e-44b2-8ecd-dae33bfbecee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_train_loss, 'r', label='train loss')  # Plotting\n",
    "plt.plot(total_val_loss, 'g', label='val loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "11323365-f5be-48cc-ac64-859d4e98fdad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(total_train_acc, 'r', label='train accuracy')  # Plotting\n",
    "plt.plot(total_val_acc, 'g', label='val accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0bd161f4-b358-4e4c-8ce2-ceba81af4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_list = []\n",
    "total, correct = 0, 0\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    for data,labels in test_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        y_test_pred = model(data)\n",
    "        _, predicted = torch.max(y_test_pred.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        y_pred_list.extend(predicted.cpu().numpy())\n",
    "accuracy = 100 * correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e778f8d8-958d-4939-b2e1-6b225edd5c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43.22620969842179"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8b8a95-7203-4f4b-b3ec-d0e987b52f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f968bd0c-7c61-4712-99ff-3d953a22f546",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95995"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "ce681d08-9442-4b68-8fd2-08c05ccaa1fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix_df = pd.DataFrame(confusion_matrix(Y_test, y_pred_list)).rename(columns=idx2class, index=idx2class)\n",
    "\n",
    "sns.heatmap(confusion_matrix_df, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4b067393-7f29-4e58-b9bc-d511afd325c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.45      0.48     31353\n",
      "           1       0.36      0.30      0.33     32435\n",
      "           2       0.43      0.54      0.48     32207\n",
      "\n",
      "    accuracy                           0.43     95995\n",
      "   macro avg       0.43      0.43      0.43     95995\n",
      "weighted avg       0.43      0.43      0.43     95995\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, y_pred_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "01c81c86-d447-4465-bace-de4a68614ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved PyTorch Mimic Model \n"
     ]
    }
   ],
   "source": [
    "#save model\n",
    "torch.save(model.state_dict(), \"mimic.pth\")\n",
    "print(\"Saved PyTorch Mimic Model \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d893c-4c40-4b2a-bba1-22123c89259b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
